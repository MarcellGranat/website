<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2023-04-22">

<title>Marcell Granat - QuantChallenge</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../logo.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-CEC6152V08"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-CEC6152V08', { 'anonymize_ip': true});
</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Marcell Granat</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About me</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.linkedin.com/in/granatmarcell"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text">LinkedIn</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/MarcellGranat"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text">GitHub</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://marcellgranat.github.io/cv/"><i class="bi bi-file-earmark-person" role="img">
</i> 
 <span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:granat.marcell@uni-neumann.hu"><i class="bi bi-envelope" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">QuantChallenge</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">ML</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 22, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p><a href="https://balintmazzag.netlify.app">BÃ¡lint Mazzag</a> and I participated in the QuantChallenge organized by Morgan Stanley, where we competed against 120 teams. Our hard work and determination paid off as we secured the 2nd place at the finals. This post presents our solution to first task.</p>
<p>Codes are available at the corresponding <a href="https://github.com/MarcellGranat/MorganStanley-QuantChallenge">GitHub repository</a>.</p>
<hr>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract <img src="logo.png" align="right" width="120px"></h2>
<p>We applied several machine learning and ensemble models to predict yields of corn, oats and soybeans. Our approach involved splitting historical data into training and testing sets. We utilized a walk-forward cross-validation method for tuning hyperparameters and testing the performance of the models to avoid overfitting with the aim that they produce reliable out-of-sample performance. LightGBM performed better than other base learners and stacked ensembles. We claim that the use of this model is appropriate to assess the climate risk of the investment, since its performance is stable on testing set and very close to currently published state-of-the-art methods even under the specified conditions.</p>
<hr>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>In this paper, we describe our framework to predict yearly crop yields in the counties of Minnesota. Accurately predicting crop yields is critical for farmers, policymakers and investors to make informed decisions related to agriculture. The climate is a crucial factor in crop production, and we aim to build a model based on weather data, including minimum, maximum and average daily temperatures and daily precipitation.</p>
<p>To conduct the analysis and come up with a model, we took into account three main factors during the process: (1) The agricultural specificities (mainly corn grain) of the investigated crops, (2) existing best practices and literature regarding such analysis and forecasting (3) tailoring our approach and model to be generic enough to predict crop yields regardless of geographic information.</p>
<p>Our research on corn yield prediction gave us insights into the modeling approaches that were successful in the past, namely <em>ensemble</em> learning methods that encompass <em>Least Absolute Shrinkage and Selection Operator</em> (LASSO), <em>Random Forest</em>, <em>Support Vector Machine</em> (SVM) and also <em>LightGBM</em> models. Other academic papers applied <em>Long Short-Term Memory</em> (LSTM) models and they were found to be effective in predicting crop yields (Sun et al.&nbsp;2019). We present a solution to combine the approach of these two existing competent methods first to predict corn yields in Minnesota counties, then for the rest of the crops in question.</p>
</section>
<section id="data" class="level1">
<h1>Data</h1>
<p>Our selection for the applied methodology was determined by the structure of the data &nbsp;and firstly, we focused our research on corn grain yield prediction. We got the daily data (minimum, maximum, average temperature and perception) of weather stations in Minnesota and annual corn yield statistics of each county in Minnesota from 1950 to 2020. Our goal was to predict the corn yield for a dataset (<em>prediction target</em>) that contains the same weather information for a period from 1991 to 2022, but its geographic location is anonymized.</p>
<p>Before combining the different datasets, we performed data cleaning and preprocessing steps. To handle missing values in the weather data, we applied&nbsp;<em>Multivariate Imputation by Chained Equations</em>&nbsp;(MICE). It is particularly useful, as it enables us to estimate missing data points by considering the relationships between variables. In the case of precipitation, which was frequently missing (63% in the weather dataset used for model building<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, 37% in the prediction target set), MICE allowed us to simulate realizations that are most likely given the weather patterns of similar days, what was the weather like on the same day and in the same location at adjacent times. We also applied this strategy on the prediction target set, where we added the observations from weather dataset to give even more accurate data.</p>
<p>This preprocessing step imputed the missing values redundant for the temperature values and ensured that we were working with a complete dataset complete already implies you donât have missing values. In the case of minimum and maximum temperature, the missing rate was below 1%, and about 9% for the average temperature. Following the literature, we used the average of the minimum and maximum temperature to calculate the average temperature, where it was originally missing. We noticed that the average temperature in the data is very close to the mean of the two limit measurement and this is a reliable approximation method.</p>
<p>After preprocessing the weather data, we merged it with the corn yield statistics by county and year to create a single dataset. In doing this we faced two challenges. The first challenge was to match the geographic locations of weather stations and counties accurately. The latitude and longitude coordinatES of the counties were given in the dataset and also for the weather stations. Even though we imputed the missing values in the database, we still did not have weather data for per day per station for the period under study. The stations were installed at different points in time, so in some cases, there was no observation at all for the given day. To address this issue, we calculated the distance between each weather station (based on Hijmans (2022)) and county to determine which three stations were closest to a given county and we assigned the distance-weighted average values to the counties.</p>
<figure class="figure">
<img src="manuscript_files/figure-gfm/fig-map-1.png" id="fig-map" width="90%" alt="Figure 1: Geographical location of the weather stations (green) included in the historical dataset and the counties (red) of Minnesota in 1950 and 2010." class="figure-img">
<figcaption aria-hidden="true" class="figure-caption">
Figure 1: Geographical location of the weather stations (green) included in the historical dataset and the counties (red) of Minnesota in 1950 and 2010.
</figcaption>
</figure>
<p>The second challenge we faced was dealing with the differences in temporal granularity between the weather data and yield statistics. The weather data had a daily time granularity while the corn yield statistics were at an annual frequency. One possible solution is to use each dayâs observation of the weather data as a predictor and combine the two datasets based on the year and the distance. After some initial attempts, we realized that this leads to a very small observation-to-feature ratio and overfitting in the built models. Shahhosseini, Hu, and Archontoulis (2020) also described this problem in their study and addressed it through a three-stage feature selection. Following their approach, we aggregated the daily weather data to a weekly level.</p>
<p>Based on the literature of agricultural research we also derived&nbsp;<em>Growing Degree Days</em>&nbsp;(GDD) and&nbsp;<em>Killing Degree Days</em>&nbsp;(KDD). KDD is a concept used to measure the accumulated heat units above a specific temperature threshold that can potentially harm or kill a plant and GDD is a measure of heat accumulation used to predict plant growth stages (Butler and Huybers 2013). It is calculated by taking the average daily temperature above a certain base temperature, which is specific to the plant species. For corn, the base temperature is typically around 9Â°C (41Â°F). The threshold for KDD is set at 29Â°C (Yu et al.&nbsp;2022; Lin et al. 2020). In the case of oats these values are 7Â°C and 25Â°C, while for soybean 12Â°C and 30Â°C (Porter and Parry 1993).</p>
<p>Corn is an annual plant, meaning it completes its life cycle within one growing season, from planting (spring) to harvesting (usually ends in October). The case of oats and soybean are the same. But weather in the previous year influence the quality of the soil, hence previous yearsâ precipitation and temperature could also affect yields. Therefore, we discarded all information from the given yearâs November and December and combined them with the previous yearâs data from winter. Although the modelâs forecast can be improved in this way, it involves the issue that for observations in the prediction target set where we do not have information on the previous yearâs weather, it is necessary to build a restricted model that only uses data from the current year.</p>
<p>Shahhosseini, Hu, and Archontoulis (2020) presents that an increasing trend in corn yields are observable. This also holds for our data (see <a href="#fig-trend-yield">Figure&nbsp;2</a>), but it also shows high fluctuations. Since most of the observations in the prediction target set overlap with the historical dataset we use for modelling, we decided to add year as a predictor to the model instead of subtracting a linear trend of the yields. The existence of this trend means that extrapolation with tree based models may perform low, but the preprocessing step Shahhosseini, Hu, and Archontoulis (2020) used also does not solve this issue. But applying year as a predictor may significantly improve the prediction ability if it is not the case of extrapolation. Since we apply walk-forward cross-validation for training and testing, our models may perform even more accurately, when predicting the yields for the prediction target set.</p>
<figure class="figure">
<img src="manuscript_files/figure-gfm/fig-trend-yield-1.png" width="90%" id="fig-trend-yield" alt="Figure 2: Trend of the annual corn yields in the counties of Minnesota." class="figure-img">
<figcaption aria-hidden="true" class="figure-caption">
Figure 2: Trend of the annual corn yields in the counties of Minnesota.
</figcaption>
</figure>
</section>
<section id="methodology" class="level1">
<h1>Methodology</h1>
<p>To find an appropriate model that could accurately predict corn yields, we found the collection of existing articles and their reported forecast accuracy in Shahhosseini, Hu, and Archontoulis (2020) to be a good starting point. They highlight that their ensemble model outperformed other models in predicting corn yields when they used walk-forward cross-validation for evaluation (<strong>RMSE = 1,138 kg/ha</strong>). There exist in the literature other approaches, but the majority of these are not fit to our goal. Some methods produced better predictions with the application of other weather and soil-related variables, such as MODIS sensors or irrigation, but in our current case, the weather data was given. Jeong et al.&nbsp;(2016) reported smaller RMSE values with random forest, but their validation method was not walk-forward cross-validation, which we found to be crucial in accurately evaluating our model, otherwise, the performance of the model may be questioned for a period outside the sample (extrapolation).</p>
<p>Shahhosseini, Hu, and Archontoulis (2020) states that combining multiple machine learning models could lead to better forecasting results than a single model if the base learners are diverse. To this end, we used a stacked ensemble of five<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> diverse models (base learners): LASSO, linear SVM, lightGBM, <em>multivariate adaptive&nbsp;regression&nbsp;splines</em> MARS and random forest. We splitted our historical data initially into training (before 2010) and testing sets (after 2010). Subsequently, we applied a walk-forward cross-validation method in which the <strong>8-year analysis</strong> and <strong>1-year assessment</strong> sets were shifted one year at a time.</p>
<figure class="figure">
<img src="wfcv.png" id="fig-wfcv" width="90%" alt="Figure 3: Walk-forward cross-validation for training the baselearner and ensemble models." class="figure-img">
<figcaption aria-hidden="true" class="figure-caption">
Figure 3: Walk-forward cross-validation for training the baselearner and ensemble models.
</figcaption>
</figure>
<p>We tuned the hyperparameters of the applied base learners&nbsp;<strong>using grid search to minimize the resulted</strong>&nbsp;<strong>root mean squared error</strong>&nbsp;<strong>(RMSE) on the assessment sets</strong>, while they were trained on the analysis sets. We selected RMSE as the target metric, to produce comparable results with the existing literature.</p>
<p>We applied three models to build the stacked ensemble models based on the prediction of the base learners: ordinary least squares (OLS), LASSO and&nbsp;(MARS). To tune the hyperparameters of these models we generated predictions from the previously mentioned diverse base learners for the assessment sets and we used the previously mentioned walk-forward cross-validation. These sets contain substantially fewer variables (only the predictions from the four base learners) and the number of sets is also lower since we drop the first eight years. But this does not influence our final selection, because we compare the performance of the models on the test set which is held out and unseen during the training process.&nbsp;</p>
<p>This method allows us to identify the best-performing stacked ensemble model (or a base learner) based on its performance in predicting corn yield with the possibility of extrapolation.</p>
<p>We also extended our methodology with a geometric weighting of the observations based on the year. This approach is also known as Koyck lag (Koyck 1954), and it aimed to give more emphasis to the more recent observations, as they are expected to be more relevant for predicting the current corn yield. The theoretical justification for this approach is that several technological improvement in agriculture has been made in recent years, which might lead to changes in the relationship between the climate and corn production.&nbsp;</p>
<p>This weighting was incorporated into our framework by applying importance weights. This means that weights are applied during the model building on the analysis set, but in contrast with frequency weights, these weights do not affect directly the performance indicators related. We extended the original formula with an additional fraction to avoid the bias caused by the unequal number of observations in the different years. The equation above shows how we formed these weights:</p>
<p><span class="math display">\[
weight_{t} = \gamma^{max(T)- t} \frac{min(n)}{n_t},
\]</span></p>
<p>where <span class="math inline">\(t\)</span> denotes the current year, <span class="math inline">\(T\)</span> the set of the year, <span class="math inline">\(n\)</span> the number of observations per years, and <span class="math inline">\(n_t\)</span> the number of observation in the current year.</p>
<p>A limitation of this weighting technique is that according to our knowledge, currently there is no implementation of adding weights to SVM and lightGBM models (Kuhn and Wickham 2020). Similarly, in the MARS model there is an implementation for weighting, but it considerably increases the running time and in our case we had to abandon weighting here as well (Hastie and wrapper 2023). In these three cases, we fitted unweighted models.</p>
<p>The mentioned models require additional data preprocessing steps beyond those included in the data section. Most of the models needed normalization of the input variables, scaling them to have zero means and unit variances. Furthermore, we also performed feature selection by removing variables that are highly correlated with other variables. We set this limit to 0.9, to have comparable results with Shahhosseini, Hu, and Archontoulis (2020), who have used the same cutoff.</p>
<p>We consider that doing these preprocessing step on the whole historical dataset at once lead to look-ahead bias as evaluating the predictive ability of the model with an observation in the future based on past information is not a realistic scenario and lead to unreliable performance metrics. Therefore, we evaluated these steps on each analysis step separately.</p>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<p>To present the results of our framework, we start with the base learner models. As we described in the methods section, we used different machine learning algorithms to predict corn yield based on climate data, and we tuned the hyperparameters on the splits generated from the training set. Grid based tuning means that for each model we generated a grid of hyperparameter values and trained the model with each combination, selecting the one that performs best on the assessment sets. We applied the maximum entropy design for tuning, which means that the possible hyperparameter combinations are selected to cover the candidate space well and drastically increase the chances of finding good results (Kuhn and Silge 2022). <a href="#fig-svm-grid">Figure 4</a> shows the method for the case of SVM, in the case of this model we tried 900 different set of hyperparameters<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<figure class="figure">
<img src="manuscript_files/figure-gfm/fig-svm-grid-1.png" width="90%" id="fig-svm-grid" alt="Figure 4: Maximum entropy design based hyperparameter tuning for linear support vector machines. Hyperparameter cost is a positive number for the cost of predicting a sample within or on the wrong side of the margin and insensitivity margin is the value for the epsilon in the SVM insensitive loss function. The values of these hyperparameters are on the x-axis and the resulted average metrics (we have 51 assessment sets) are on the y-axis." class="figure-img">
<figcaption aria-hidden="true" class="figure-caption">
Figure 4: Maximum entropy design based hyperparameter tuning for linear support vector machines. Hyperparameter cost is a positive number for the cost of predicting a sample within or on the wrong side of the margin and insensitivity margin is the value for the epsilon in the SVM insensitive loss function. The values of these hyperparameters are on the x-axis and the resulted average metrics (we have 51 assessment sets) are on the y-axis.
</figcaption>
</figure>
<p>As shown in the figure the optimal values for the hyperparameters are varying based on the targeted performance metric. We decided to minimize the RMSE metric to get comparable results with the literature. The tuned base learner model workflows are reported in the <a href="#appendix">Appendix</a>. <a href="#tbl-base-learner-rs">Table 1</a> shows the resulted RMSE values of the tuned models on the testing set (also generated with walk-forward cross-validation).</p>
<div id="tbl-base-learner-rs" class="tbl-parent quarto-layout-panel anchored">
<div class="panel-caption table-caption">
<p>Table 1: Average RMSE values of the tuned base learner models on the testing set. Values between the parentheses show the standard error (we have 11 assessment sets from the testing data). The <span class="math inline">\(\gamma\)</span> in the column names refer to the weighting parameter. <span class="math inline">\(\gamma=1\)</span> means no weigthing used there.</p>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<table class="table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 19%">
<col style="width: 20%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Engine</th>
<th style="text-align: center;"><span class="math inline">\(\gamma\)</span>=0.75</th>
<th style="text-align: center;"><span class="math inline">\(\gamma\)</span>=0.9</th>
<th style="text-align: center;"><span class="math inline">\(\gamma\)</span>=0.95</th>
<th style="text-align: center;"><span class="math inline">\(\gamma\)</span>=1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">LASSO</td>
<td style="text-align: center;">27.51 (8.83)</td>
<td style="text-align: center;">29.17 (9.97)</td>
<td style="text-align: center;">29.59 (10.5)</td>
<td style="text-align: center;">30.08 (11.4)</td>
</tr>
<tr class="even">
<td style="text-align: left;">OLS</td>
<td style="text-align: center;">95.90 (67.3)</td>
<td style="text-align: center;">80.73 (70.1)</td>
<td style="text-align: center;">79.73 (70.2)</td>
<td style="text-align: center;">80.55 (71.2)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Random forest</td>
<td style="text-align: center;">26.47 (6.24)</td>
<td style="text-align: center;">25.99 (6.51)</td>
<td style="text-align: center;">26.02 (6.45)</td>
<td style="text-align: center;">25.60 (6.53)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Linear SVM</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">47.41 (38.5)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">MARS</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">47.79 (18.9)</td>
</tr>
<tr class="even">
<td style="text-align: left;">LightGBM</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">24.84 (7.64)</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>The results show that the provided weighting method could improve the performance of the LASSO regression. The performance of linear SVM and MARS is significantly lower than the other ML models (we added OLS only as a baseline example). The latter is not surprising, since MARS is especially useful to identify non-linear relationships and interaction effects in the data, but it is suggested to use lower dimensional inputs (Friedman 1991). Hence, we considered an ensemble containing all base learners and another using only the highest-performing models, Random Forest, LASSO and LightGBM. <a href="#tbl-ensemble-rs">Table 2</a> shows the performance of the tuned ensemble models on the testing set (parameters of the tuned workflows are reported in the <a href="#appendix">Appendix</a>).</p>
<div id="tbl-ensemble-rs" class="tbl-parent quarto-layout-panel anchored">
<div class="panel-caption table-caption">
<p>Table 2: Average RMSE values of the tuned ensembe models on the testing set. Values between the parentheses show the standard error (we have 11 assessment sets from the testing data). Values in the <em>Include</em> column refer to the RMSE values, when the ensemble contain all base learners, and values <em>Exclude</em> refer to RMSE, when only the highest-performing models.</p>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">Engine</th>
<th style="text-align: center;">Exclude</th>
<th style="text-align: center;">Include</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">MARS</td>
<td style="text-align: center;">27.32 (6.17)</td>
<td style="text-align: center;">30.27 (7.61)</td>
</tr>
<tr class="even">
<td style="text-align: left;">LASSO</td>
<td style="text-align: center;">26.03 (6.13)</td>
<td style="text-align: center;">25.49 (6.5)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">OLS</td>
<td style="text-align: center;">28.50 (6.34)</td>
<td style="text-align: center;">30.69 (7.93)</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Based on <a href="#tbl-base-learner-rs">Table 1</a> and <a href="#tbl-ensemble-rs">Table 2</a> we conclude that the best performing model was the LightGBM. The LASSO-based ensemble gave a very close result, with a smaller standard error, that might be advantageous when using for extrapolation (more likely to result the same performance). <a href="#fig-rs-ts">Figure 5</a> shows the trend of the performance of the applied models (selected). Based on the figure we do not see, that the performance of any model would considerably outperform the LightGBM in the most recent years. We would highlight this result, because the aim of the analysis is to build a model to assess the climate risk of a bankâs investments in this field, thus the performance in the recent years might be more important than the average performance. But we see that LightGBM is the best performing model based on both evaluation system. Furthermore, the principle of parsimony and the lower computation time are arguments in favor of LightGBM.</p>
<figure class="figure">
<img src="manuscript_files/figure-gfm/fig-rs-ts-1.png" id="fig-rs-ts" width="90%" alt="Figure 5: Trend of the performance based on the RMSE metric." class="figure-img">
<figcaption aria-hidden="true" class="figure-caption">
Figure 5: Trend of the performance based on the RMSE metric.
</figcaption>
</figure>
<p>In contrast with the results of Shahhosseini, Hu, and Archontoulis (2020), the fact that one of the base learners outperformed the best ensemble model might be possible for the following reasons: (1) We omitted xgboost from the analysis, because of computational bugs that we mentioned earlier. Also important to note that xgboost is ineffective if the prediction requires extrapolation (Bandara, Bergmeir, and Smyl 2020), thus we believe an ensemble with the addition of this model, would not be a clearly better solution. (2) Another reason might be because Shahhosseini, Hu, and Archontoulis (2020) used county level fixed dummy variables, that we could not, because the prediction target is anonymized. This might be the reason also for the higher lowest RMSE that we reached during the analysis (LightGBM results 24.84 bushels/acre, while their best is 1,138 kg/ha, which equals to 18.13 bushels/acre). The relatively lower performance might also be a result of applying walk-forward cross-validation also in the testing process, and they applied another weighting method for ensembling.</p>
<p>Based on the above results, we apply LightGBM to estimate the yields for oats and soybeans. The tuned LightGBM model for these commodities is reported in the <a href="#appendix">Appendix</a>. For these yields LightGBM was even more effective. The average RMSE of the LightGBM predictions for oats was 17.7 bushels/acre and 9.29 bushels/acre for soybeans (evaluated on the testing set).</p>
<p>One limitation of the model is worth mentioning, which can also be seen in the file containing the submitted predictions. LightGBM is tree based model, thus it is expected to perform weakly if the prediction requires extrapolation. If the data is non-stationary then it is possible that predictions remain constant all of them get to the same node. As mentioned earlier observation in the prediction target sets mostly overlap with provided historical dataset. In these cases LightGBM gives accurate predictions. Hence, we trained the final model on the observation after 2006 from the historical dataset and gave predictions based on that. But for the rarely observable old datapoints in the prediction target set, we would suggest using the ensemble or the lasso model instead (we managed these points as they are not the main target in this exercise). We would also suggest the ensemble if our target is assess the climate risk of the investment on the long-run (distribution of the climate might substantially differ).</p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>We have presented a framework for predicting corn yields using machine learning algorithms and ensemble based on climate data. The demonstrated Koyck-lag based weighting could improve the performance of the base learners, but the ensemble still underperformed the LightGBM in terms of RMSE. We applied walk-forward cross-validation to ensure that our model is not overfitted and it produces reliable out-of-sample performance, so it can be used to assess the climate risk of the investment. LightGBM came up with promising results, since its performance on the testing set was consistently better than that of the other algorithms, even in the most recent years. Its performance still falls short compared to the current state-of-the-art methods, but one explanation could be our constraint not to use counties as variables in our prediction, therefore we could not include fixed effect dummies in the model.</p>
</section>
<section id="appendix" class="level1">
<h1>Appendix</h1>
<section id="tuned-base-learners" class="level2">
<h2 class="anchored" data-anchor-id="tuned-base-learners">Tuned base learners</h2>
<pre><code>ââ Workflow ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Preprocessor: Recipe
Model: linear_reg()

ââ Preprocessor ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
4 Recipe Steps

â¢ step_rm()
â¢ step_zv()
â¢ step_corr()
â¢ step_normalize()

ââ Model âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Linear Regression Model Specification (regression)

Computational engine: lm 

ââ Workflow ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Preprocessor: Recipe
Model: rand_forest()

ââ Preprocessor ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
4 Recipe Steps

â¢ step_rm()
â¢ step_zv()
â¢ step_corr()
â¢ step_normalize()

ââ Model âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Random Forest Model Specification (regression)

Main Arguments:
  mtry = 45
  min_n = 17

Computational engine: ranger 

ââ Workflow ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Preprocessor: Recipe
Model: svm_linear()

ââ Preprocessor ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
4 Recipe Steps

â¢ step_rm()
â¢ step_zv()
â¢ step_corr()
â¢ step_normalize()

ââ Model âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Linear Support Vector Machine Model Specification (regression)

Main Arguments:
  cost = 6.54552673129715
  margin = 0.0207393052951536

Computational engine: LiblineaR 

ââ Workflow ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Preprocessor: Recipe
Model: boost_tree()

ââ Preprocessor ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
4 Recipe Steps

â¢ step_rm()
â¢ step_zv()
â¢ step_corr()
â¢ step_normalize()

ââ Model âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Boosted Tree Model Specification (regression)

Main Arguments:
  trees = 305
  min_n = 38
  tree_depth = 1
  learn_rate = 0.050851280760922
  loss_reduction = 0.276579958596952
  sample_size = 0.283269332054071
  stop_iter = 13

Computational engine: lightgbm 

ââ Workflow ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Preprocessor: Recipe
Model: linear_reg()

ââ Preprocessor ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
4 Recipe Steps

â¢ step_rm()
â¢ step_zv()
â¢ step_corr()
â¢ step_normalize()

ââ Case Weights ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
case_wts

ââ Model âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Linear Regression Model Specification (regression)

Main Arguments:
  penalty = 0.996509087448771
  mixture = 1

Computational engine: glmnet 

ââ Workflow ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Preprocessor: Recipe
Model: mars()

ââ Preprocessor ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
4 Recipe Steps

â¢ step_rm()
â¢ step_zv()
â¢ step_corr()
â¢ step_normalize()

ââ Model âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
MARS Model Specification (regression)

Main Arguments:
  prod_degree = 1

Computational engine: earth </code></pre>
</section>
<section id="tuned-ensembles-using-all-the-base-learners" class="level2">
<h2 class="anchored" data-anchor-id="tuned-ensembles-using-all-the-base-learners">Tuned ensembles using all the base learners</h2>
<pre><code>ââ Workflow ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Preprocessor: Recipe
Model: linear_reg()

ââ Preprocessor ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
2 Recipe Steps

â¢ step_normalize()
â¢ step_zv()

ââ Model âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Linear Regression Model Specification (regression)

Main Arguments:
  penalty = 0.785607671096585
  mixture = 1

Computational engine: glmnet 

ââ Workflow ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Preprocessor: Recipe
Model: linear_reg()

ââ Preprocessor ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
2 Recipe Steps

â¢ step_normalize()
â¢ step_zv()

ââ Model âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Linear Regression Model Specification (regression)

Computational engine: lm 

ââ Workflow ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Preprocessor: Recipe
Model: mars()

ââ Preprocessor ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
2 Recipe Steps

â¢ step_normalize()
â¢ step_zv()

ââ Model âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
MARS Model Specification (regression)

Main Arguments:
  prod_degree = 1

Computational engine: earth </code></pre>
</section>
<section id="tuned-ensembles-using-only-high-performing-base-learners" class="level2">
<h2 class="anchored" data-anchor-id="tuned-ensembles-using-only-high-performing-base-learners">Tuned ensembles using only high-performing base learners</h2>
<pre><code>ââ Workflow ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Preprocessor: Recipe
Model: linear_reg()

ââ Preprocessor ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
2 Recipe Steps

â¢ step_normalize()
â¢ step_zv()

ââ Model âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Linear Regression Model Specification (regression)

Main Arguments:
  penalty = 0.596098767898989
  mixture = 1

Computational engine: glmnet 

ââ Workflow ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Preprocessor: Recipe
Model: linear_reg()

ââ Preprocessor ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
2 Recipe Steps

â¢ step_normalize()
â¢ step_zv()

ââ Model âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Linear Regression Model Specification (regression)

Computational engine: lm 

ââ Workflow ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Preprocessor: Recipe
Model: mars()

ââ Preprocessor ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
2 Recipe Steps

â¢ step_normalize()
â¢ step_zv()

ââ Model âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
MARS Model Specification (regression)

Main Arguments:
  prod_degree = 1

Computational engine: earth </code></pre>
</section>
<section id="tuned-lightgbm-for-oats" class="level2">
<h2 class="anchored" data-anchor-id="tuned-lightgbm-for-oats">Tuned LightGBM for oats</h2>
<pre><code>ââ Workflow ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Preprocessor: Recipe
Model: boost_tree()

ââ Preprocessor ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
4 Recipe Steps

â¢ step_rm()
â¢ step_zv()
â¢ step_corr()
â¢ step_normalize()

ââ Model âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Boosted Tree Model Specification (regression)

Main Arguments:
  trees = 1902
  min_n = 24
  tree_depth = 7
  learn_rate = 4.05162706037683e-10
  loss_reduction = 0.000344450632697509
  sample_size = 0.214169466402382
  stop_iter = 7

Computational engine: lightgbm </code></pre>
</section>
<section id="tuned-lightgbm-for-soybeans" class="level2">
<h2 class="anchored" data-anchor-id="tuned-lightgbm-for-soybeans">Tuned LightGBM for soybeans</h2>
<pre><code>ââ Workflow ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Preprocessor: Recipe
Model: boost_tree()

ââ Preprocessor ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
4 Recipe Steps

â¢ step_rm()
â¢ step_zv()
â¢ step_corr()
â¢ step_normalize()

ââ Model âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
Boosted Tree Model Specification (regression)

Main Arguments:
  trees = 1792
  min_n = 5
  tree_depth = 13
  learn_rate = 0.0666210324583905
  loss_reduction = 6.36062768974398e-05
  sample_size = 0.469276522332802
  stop_iter = 19

Computational engine: lightgbm </code></pre>
</section>
</section>
<section id="reference" class="level1">
<h1>Reference</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-bandara2020machine" class="csl-entry" role="doc-biblioentry">
Bandara, Kasun, Christoph Bergmeir, and Slava Smyl. 2020. âMachine Learning Models for Time Series Forecasting: Identifying Model Limitations.â <a href="https://arxiv.org/abs/2007.08492" class="uri">https://arxiv.org/abs/2007.08492</a>.
</div>
<div id="ref-butler2013adaptation" class="csl-entry" role="doc-biblioentry">
Butler, Ethan E, and Peter Huybers. 2013. âAdaptation of US Maize to Temperature Variations.â <em>Nature Climate Change</em> 3 (1): 68â72.
</div>
<div id="ref-friedman1991multivariate" class="csl-entry" role="doc-biblioentry">
Friedman, Jerome H. 1991. âMultivariate Adaptive Regression Splines.â <em>The Annals of Statistics</em> 19 (1): 1â67.
</div>
<div id="ref-hastieEarthMultivariateAdaptive2023" class="csl-entry" role="doc-biblioentry">
Hastie, Stephen Milborrow Derived from mda:mars by Trevor, and Rob Tibshirani Uses Alan Millerâs Fortran utilities with Thomas Lumleyâs leaps wrapper. 2023. <em>Earth: Multivariate Adaptive Regression Splines</em>. <a href="https://CRAN.R-project.org/package=earth" class="uri">https://CRAN.R-project.org/package=earth</a>.
</div>
<div id="ref-hijmansGeosphereSphericalTrigonometry2022" class="csl-entry" role="doc-biblioentry">
Hijmans, Robert J. 2022. <em>Geosphere: Spherical Trigonometry</em>. Manual. <a href="https://CRAN.R-project.org/package=geosphere" class="uri">https://CRAN.R-project.org/package=geosphere</a>.
</div>
<div id="ref-jeong2016random" class="csl-entry" role="doc-biblioentry">
Jeong, Jig Han, Jonathan P Resop, Nathaniel D Mueller, David H Fleisher, Kyungdahm Yun, Ethan E Butler, Dennis J Timlin, et al.&nbsp;2016. âRandom Forests for Global and Regional Crop Yield Predictions.â <em>PloS One</em> 11 (6): e0156571.
</div>
<div id="ref-koyckDistributedLagsInvestment1954" class="csl-entry" role="doc-biblioentry">
Koyck, Leendert Marinus. 1954. <em>Distributed Lags and Investment Analysis</em>. Vol. 4. North-Holland Publishing Company.
</div>
<div id="ref-kuhn2022tidy" class="csl-entry" role="doc-biblioentry">
Kuhn, Max, and Julia Silge. 2022. <em>Tidy Modeling with R</em>. â OâReilly Media, Inc.â.
</div>
<div id="ref-kuhnTidymodelsCollectionPackages2020" class="csl-entry" role="doc-biblioentry">
Kuhn, Max, and Hadley Wickham. 2020. <em>Tidymodels: A Collection of Packages for Modeling and Machine Learning Using Tidyverse Principles.</em> <a href="https://www.tidymodels.org" class="uri">https://www.tidymodels.org</a>.
</div>
<div id="ref-lin2020deepcropnet" class="csl-entry" role="doc-biblioentry">
Lin, Tao, Renhai Zhong, Yudi Wang, Jinfan Xu, Hao Jiang, Jialu Xu, Yibin Ying, Luis Rodriguez, KC Ting, and Haifeng Li. 2020. âDeepCropNet: A Deep Spatial-Temporal Learning Framework for County-Level Corn Yield Estimation.â <em>Environmental Research Letters</em> 15 (3): 034016.
</div>
<div id="ref-porter1993climatic" class="csl-entry" role="doc-biblioentry">
Porter, John R, and Martin H Parry. 1993. âClimatic Variability and the Modelling of Crop Yields.â <em>Agricultural and Forest Meteorology</em> 63: 97â132.
</div>
<div id="ref-shahhosseiniForecastingCornYield2020" class="csl-entry" role="doc-biblioentry">
Shahhosseini, Mohsen, Guiping Hu, and Sotirios V. Archontoulis. 2020. âForecasting Corn Yield With Machine Learning Ensembles.â <em>Frontiers in Plant Science</em> 11 (July): 1120. <a href="https://doi.org/10.3389/fpls.2020.01120" class="uri">https://doi.org/10.3389/fpls.2020.01120</a>.
</div>
<div id="ref-sunCountyLevelSoybeanYield2019" class="csl-entry" role="doc-biblioentry">
Sun, Jie, Liping Di, Ziheng Sun, Yonglin Shen, and Zulong Lai. 2019. âCounty-Level Soybean Yield Prediction Using Deep CNN-LSTM Model.â <em>Sensors</em> 19 (20): 4363. <a href="https://doi.org/10.3390/s19204363" class="uri">https://doi.org/10.3390/s19204363</a>.
</div>
<div id="ref-yu2022climate" class="csl-entry" role="doc-biblioentry">
Yu, Jina, David A Hennessy, Jesse Tack, and Felicia Wu. 2022. âClimate Change Will Increase Aflatoxin Presence in US Corn.â <em>Environmental Research Letters</em> 17 (5): 054017.
</div>
</div>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Considering only the years included in the analysis.<a href="#fnref1" class="footnote-back" role="doc-backlink">â©ï¸</a></p></li>
<li id="fn2"><p>We ran into a number of technical issues using xgboost, which ultimately led us to abandon its use. As the model created its prediction for the analysis set, a <a href="https://github.com/dmlc/xgboost/issues/5935">reported computational bug</a> occured that we could solve only with manipulation of the data. Even if this change may not affect the final prediction, it could introduce bias and compromise the modelâs accuracy. Another issue was related to its low performance in extrapolating when a value in the assessment set is out of the range of the analysis set. To address these issues, we decided to exclude xgboost from our model stack and continued with the remaining five models.<a href="#fnref2" class="footnote-back" role="doc-backlink">â©ï¸</a></p></li>
<li id="fn3"><p>Computed in parallel with 9 cores of M1 Pro CPU.<a href="#fnref3" class="footnote-back" role="doc-backlink">â©ï¸</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>