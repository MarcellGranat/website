[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Marcell’s blog2",
    "section": "",
    "text": "From John von Neumann to the ChatGPT: the rise, shine and shine of computer science\n\n\n\nEconomania\n\n\nNLP\n\n\n\n\n\n\n\n\n\n\nMar 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeb scraping with Shiny - automate your weekly food order\n\n\n\n\n\n\n\n\n\nMar 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggProfessional\n\n\n\nPackage\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurrr\n\n\n\nPackage\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplication of NLP to reduce the unemployment\n\n\n\nNLP\n\n\n\n\n\n\n\n\n\n\nJan 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDigitalisation and business performance - focusing on Hungarian manufacturing firms\n\n\n\nDigitalisation\n\n\nMCA\n\n\n\n\n\n\n\n\n\n\nJan 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRising prices, rising returns: how does inflation affect stock returns?\n\n\n\nEconomania\n\n\nInflation\n\n\n\n\n\n\n\n\n\n\nJan 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedia Attention to Environmental Issues and ESG Investing\n\n\n\n\n\n\n\n\n\nDec 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis in Practice\n\n\n\nUsed cars\n\n\nML\n\n\nMCA\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow long does it take to sell a used car?\n\n\n\nUsed cars\n\n\n\n\n\n\n\n\n\n\nOct 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantify the Economania Blog\n\n\n\nEconomania\n\n\nNLP\n\n\n\n\n\n\n\n\n\n\nSep 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan we measure inflation just with our laptop?\n\n\n\nEconomania\n\n\nInflation\n\n\n\n\n\n\n\n\n\n\nAug 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBest Paper Award\n\n\n\n\n\n\n\n\n\nMar 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAT WARS\n\n\n\n\n\n\n\n\n\nJan 27, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "I am an economic analyst, researcher and assistant lecturer at John von Neumann University. I love statistics, econometrics, ML and mostly R pogramming. This website is maintained to share lecture slides, materials related to my subject and blog posts about my actual research activities."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About me",
    "section": "Experience",
    "text": "Experience\nCentral Bank of Hungary| Education and Research Expert | February 2022 - present\nJohn von Neumann University Teaching Assistant | August 2022 - present"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nEötvös Loránd University | Budapest, HU PhD in Economics | Sept 2022 -present\nCorvinus University of Budapest | Budapest\nMsC in Economics | Sept 2017- June 2022"
  },
  {
    "objectID": "about.html#education-fa-brands-bluetooth",
    "href": "about.html#education-fa-brands-bluetooth",
    "title": "About me",
    "section": "Education ",
    "text": "Education \nEötvös Loránd University | Budapest, HU\nPhD in Economics | Sept 2022 -present\nCorvinus University of Budapest | Budapest\nMsC in Economics | Sept 2017- June 2022"
  },
  {
    "objectID": "about.html#education-fa-graduation-cap",
    "href": "about.html#education-fa-graduation-cap",
    "title": "About me",
    "section": "Education ",
    "text": "Education \nEötvös Loránd University\nPhD in Economics | Sept 2022 -present\nCorvinus University of Budapest\nMsC in Economics | Sept 2017- June 2022"
  },
  {
    "objectID": "about.html#experience-fa-briefcase",
    "href": "about.html#experience-fa-briefcase",
    "title": "About me",
    "section": "Experience ",
    "text": "Experience \nCentral Bank of Hungary\nEducation and Research Expert | February 2022 - present\nJohn von Neumann University\nTeaching Assistant | August 2022 - present"
  },
  {
    "objectID": "posts/analysis-in-practice/index.html",
    "href": "posts/analysis-in-practice/index.html",
    "title": "Data Analysis in Practice",
    "section": "",
    "text": "I had the honor of being invited to present one our current research at the “Data Analysis in Practice” series.\nThis time, the focus was the used car market, but I was able to present our research to interested visitors with a detailed description of the methodology. We are extremely proud of this study because it combines the modern machine learning tools and survival analysis in a complex way to answer a research question that is important to everyone.\n\n\n\n\n\n\n\n\nThe enclosed slideshow helps you understand the intuition behind the models and why each methodological step is justified."
  },
  {
    "objectID": "posts/whiteshield/index.html",
    "href": "posts/whiteshield/index.html",
    "title": "Application of NLP to reduce the unemployment",
    "section": "",
    "text": "My friend Bálint and I participated in and won the 2022 Global Data Science Competition organized by Whiteshield.\nOur task was to build an NLP algorithm that finds the most relevant job advertisements for the registered unemployed.\nAn effective pairing in the model we put together is based on 3 pillars. The first one is that the field of the individual’s educational background (major) should match the job description. We applied BERT-model (Devlin et al. - 2019) to sort the jobs into ISCO3 categories (like Administration professionals or Software and applications developers) so we can merge the job posts with the possible employees since their major studies are given in the database.\nWe now created 132 subgroups of the population, but not each job may be relevant for the individuals. We extracted several requirements (with numerous regex filters to identify them) for the jobs from the given descriptions united with the pre-structured information set, such as required degree, experience or age.\nBut in most cases, we could suggest a bunch of vacancies to an individual so we also calculated the distance between the residence of the possible employee and the possible workplace, and arrange them so that we prefer matches where the distance is shorter (so we also used spatial manipulation functions).\nA bonus challenge was to create a visual representation that compares the universe of skills available among the unemployed to the universe of skills required by the job posts. We used here a Structural Topic Model to identify the terms appearing in the description of job categories where the number of vacancies is substantially higher than the number of unemployed (with related majors). The visualisations are presented via Flexdashboard."
  },
  {
    "objectID": "posts/whiteshield/index.html#references",
    "href": "posts/whiteshield/index.html#references",
    "title": "Application of NLP to reduce the unemployment",
    "section": "References",
    "text": "References\n\nDevlin, J., M. Chang, K. Lee, et al. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. DOI: 10.48550/arXiv.1810.04805."
  },
  {
    "objectID": "posts/mca-digitalisation/index.html",
    "href": "posts/mca-digitalisation/index.html",
    "title": "Digitalisation and business performance - focusing on Hungarian manufacturing firms",
    "section": "",
    "text": "Our recent article in the Economic Review investigates the relationship between ICT usage and the profitability of Hungarian manufacturing firms.\nMy contribution to the final paper is the data analysis part. We applied Multiple Correspondence Analysis (MCA) as dimension reduction, which can be applied to categorical variables rather than numerical ones. This methodology was suited to our problem, as all of ICT characteristics of the firms are represented by dummy or categorical variables."
  },
  {
    "objectID": "posts/mca-digitalisation/index.html#abstract",
    "href": "posts/mca-digitalisation/index.html#abstract",
    "title": "Digitalisation and business performance - focusing on Hungarian manufacturing firms",
    "section": "Abstract",
    "text": "Abstract\nDigitalisation and Industry 4.0, which represents a cutting-edge dimension in manufacturing is now spreading all over the world. Nowadays, both managers and academics expect that digitalization is the main source of business performance improvement - and the first empirical results proved their positive relationship. The aim of our paper is to examine this relationship by merging two databases - a) Hungarian manufacturing firms data from the ICT usage in enterprise survey managed by Eurostat and b) a database containing the balance sheets and profit and loss statements of Hungarian firms. There are several unique characteristics to our work, e.g., our sample size consists of approx. 1/3rd of the entire population, our performance assessments rely on report-based data instead of perceptual items, finally, we analyse many digital tools and performance indicators. Our results indicate that the prevalence of digital tools in Hungarian manufacturing firms is not at all as widespread as it is usually suggested in the literature. Digitalisation has positive relationships with performance measures are not evident, at least for the selected period. Although, we have concluded that there is a group of companies in which progress in complex digital programs is part of daily business practice, however, the performance implications are blurred even in this group.\nThe article is accessible in Hungarian."
  },
  {
    "objectID": "posts/ggprofessional/index.html",
    "href": "posts/ggprofessional/index.html",
    "title": "ggProfessional",
    "section": "",
    "text": "This package is helpful for those who regularly use the {ggplot2} package to perform their daily institutional tasks."
  },
  {
    "objectID": "posts/ggprofessional/index.html#overview",
    "href": "posts/ggprofessional/index.html#overview",
    "title": "ggProfessional",
    "section": "Overview",
    "text": "Overview\nBased on my several years of experience in the research, the private and the public sector, this package is helpful for those who regularly use the {ggplot2} package to perform their daily institutional tasks.\nggProfessional helps you with:\n\nexporting all your plots\nmanage themes and palettes\nadd your logo or watermark to the figures"
  },
  {
    "objectID": "posts/ggprofessional/index.html#installation",
    "href": "posts/ggprofessional/index.html#installation",
    "title": "ggProfessional",
    "section": "Installation",
    "text": "Installation\nggProfessional is not available in the CRAN, since it does not satisfy the requirements (saves into your wd, creates global variables to work with gg elements), and it is built on packages that are not available on CRAN.\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"MarcellGranat/ggProfessional\")"
  },
  {
    "objectID": "posts/ggprofessional/index.html#usage",
    "href": "posts/ggprofessional/index.html#usage",
    "title": "ggProfessional",
    "section": "Usage",
    "text": "Usage\n\nExport\nAs you load the package, two functions are immediately assigned to your global environment: - & .gg_finalise (hidden).\n\nlibrary(ggProfessional)\n\n\nls()\n\n[1] \"-.gg\"\n\n\n-.gg is taken from the following SW solution. It allows you to use a function on a gg object, without assigning in. I recommend you to add - .gg_finalise to all your gg code blocks at the end, by default it will return the plot, without any modification, but if you later update the .gg_finalise function, you can specify your saving method, and it will be used to all your figures.\nCombine this with the plot_to functions from the package, helps you in the situations when you have to share your figures also in ppt/excel/csv.\n\nlibrary(ggplot2)\n\nggplot(iris, aes(Sepal.Length, Petal.Width)) + \n  geom_point() - # <\n  .gg_finalise # no brackets!\n\n\n\n\n\n\n\n\n\n.gg_finalise <<- function(plot = ggplot2::last_plot()) {\n  plot_to_ppt(plot) # save all plots into one ppt file\n  plot_to_excel(plot) # save all plots into one excel file (as table)\n  plot_to_csv(plot) # save all plots into separate csv files (as table)\n  plot\n  }\n\n\n\n\n\n\n\n\nManage themes and palettes\nSet theme for ggplot figures was already built in the {ggplot2} package. In this field I can only extend that with the register_theme/available_themes/get_theme functions. This help you to save your theme and reuse that in all of your projects.\n\nregister_theme(ggplot2::theme_bw() + theme(title = element_text(color = \"red\")), name = \"rbw\")\nregister_theme(ggplot2::theme_minimal() + theme(title = element_text(color = \"green\")), name = \"gminimal\")\n\n\navailable_themes()\n\n\n\n\n\n\n\n\n\n\n\nSimilarly you have these functions for palettes. But set_palette will create a .co hidden function that helps you to easily use your own palette (e.g. .co(1:2) returns the first two colors in your palette, but you can also use named colors.). set_palette can also register the colors as default.\n\nregister_palette(c(\"red1\", \"#0c4853\", \"steelblue\", \"blue2\", \"green\", \"yellow\", \"#FF6F91\"), name = \"first\")\nregister_palette(c(\"blue\", \"purple\", \"cyan\", \"blue2\", \"orange\", \"yellow\", \"#FF6F91\"), name = \"sec\")\n\n\navailable_palettes()\n\n\n\n\n\n\n\n\n\nset_palette(\"first\")\n\n\n.co(1:3)\n\n[1] \"red1\"      \"#0c4853\"   \"steelblue\"\n\n\nNote: If you set attach = TRUE, then colors on all ggplot figures will be automatically updates with this palette.\n\n\n\n\n\nLogos\nYou can register your institutional logos (saved only locally) and reuse in every project.\n\nregister_logo(path = \"logo.png\", \"ggp\") # example\n\n[1] TRUE\n\nggplot(iris, aes(Sepal.Length, Petal.Width)) + \n  geom_point() +\n  watermark_logo(\"ggp\", alpha = .2)\n\n\n\n\n\n\n\n\n\nggplot(iris, aes(Sepal.Length, Petal.Width)) + \n  geom_point() -\n  corner_logo"
  },
  {
    "objectID": "posts/neumann/index.html",
    "href": "posts/neumann/index.html",
    "title": "From John von Neumann to the ChatGPT: the rise, shine and shine of computer science",
    "section": "",
    "text": "Neumann’s work was indispensable for the development of computer technology and its current level. An example of this is the ChatGPT, what does unbeatable in what it’s supposed to do: generate text.\nOriginal blog post (Hungarian) available at: Economania\nJohn von Neumann mainly conducted research in the fields of mathematics, natural sciences and IT. In the 1930s, he participated in the development of mathematical logic and the planning of the first steps in computer technology. He was the first to give an idea for the development of the basic principles of computer technology and as a participant in the first computer, the “Manhattan Project”, he participated in its design and construction.\nBased on Neumann’s vision, the first computers worked on the principle of programmability, which allowed them to perform multiple tasks without having to rebuild them. He was also the first to write about memory and how it works, which was the basis for the development of computers.\nNeumann’s work was indispensable for the development of computer technology and its current level. His memory will remain forever in the history of computer technology as one of the most important founders and creators of the basic principles of computers.\nThe previous 3 paragraphs were produced by the ChatGPT, which is a good example of the applicability of the latest achievements in computer technology. Even after many weeks of active research, I probably couldn’t summarise Neumann’s importance more accurately in a blog post, the “artificial intelligence” (as many people refer to it) that currently dominates the public discourse can do it all in just seconds. But why is it so effective and what else can it do?\nChatGPT is based on a transformer architecture, the purpose of which is to form a dialogue with its user. More specifically, the GPT (Generative Pre-trained Transformer) itself means that it predicts the appropriateness of the next token (a piece of text) based on previously learned samples from a database. The model was built by having a mixture of a training database and sample answers from human-generated dialogues, and then also humans taught it about which answers they like (OpenAi, 2023).\nThe outcome is an interface that generates dialogues that immediately pushed the popular digital frenzies of recent years into the background in the field of Internet searches (see the attached figure). Many people may have tried it in order to admire the new achievement of technology, but many forums have already mentioned (mainly in an ominous tone) how ChatGPT can transform our lives. Is this the technology that will really take away the jobs of “white-collar” workers?\nIn response to these, memes were immediately born: “ChatGPT could only replace the programmers if the clients could tell us what they wanted. We are safe for now.” The truth lies somewhere in between.\nChatGPT does unbeatable in what it’s supposed to do: generate text. But the creators also draw attention to the fact that the answer is always generated in such a way that it seems to be true, while it is often not even close to the truth (OpenAi, 2023). The reason for this is that during the training, it solves the task of selecting the most accepted text, but there is no training database that labelled which information is correct and which is not, so it cannot check the validity of the information. Accordingly, it cannot compete in areas where a real understanding of a given topic is required.\nHow can ChatGPT and its “relatives” transform our lives in the short run? The weights of the tasks of those performing intellectual work will definitely be reevaluated. The importance of drafting an official e-mail or preparing low-priority summaries is reduced, but those who can properly integrate these technologies into their complex and new knowledge-creating work can achieve many times more efficiency. However, it is certain that a model created to generate text will not wake up to self-awareness tomorrow and tame humanity. At most, it will write us another one based on the science fiction stories in the training database."
  },
  {
    "objectID": "posts/neumann/index.html#references",
    "href": "posts/neumann/index.html#references",
    "title": "From John von Neumann to the ChatGPT: the rise, shine and shine of computer science",
    "section": "References",
    "text": "References\n\nOpenAi (2023). Introducing ChatGPT. URL: https://openai.com/blog/chatgpt (visited on Mar. 6, 2023)."
  },
  {
    "objectID": "posts/interfood/index.html",
    "href": "posts/interfood/index.html",
    "title": "Web scraping with Shiny - automate your weekly food order",
    "section": "",
    "text": "Simple, but useful application of static web scraping in a shiny web application to find your favorites in the menu.\n\nWeb scraping with Shiny - automate your weekly food order \nWeb scraping is a technology that can be used to automatically download data from online pages. It can be used in many areas: collecting data for research, monitoring prices, and automatically reporting any regularly updated online data source. In this fun project, I give an example of the latter.\nThe menu of Interfood is immense and I usually like to get the weekly order done quickly, so I built a small shiny application for it (Do not laugh, pancakes are really hard to find). Shiny is an application written in R (or Python) that can be stored on an online server, the biggest advantage of which is that it is incredibly easy to create.\nThe application works as follows: when you open it, it visits the Interfood’s menu for the given week and downloads the entire offer.\n\nlibrary(rvest)\n\nread_html(\"https://www.interfood.hu/etlap-es-rendeles/\") |> # download the entire website\n    html_nodes(paste0(\".cell:nth-child(1) .food-etlap-szoveg\")) |> # 1st day\n    html_text() |> # format to text\n    head() # first 6 item\n\n[1] \"Frankfurti leves\"            \"Frankfurti leves\"           \n[3] \"Zöldborsóleves V\"            \"Zöldborsóleves V\"           \n[5] \"Tejszínes cseresznyeleves V\" \"Tejszínes cseresznyeleves V\"\n\n\nAfter a few cleaning steps, the entire menu is loaded, as if reading it on the web page. The next function of the application is that we can specify which dishes we have rated and which days we would like to order. The Shiny then simply collects the dishes with the highest ratings for the given days and visually displays the recommended order.\nThis is not a serious data analysis task, but an excellent example of how an application that can be written in seconds can help us in many areas of life and how rewarding coding can be. The full code is available in the follwing GitHub repository just as the link to the app."
  },
  {
    "objectID": "posts/economania-textmining/index.html",
    "href": "posts/economania-textmining/index.html",
    "title": "Quantify the Economania Blog",
    "section": "",
    "text": "TF-IDF to analyse the Economania blog.\n\n\n\n\nThe MNB Institute organized the opening event of the Economania professional workshop, and I also had the opportunity to present the results of text analysis on the blog posts.\n\n\n\n\nMy presentation was about the analysis of blog posts, based on TF-IDF indicators. TF-IDF is the frequency of the term (word) adjusted for how rarely it appears in the collection of documents (aggregated to years and authors in this case). This decreases the weight of the commonly used words and highlights the terms related only to that specific year or author.\n\n\n\n\n\nTo carry out the analysis, I collected the individual blog posts myself using the web scraping method and cleaned the data, the code is available at the following link: https://github.com/MarcellGranat/economania-textmining/blob/main/scrape.R"
  },
  {
    "objectID": "posts/statwars/index.html",
    "href": "posts/statwars/index.html",
    "title": "STAT WARS",
    "section": "",
    "text": "With my friend Bálint, we successfully won the 2021 “STAT WARS”, due to which we were also featured in a morning TV show and in newspapers\nIn 2021, the Central Statistical Office announced its data analysis competition for MSc students. Since my friend Balint Mazzag and I successfully won this, we had the opportunity to talk about our experiences and the importance of statistical knowledge in the Hungarian TV2 television channel’s morning show. Also thanks to this, we gave an interview to Figyelő.\n\nSource: TV2\n\n\n\n\n\n\n\n\n\nSource: LinkedIn"
  },
  {
    "objectID": "posts/online-inflation/index.html",
    "href": "posts/online-inflation/index.html",
    "title": "Can we measure inflation just with our laptop?",
    "section": "",
    "text": "Between 2008 and 2019, the proportion of regular online shoppers doubled in the EU, and COVID-19 further increased the frequency of online purchases. The prices available online are thus becoming more and more important and they are representative of the inflation perceived by the population.\nOriginal blog post (Hungarian) available at: Economania\nA significant part of the tasks of economist researchers and analysts is often to search for data collected by others. With the help of data, we get to know the world and can draw correct conclusions about the state of the economy. Today’s modern data collection methods (“Big Data”) thus represent a significant contribution to the expansion of our knowledge about the economy.\nThere are several advantages of monitoring prices online compared to the classic methodology. The most obvious is that it is cheaper and faster. The traditional way of measuring inflation, on the other hand, is to have a large number of skilled workers visit hundreds of stores and record the prices of products in a predetermined basket every month.\nObserving prices online is much simpler than that. With the help of different “scraping” software, we can monitor the evolution of prices on thousands of websites even without in-depth knowledge of IT. With a simple laptop and wired internet, we can visit up to 100,000 pages per hour and collect the prices in a structured format. We must not forget the time spent on data cleaning, but there is a difference in magnitude between the costs of online and offline measurement. There is no serious cost to download the prices every day and see their changes with almost no delay. This is crucial for both academic researchers and policymakers.\nMoreover, the benefits mentioned so far have increased in value many times over during the pandemic. Curfews prevented manual price monitoring, and online estimates preceded the official data release by 30 days (Jaworski, 2021).\nThe possibility of monitoring from a distance has already been extremely useful. Argentina’s official inflation statistics were heavily criticized between 2007 and 2015, but local economists were prevented from collecting the data independently. The Argentine government reported average inflation of 8 percent between 2007 and 2011, while online price increases suggest that it was actually around 20 percent. The online and official value, on the other hand, was the same for the other Latin American countries (Cavallo, 2013). Data falsification has also become a serious political issue in Argentina.\nAnother advantage of online data collection is that it can be done with the same methodology for all countries and the price level of the same products can be analyzed for different countries (similar to the famous Big Mac index). This represents a huge opportunity to analyze the topic of purchasing power parity (Cavallo, 2016).\nOf course, there is a reason that online measurement has not yet replaced traditional inflation calculation. It is not possible, or very difficult, to observe all areas of the economy online. Many services and the price of housing (Cavallo, 2022), for example, cannot be obtained in this way. It is also important to compile our estimate based on a sufficiently large and representative sample of observed traders, as their heterogeneity can also significantly distort (Cavallo, 2017). For example, the pioneering project of online measurement, the Billion Prices Project, can cover a total of 60-70 percent of the consumer basket per country (Cavallo, 2016). In this area, COVID-19 brought progress, as the advertisement of many services became available online (Cavallo, 2022). Another disadvantage of monitoring on the Internet is that we do not have information on the quantities sold, so the weights calculated by the statistical offices are still needed to weigh the products.\nCan the index produced in this way be used to estimate the consumer price index? In a large-scale project, Cavallo (2017) empirically investigated whether inflation measured online leads to significantly different results than inflation measured offline. To do this, he simultaneously monitored the pricing of commercial stores that sell both online and offline. All this by using a phone application to take pictures of the prices available in the store (10-50 products) and send them for analysis, while the online websites of the stores were monitored daily using the “scraping” method.\nWhen comparing a given product in a given store, the online and offline prices were the same in 72 percent of the cases, although this varied by country and sector (the degree of agreement was greatest in the case of clothing and electronic devices).\nThe size and extent of the price changes measured online and offline do not statistically differ significantly from each other, but they do not move completely together over time. Online prices react more quickly to shocks, so they can even be a good predictor of inflation trends.\nThe relevance of the cited research is constantly increasing due to the increase in the proportion of online purchases. From 2008 to 2019, the proportion of regular online shoppers doubled in the EU, and COVID-19 further increased the frequency of online shopping. The prices available online are thus becoming more and more important and representative of the inflation perceived by the population.\nMonitoring prices online is only one possible use of modern Big Data tools. These technologies allow economists not to consider the data as “given”, but to be able to produce and immediately analyze databases suitable for research."
  },
  {
    "objectID": "posts/online-inflation/index.html#references",
    "href": "posts/online-inflation/index.html#references",
    "title": "Can we measure inflation just with our laptop?",
    "section": "References",
    "text": "References\n\nCavallo, A. (2013). “Online and official price indexes: Measuring Argentina’s inflation”. In: Journal of Monetary Economics 60.2, pp. 152–165. ISSN: 03043932. DOI: 10.1016/j.jmoneco.2012.10.002. URL: https://linkinghub.elsevier.com/retrieve/pii/S0304393212000967.\n\n\n— (2017). “Are Online and Offline Prices Similar? Evidence from Large Multi-Channel Retailers”. In: American Economic Review 107.1, pp. 283–303. ISSN: 0002-8282. DOI: 10.1257/aer.20160542.\n\n\n— (2022). “Big Data, Covid inflation and Stockouts”. Big Data and New Tools to Track infaltion in Real-Time.\n\n\nCavallo, A. and R. Rigobon (2016). “The Billion Prices Project: Using Online Prices for Measurement and Research”. In: Journal of Economic Perspectives 30.2, pp. 151–178. ISSN: 0895-3309. DOI: 10.1257/jep.30.2.151. URL: https://pubs.aeaweb.org/doi/10.1257/jep.30.2.151.\n\n\nJaworski, K. (2021). “Measuring food inflation during the COVID-19 pandemic in real time using online data: a case study of Poland”. In: British Food Journal."
  },
  {
    "objectID": "posts/inflation-and-stocks/index.html",
    "href": "posts/inflation-and-stocks/index.html",
    "title": "Rising prices, rising returns: how does inflation affect stock returns?",
    "section": "",
    "text": "We have to consider several factors when making investment decisions, but one thing for sure: how will inflation change?\nOriginal blog post (Hungarian) available at: Economania\nWe have to consider several factors when making investment decisions, but one thing for sure: how will inflation change? Our opinion about this is greatly influenced by what kind of news we receive about the current inflation. However, it is an unclear question in which direction the market changes in light of all this, and through which channels the inflation expectation influence the returns.\nThe value of a given investment is certainly significantly affected by future inflation, but there are several reasons for this. The same expected net payout will be less valuable with higher inflation, making it less valuable to invest, which is why we would expect a negative correlation between inflation and stock returns. However, it is worth keeping in mind that the value of a share is determined by the expected payments, which are generated by real assets, so it is believed that in the long term, the payments will increase along with inflation. Added to this, we can expect that there will be no relationship between inflation expectations and returns, but higher future inflation also represents a different risk. For example, the future interest rate policy of the central bank may become uncertain, which increases risks. On the other hand, due to rising inflation, the probability that the economy will fall into a deflationary trap decreases, which is positive news for investors. The question is how do investors react to these risks, i.e. how does the risk premium develop when inflation rises?\nThe article by Chaudharry and Marrow (2022) explores empirically the question of how stock prices react to the development of inflation expectations as a result of the aforementioned effects, and which channel plays a key role in the process. The article contains several novelties. First, it uses daily data. The daily development of inflation expectations is proxied by the difference between the yield of inflation-indexed and non-indexed government securities, which precisely gives the risk-neutral inflation expectation. Due to the low liquidity, the calculations are also performed with expectations derived from swap transactions, but their results did not show any significant deviation in any case.\nThe answer to the first question can be clarified relatively quickly: an increase of 1 basis point in 10-year inflation expectations is associated with an average increase of 11.2 basis points in stock returns on the same day, i.e., their price rises. But it is not only long-term expectations that matter: yields react somewhat even more strongly to shorter-term (5-year) expectations (11.4 basis points). With the exception of the 2008 crisis, this result can be considered stable since the 2000s, and stock returns responded to the increase in inflation to varying degrees, but always with an increase. Furthermore, the result is also robust in a cross-section: a positive correlation can be observed between the increase in inflation and stock returns in all industries.\nThe second question of the study is to what extent individual channels play a role in the effect of inflation on returns. To do this, the authors use observations on days when new inflation data is published by the Bureau of Labor Statistics. The current inflation data certainly has a substantial influence on inflation expectations (the variability of changes in inflation expectations is significantly higher on these days), but it does not contain any other fundamental information. The results described above do not change substantially, and the estimated effects are not followed by a correction later, so it can be stated that the current inflation indicator is treated as fundamental information by the market.\nThe authors examine the mechanism of action with a regression path analysis. First, a bivariate model is used to estimate the impact of inflation expectations on stock returns. In the second step, they also control for changes in real returns, thus extracting the effect of cash flow and the risk premium. After that, they also control for the expected increase in dividends, with which the authors get the effect of the risk premium. By comparing the three effects, it can be determined which channel plays a role in the evolution of share prices. According to the results obtained in this way, the increase in stock returns is caused by 2.6 percent of the real return, 15.6 percent by the cash flow and 81.8 percent by the risk premium.\nWhen the inflation data is published, opinions about inflation expectations change strongly, as a result of which share prices rise. And the main source of this is none other than the decreasing risk premium, which results from the fact that the probability of falling into a deflationary trap decreases. However, since the deflation risk - for the time being - seems to be a thing of the past, it is quite possible that the relationship between inflation prospects and stock returns will change again."
  },
  {
    "objectID": "posts/inflation-and-stocks/index.html#references",
    "href": "posts/inflation-and-stocks/index.html#references",
    "title": "Rising prices, rising returns: how does inflation affect stock returns?",
    "section": "References",
    "text": "References\n\nChaudhary, M. and B. Marrow (2022). Inflation Expectations and Stock Returns. SSRN Scholarly Paper 4154564. Rochester, NY: Social Science Research Network. DOI: 10.2139/ssrn.4154564. URL: https://papers.ssrn.com/abstract=4154564 (visited on Aug. 28, 2022)."
  },
  {
    "objectID": "posts/media-esg/index.html",
    "href": "posts/media-esg/index.html",
    "title": "Media Attention to Environmental Issues and ESG Investing",
    "section": "",
    "text": "Our recent article in the Financial and Economic Review investigates the relationship between the future yield of ESG investment and the news in the media.\n\n\n\n\n\nWe analyse how ESG scores affect future returns when environmental issues receive higher media coverage. Investors might take environmental aspects into account if they are confronted with the issue of global warming more frequently in the press. We assess the prevalence of environmental issues in the media with a machine learning-based Structural Topic Modelling (STM) methodology, using a news archive published in the USA. Running Fama-MacBeth regressions, we find that in periods when the media actively report on environmental issues, ESG scores have a significant negative impact on future returns, whereas, in months when fewer such articles are published, investors do not take sustainability measures into account, and ESG scores have no explanatory power.\nThe article is accessible in English and Hungarian."
  },
  {
    "objectID": "posts/currr/index.html",
    "href": "posts/currr/index.html",
    "title": "Currr",
    "section": "",
    "text": "I’ve recently developed a new R package called `currr` (checkpoints & purrr). It manages time-consuming iterations, parallel computing and multitasking."
  },
  {
    "objectID": "posts/portfolio-used-cars/index.html",
    "href": "posts/portfolio-used-cars/index.html",
    "title": "How long does it take to sell a used car?",
    "section": "",
    "text": "Buying a car is a major item in the household consumption basket, so participants in this market make their decisions based on carefully considered arguments. We build a survival model based on a large dataset of used car ads, revealing what makes a used car liquid (easy to sell).\nOriginal blog post (Hungarian) available at Portfolio.hu\nA car is one of the most expensive consumer goods in the life of an average household, so you have to cautiously consider what kind of vehicle you spend your money on. Everyone has their personal preferences and experiences with cars, but data-based empirical research in April sheds light on what is worth paying attention to.\nIt may be obvious that the price of a car are determined by comparing it with the price of other cars available on the market, but when buying, it is worth thinking about how easy it will be to find a buyer for it when we replace it.\nGranát Marcell, teaching assistant at Neumann János University, and Dr. Péter Vékás, associate professor at Budapest Corvinus University, investigated what determines the market value of a car and the time required to sell it, based on the database of advertisements appearing on the most trafficked website in Hungary. The authors examine the market based on the most modern machine learning and statistical methodology."
  },
  {
    "objectID": "posts/portfolio-used-cars/index.html#the-theoretical-consideration",
    "href": "posts/portfolio-used-cars/index.html#the-theoretical-consideration",
    "title": "How long does it take to sell a used car?",
    "section": "The theoretical consideration",
    "text": "The theoretical consideration\nCars are advertised on the Internet after a few years of use, so online advertisements can be excellent raw material for data-based analyses. The researchers collected data on available used vehicles every day for a year, so for each ad, the date of publication and sale is known, together with the information and price of the car. Based on these, the study shows what determines the price of a car and how pricing affects the expected time of sale."
  },
  {
    "objectID": "posts/portfolio-used-cars/index.html#price-determination-using-a-modern-machine-learning-approach",
    "href": "posts/portfolio-used-cars/index.html#price-determination-using-a-modern-machine-learning-approach",
    "title": "How long does it take to sell a used car?",
    "section": "Price determination using a modern machine learning approach",
    "text": "Price determination using a modern machine learning approach\nPrice of a car depends on many details: mileage, year of manufacture, engine performance or fuel type. The author apply a number of models to estimate how much a given car would cost based on the offer price and characteristics of other cars available on the market. The value of each vehicle is estimated based on hundreds of basic information from a total of 613,604 car ads. And for this, they used modern machine learning techniques that occur in international forecasting championships, such as neural networks. These models are able to estimate the value of cars within a margin of error of +/-3 percent. Since the estimate determine how much the given vehicle would cost based on the data of other cars available on the market, the deviation from the estimate can be interpreted as overpricing or underpricing compared to the market."
  },
  {
    "objectID": "posts/portfolio-used-cars/index.html#the-liquidity-of-cars",
    "href": "posts/portfolio-used-cars/index.html#the-liquidity-of-cars",
    "title": "How long does it take to sell a used car?",
    "section": "The liquidity of cars",
    "text": "The liquidity of cars\nLater, when we want to sell our purchased car, we have to consider how liquid our vehicle is, i.e. in how much time and at what loss we are willing to part with it. The authors apply a so-called survival model, in which the sale time is estimated based on the degree of underpricing and overpricing and the characteristics of the car.\nThe results can easily be illustrated with median selling time. This is 14 days for all the cars listed on the website, which means that, on average, half of the used cars on the market are sold in this time."
  },
  {
    "objectID": "posts/portfolio-used-cars/index.html#what-is-worth-paying-attention-to",
    "href": "posts/portfolio-used-cars/index.html#what-is-worth-paying-attention-to",
    "title": "How long does it take to sell a used car?",
    "section": "What is worth paying attention to?",
    "text": "What is worth paying attention to?\nAn important question is which properties have a significant influence on the selling time. As expected, the most important factor is the degree of overpricing or underpricing relative to the market. The figure below shows how our chances of selling our car change over a certain period of time, depending on how much we price it below or above the market.\n\n\n\n\n\nHowever, this does not cause a significant change in the case of minor differences: to shorten the expected sales time by 1 day, you have to offer it 4 percent below the market, and if you offer an average car at a 7 percent higher price, even then, the sales time increases by only one day on average. However, it can be a problem if we want to sell an American model, because in this case the median sales time is 10 days longer. It’s even harder to sell two-door models and display pieces. In the case of the latter, it typically takes 33 days until someone finds a buyer. Based on the data, the jack output can significantly reduce the sales time, although in this case it is worth considering that such a car may have additional attractive features for young people, which cannot be filtered out from the data. It further improves the saleability if the car is already registered in Hungary, or if the vehicle is equipped with additonal headlights."
  },
  {
    "objectID": "posts/currr/index.html#overview",
    "href": "posts/currr/index.html#overview",
    "title": "Currr",
    "section": "Overview",
    "text": "Overview\n\nA long journey is best broken into small steps, and the importance of taking a rest must never be underestimated.\n\nThe currr package is a wrapper for the purrr::map() family but extends the iteration process with a certain number of checkpoints (currr = checkpoints + purrr), where the evaluated results are saved, and we can always restart from there.\n\n\n\n\n\nImplementations of the family of map() functions with a frequent saving of the intermediate results. The contained functions let you start the evaluation of the iterations where you stopped (reading the already evaluated ones from the cache), and work with the currently evaluated iterations while the remaining ones are running in a background job. Parallel computing is also easier with the workers parameter."
  },
  {
    "objectID": "posts/currr/index.html#installation",
    "href": "posts/currr/index.html#installation",
    "title": "Currr",
    "section": "Installation",
    "text": "Installation\n\ninstall.packages(\"currr\")"
  },
  {
    "objectID": "posts/currr/index.html#usage",
    "href": "posts/currr/index.html#usage",
    "title": "Currr",
    "section": "Usage",
    "text": "Usage\nThe following example uses currr to present an everyday issue: run a time-demanding iteration, but you want to rerun it again.\n\nlibrary(tidyverse)\nlibrary(currr)\n\noptions(currr.folder = \".currr\", currr.wait = Inf)\n# folder in your wd, where to save cache data\n\navg_n <- function(.data, .col, x) {\n  # meaningless function that takes about 1 sec\n  Sys.sleep(1)\n\n  .data |>\n    dplyr::pull({{ .col }}) |>\n    (\\(m) mean(m) * x) ()\n}\n\n\nCheckpoints\n\ntictoc::tic(msg = \"First evaluation\")\n\ncp_map(.x = 1:50, .f = avg_n, .data = iris,\n       .col = Sepal.Length,\n       name = \"iris_mean\", cp_options = ) |>\n  head(3)\n\n|██████████████████████████████████████████████████ | 2% ETA:  50 sec                         \n|██████████████████████████████████████████████████ | 4% ETA:  49 sec                         \n|██████████████████████████████████████████████████ | 6% ETA:  48 sec                         \n|██████████████████████████████████████████████████ | 8% ETA:  47 sec                         \n|██████████████████████████████████████████████████ | 10% ETA:  46 sec                         \n|██████████████████████████████████████████████████ | 12% ETA:  45 sec                         \n|██████████████████████████████████████████████████ | 14% ETA:  44 sec                         \n|██████████████████████████████████████████████████ | 16% ETA:  43 sec                         \n|██████████████████████████████████████████████████ | 18% ETA:  42 sec                         \n|██████████████████████████████████████████████████ | 20% ETA:  41 sec                         \n|██████████████████████████████████████████████████ | 22% ETA:  40 sec                         \n|██████████████████████████████████████████████████ | 24% ETA:  39 sec                         \n|██████████████████████████████████████████████████ | 26% ETA:  38 sec                         \n|██████████████████████████████████████████████████ | 28% ETA:  37 sec                         \n|██████████████████████████████████████████████████ | 30% ETA:  36 sec                         \n|██████████████████████████████████████████████████ | 32% ETA:  35 sec                         \n|██████████████████████████████████████████████████ | 34% ETA:  34 sec                         \n|██████████████████████████████████████████████████ | 36% ETA:  33 sec                         \n|██████████████████████████████████████████████████ | 38% ETA:  32 sec                         \n|██████████████████████████████████████████████████ | 40% ETA:  31 sec                         \n|██████████████████████████████████████████████████ | 42% ETA:  30 sec                         \n|██████████████████████████████████████████████████ | 44% ETA:  29 sec                         \n|██████████████████████████████████████████████████ | 46% ETA:  28 sec                         \n|██████████████████████████████████████████████████ | 48% ETA:  27 sec                         \n|██████████████████████████████████████████████████ | 50% ETA:  26 sec                         \n|██████████████████████████████████████████████████ | 52% ETA:  25 sec                         \n|██████████████████████████████████████████████████ | 54% ETA:  24 sec                         \n|██████████████████████████████████████████████████ | 56% ETA:  23 sec                         \n|██████████████████████████████████████████████████ | 58% ETA:  22 sec                         \n|██████████████████████████████████████████████████ | 60% ETA:  21 sec                         \n|██████████████████████████████████████████████████ | 62% ETA:  20 sec                         \n|██████████████████████████████████████████████████ | 64% ETA:  19 sec                         \n|██████████████████████████████████████████████████ | 66% ETA:  18 sec                         \n|██████████████████████████████████████████████████ | 68% ETA:  17 sec                         \n|██████████████████████████████████████████████████ | 70% ETA:  16 sec                         \n|██████████████████████████████████████████████████ | 72% ETA:  15 sec                         \n|██████████████████████████████████████████████████ | 74% ETA:  14 sec                         \n|██████████████████████████████████████████████████ | 76% ETA:  13 sec                         \n|██████████████████████████████████████████████████ | 78% ETA:  12 sec                         \n|██████████████████████████████████████████████████ | 80% ETA:  11 sec                         \n|██████████████████████████████████████████████████ | 82% ETA:  10 sec                         \n|██████████████████████████████████████████████████ | 84% ETA:  9 sec                          \n|██████████████████████████████████████████████████ | 86% ETA:  8 sec                          \n|██████████████████████████████████████████████████ | 88% ETA:  7 sec                          \n|██████████████████████████████████████████████████ | 90% ETA:  6 sec                          \n|██████████████████████████████████████████████████ | 92% ETA:  5 sec                          \n|██████████████████████████████████████████████████ | 94% ETA:  4 sec                          \n|██████████████████████████████████████████████████ | 96% ETA:  3 sec                          \n|██████████████████████████████████████████████████ | 98% ETA:  2 sec                          \n|██████████████████████████████████████████████████ | 100% ETA:  0 sec                          \n\n\n[[1]]\n[1] 5.843333\n\n[[2]]\n[1] 11.68667\n\n[[3]]\n[1] 17.53\n\ntictoc::toc() # ~ 1:50 => 50 x 1 sec\n\nFirst evaluation: 52.782 sec elapsed\n\n\n\ntictoc::tic(msg = \"Second evaluation\")\n\ncp_map(.x = 1:50, .f = avg_n, .data = iris,\n       .col = Sepal.Length,\n       name = \"iris_mean\") |>\n  head(3)\n\n✓ Everything is unchanged. Reading cache.\n\n\n[[1]]\n[1] 5.843333\n\n[[2]]\n[1] 11.68667\n\n[[3]]\n[1] 17.53\n\ntictoc::toc() # ~ 0 sec\n\nSecond evaluation: 0.034 sec elapsed\n\n\nIf the .x input and .f are the same, then the 2nd time you call the function, it reads the outcome from the specified folder (.currr). Also if .x changes, but some of its part remain the same, then that part is taken from the previously saved results, and only the new elements of .x are called for evaluation. (If .f changes, then the process will start from zero.)\n\ntictoc::tic(msg = \"Partly modification\")\n\ncp_map(.x = 20:60, .f = avg_n, .data = iris,\n       .col = Sepal.Length,\n       name = \"iris_mean\") |>\n  head(3)\n\n⚠ .x has changed. Looking for mathcing result to save them as cache\n\n\n◌ Cache updated based on the new .x values\n\n\n|███████████████████████████████████████████████████████████████████████████████████████ | 2% ETA:  41 sec                         \n|█████████████████████████████████████████████████████████████████████████████████████ | 6% ETA:  40 sec                         \n|████████████████████████████████████████████████████████████████████████████████████ | 8% ETA:  39 sec                         \n|███████████████████████████████████████████████████████████████████████████████████ | 10% ETA:  38 sec                         \n|██████████████████████████████████████████████████████████████████████████████████ | 12% ETA:  37 sec                         \n|████████████████████████████████████████████████████████████████████████████████ | 16% ETA:  36 sec                         \n|███████████████████████████████████████████████████████████████████████████████ | 18% ETA:  35 sec                         \n|██████████████████████████████████████████████████████████████████████████████ | 20% ETA:  34 sec                         \n|█████████████████████████████████████████████████████████████████████████████ | 22% ETA:  33 sec                         \n|████████████████████████████████████████████████████████████████████████████ | 24% ETA:  32 sec                         \n\n\n[[1]]\n[1] 116.8667\n\n[[2]]\n[1] 122.71\n\n[[3]]\n[1] 128.5533\n\ntictoc::toc() # ~ 50:60 => 10 x 1 sec\n\nPartly modification: 10.384 sec elapsed\n\n\nYou can remove the cache files, if you want to reset the process (or remove the already unnecessary files from your folder).\n\n# only cache files for iris_mean\nremove_currr_cache(\"iris_mean\")\n\n# all cache files\nremove_currr_cache()\n\n\n\nBackground process\nThis is another functionality that makes currr to be cool. Working in RStudio you can set the wait parameter to 0-1/1+, define how many iterations you want to wait, and then let R work on the remaining iterations in the background, while you can work with the evaluated ones. If wait < 1, then it is interpreted as what proportion of the iterations you want to wait. Whenever you recall the function, it will return the already evaluated ones (use the fill parameter to specify whether you want to get NULLs to the pending ones.)\n\noptions(currr.wait = 20, currr.fill = FALSE)\n\n\n\n\n\n\nIn the example above, you get your results, when 20 iterations are evaluated, but the job in the background keeps running."
  }
]