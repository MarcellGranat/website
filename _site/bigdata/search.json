[
  {
    "objectID": "content/01-content.html",
    "href": "content/01-content.html",
    "title": "The base R",
    "section": "",
    "text": "üìà Handling large datasets: Economic data often consists of thousands or even millions of observations. Programming enables us to organize and process this data effectively. By writing code, we can automate repetitive tasks, explore the data, and perform calculations on a scale that would be impractical or time-consuming with manual methods.\n\nüßπ Data cleaning and preprocessing: Real-world data is often messy and inconsistent. Programming allows us to clean and preprocess the data, removing any errors, inconsistencies, or missing values. By writing code to handle such data cleaning tasks, we can ensure the accuracy and integrity of our analysis.\n\nüîÅ Reproducibility: Programming promotes reproducibility in statistical analysis. By documenting and sharing our code, others can replicate our analyses, verify our findings, and build upon our work. This promotes transparency and strengthens the validity of our results.\n\nüõÉ Flexibility and customization: Programming languages like  and Python provide a wide range of statistical libraries and packages specifically designed for data analysis. These libraries offer various functions and algorithms to perform statistical tests, regression models, or other techniques. The ability to customize and tailor these tools to specific research questions allows for more precise and detailed analysis.\n\n\n\n\n\nProgramming, like any other skill, requires practice and persistence ‚öíÔ∏è. As Hadley Wickham, a prominent figure in the  community, once said, ‚ÄúThe only way to write good code is to write tons of subpar code first. Feeling shame about bad code stops you from getting to good code.‚Äù üöÄ This sentiment underscores the importance of perseverance and learning from mistakes in the journey of mastering ."
  },
  {
    "objectID": "content/01-content.html#example---fertility-rates-by-country-by-year-1",
    "href": "content/01-content.html#example---fertility-rates-by-country-by-year-1",
    "title": "The base R",
    "section": "Example - Fertility rates by country / by year",
    "text": "Example - Fertility rates by country / by year\nData from OECD\n\n\nData\nCode to reproduce\n\n\n\n\nfertility_df\n\n      AUS  AUT  BEL  CAN  CZE  DNK  FIN  FRA  DEU  GRC  HUN  ISL  IRL  ITA  JPN\n1960 3.45 2.69 2.54 3.90 2.11 2.54 2.71 2.74 2.37 2.23 2.02 4.26 3.76 2.41 2.00\n1961 3.55 2.78 2.63 3.84 2.13 2.55 2.65 2.82 2.44 2.13 1.94 3.88 3.79 2.41 1.96\n1962 3.43 2.80 2.59 3.76 2.14 2.54 2.66 2.80 2.44 2.16 1.79 3.98 3.92 2.46 1.98\n1963 3.34 2.82 2.68 3.67 2.33 2.64 2.66 2.90 2.51 2.14 1.82 3.98 4.01 2.56 2.00\n1964 3.15 2.79 2.71 3.50 2.36 2.60 2.58 2.91 2.53 2.24 1.80 3.86 4.06 2.70 2.05\n1965 2.97 2.70 2.61 3.15 2.18 2.61 2.46 2.85 2.50 2.25 1.81 3.71 4.03 2.67 2.14\n1966 2.89 2.66 2.52 2.81 2.01 2.62 2.40 2.80 2.51 2.32 1.88 3.58 3.95 2.63 1.58\n1967 2.85 2.62 2.41 2.60 1.90 2.35 2.32 2.67 2.45 2.45 2.01 3.28 3.84 2.54 2.23\n1968 2.89 2.58 2.31 2.45 1.83 2.12 2.15 2.59 2.36 2.42 2.06 3.07 3.78 2.49 2.13\n1969 2.89 2.49 2.27 2.40 1.86 2.00 1.94 2.53 2.21 2.36 2.04 2.99 3.85 2.51 2.13\n1970 2.86 2.29 2.25 2.33 1.91 1.95 1.83 2.48 2.03 2.40 1.97 2.81 3.87 2.42 2.13\n1971 2.95 2.20 2.21 2.19 1.98 2.04 1.70 2.50 1.97 2.32 1.92 2.92 3.98 2.41 2.16\n1972 2.74 2.08 2.09 2.02 2.07 2.03 1.59 2.42 1.74 2.32 1.93 3.09 3.88 2.36 2.14\n1973 2.49 1.94 1.95 1.93 2.29 1.92 1.50 2.31 1.56 2.27 1.95 2.95 3.74 2.34 2.14\n1974 2.32 1.91 1.83 1.82 2.43 1.90 1.62 2.11 1.53 2.38 2.30 2.66 3.62 2.33 2.05\n1975 2.15 1.83 1.74 1.80 2.40 1.92 1.69 1.93 1.48 2.33 2.38 2.65 3.40 2.21 1.91\n1976 2.06 1.69 1.73 1.76 2.36 1.75 1.72 1.83 1.51 2.35 2.26 2.52 3.31 2.11 1.85\n1977 2.01 1.63 1.71 1.75 2.32 1.66 1.69 1.86 1.51 2.27 2.17 2.31 3.27 1.97 1.80\n1978 1.95 1.60 1.69 1.70 2.32 1.67 1.65 1.82 1.50 2.28 2.08 2.35 3.24 1.87 1.79\n1979 1.91 1.60 1.69 1.70 2.29 1.60 1.64 1.86 1.50 2.26 2.02 2.49 3.23 1.76 1.77\n1980 1.89 1.65 1.68 1.68 2.10 1.55 1.63 1.95 1.56 2.23 1.92 2.48 3.23 1.68 1.75\n1981 1.94 1.67 1.66 1.65 2.02 1.44 1.65 1.95 1.53 2.10 1.88 2.33 3.07 1.60 1.74\n1982 1.93 1.66 1.61 1.64 2.01 1.43 1.72 1.91 1.51 2.03 1.78 2.26 2.96 1.60 1.77\n1983 1.92 1.56 1.57 1.63 1.96 1.38 1.74 1.78 1.43 1.94 1.73 2.24 2.76 1.54 1.80\n1984 1.84 1.52 1.54 1.63 1.97 1.40 1.70 1.80 1.39 1.82 1.73 2.08 2.59 1.48 1.81\n1985 1.92 1.47 1.51 1.61 1.96 1.45 1.64 1.81 1.37 1.68 1.83 1.93 2.50 1.45 1.76\n1986 1.87 1.45 1.54 1.59 1.94 1.48 1.60 1.83 1.41 1.60 1.83 1.93 2.44 1.37 1.72\n1987 1.85 1.43 1.54 1.58 1.91 1.50 1.59 1.80 1.43 1.50 1.81 2.07 2.31 1.35 1.69\n1988 1.83 1.45 1.57 1.60 1.94 1.56 1.70 1.81 1.46 1.50 1.79 2.27 2.17 1.38 1.66\n1989 1.84 1.45 1.58 1.66 1.87 1.62 1.71 1.79 1.42 1.40 1.78 2.20 2.08 1.35 1.57\n1990 1.90 1.46 1.62 1.71 1.89 1.67 1.79 1.78 1.45 1.39 1.84 2.31 2.12 1.36 1.54\n1991 1.85 1.51 1.66 1.72 1.86 1.68 1.80 1.77 1.33 1.37 1.86 2.19 2.09 1.33 1.53\n1992 1.89 1.51 1.65 1.71 1.72 1.76 1.85 1.73 1.29 1.36 1.77 2.21 1.99 1.32 1.50\n1993 1.86 1.50 1.61 1.69 1.67 1.75 1.81 1.66 1.28 1.32 1.69 2.22 1.91 1.26 1.46\n1994 1.84 1.47 1.56 1.69 1.44 1.81 1.85 1.66 1.24 1.33 1.64 2.14 1.85 1.22 1.50\n1995 1.82 1.42 1.56 1.67 1.28 1.81 1.81 1.71 1.25 1.28 1.57 2.08 1.85 1.19 1.42\n1996 1.80 1.45 1.59 1.63 1.19 1.75 1.76 1.73 1.32 1.26 1.46 2.12 1.89 1.22 1.43\n1997 1.78 1.39 1.60 1.57 1.17 1.76 1.75 1.73 1.37 1.27 1.38 2.04 1.94 1.23 1.39\n1998 1.76 1.37 1.60 1.56 1.16 1.73 1.71 1.76 1.36 1.24 1.33 2.05 1.95 1.21 1.38\n1999 1.76 1.34 1.62 1.55 1.13 1.74 1.73 1.79 1.36 1.23 1.29 1.99 1.91 1.23 1.34\n2000 1.76 1.36 1.67 1.51 1.14 1.77 1.73 1.87 1.38 1.25 1.33 2.08 1.90 1.26 1.36\n2001 1.73 1.33 1.67 1.54 1.15 1.75 1.73 1.88 1.35 1.25 1.31 1.95 1.96 1.25 1.33\n2002 1.77 1.39 1.65 1.52 1.17 1.72 1.72 1.86 1.34 1.28 1.31 1.93 1.98 1.27 1.32\n2003 1.77 1.38 1.67 1.55 1.18 1.76 1.76 1.87 1.34 1.29 1.28 1.99 1.98 1.29 1.29\n2004 1.78 1.42 1.72 1.56 1.23 1.79 1.80 1.90 1.36 1.31 1.28 2.03 1.95 1.34 1.29\n2005 1.85 1.41 1.76 1.58 1.28 1.80 1.80 1.92 1.34 1.34 1.32 2.05 1.88 1.33 1.26\n2006 1.88 1.41 1.80 1.63 1.33 1.85 1.84 1.98 1.33 1.40 1.35 2.07 1.94 1.37 1.32\n2007 1.99 1.39 1.82 1.67 1.44 1.84 1.83 1.95 1.37 1.41 1.32 2.09 2.01 1.39 1.34\n2008 2.02 1.42 1.85 1.70 1.50 1.89 1.85 1.99 1.38 1.50 1.35 2.14 2.06 1.44 1.37\n2009 1.97 1.40 1.84 1.69 1.49 1.84 1.86 1.99 1.36 1.50 1.33 2.22 2.06 1.44 1.37\n2010 1.95 1.44 1.86 1.65 1.49 1.87 1.87 2.02 1.39 1.48 1.26 2.20 2.05 1.44 1.39\n2011 1.92 1.43 1.81 1.63 1.43 1.75 1.83 2.00 1.39 1.40 1.24 2.02 2.03 1.42 1.39\n2012 1.93 1.44 1.80 1.63 1.45 1.73 1.80 1.99 1.41 1.34 1.34 2.04 1.98 1.42 1.41\n2013 1.88 1.44 1.76 1.61 1.46 1.67 1.75 1.97 1.42 1.29 1.34 1.93 1.93 1.39 1.43\n2014 1.79 1.46 1.74 1.61 1.53 1.69 1.71 1.97 1.47 1.30 1.41 1.93 1.89 1.38 1.42\n2015 1.79 1.49 1.70 1.60 1.57 1.71 1.65 1.93 1.50 1.33 1.44 1.81 1.85 1.36 1.45\n2016 1.79 1.53 1.68 1.59 1.63 1.79 1.57 1.89 1.59 1.38 1.49 1.75 1.82 1.36 1.44\n2017 1.74 1.52 1.65 1.55 1.69 1.75 1.49 1.86 1.57 1.35 1.49 1.71 1.78 1.34 1.43\n2018 1.74 1.48 1.62 1.51 1.71 1.73 1.41 1.84 1.57 1.35 1.49 1.71 1.75 1.31 1.42\n2019 1.67 1.46 1.60 1.47 1.71 1.70 1.35 1.83 1.54 1.34 1.49 1.75 1.70 1.27 1.36\n2020 1.59 1.44 1.55 1.41 1.71 1.67 1.37 1.79 1.53 1.39 1.56 1.72 1.63 1.24 1.33\n2021 1.70 1.48 1.60 1.43 1.83 1.72 1.46 1.80 1.58 1.43 1.59 1.82 1.72 1.25 1.30\n2022   NA   NA   NA   NA   NA 1.55   NA   NA   NA   NA 1.52   NA   NA   NA   NA\n      KOR  LUX  MEX  NLD  NZL  NOR  POL  PRT  SVK  ESP  SWE  CHE  TUR  GBR  USA\n1960 6.00 2.28 6.77 3.12 4.24 2.91 2.98 3.10 3.07 2.86 2.20 2.44 6.40 2.72 3.65\n1961 5.80 2.33 6.76 3.22 4.31 2.94 2.83 3.16 2.96 2.76 2.23 2.53 6.33 2.80 3.62\n1962 5.60 2.35 6.76 3.18 4.19 2.91 2.72 3.21 2.83 2.80 2.26 2.60 6.26 2.88 3.46\n1963 5.40 2.33 6.75 3.19 4.05 2.93 2.70 3.11 2.93 2.88 2.34 2.67 6.19 2.92 3.32\n1964 5.20 2.38 6.75 3.17 3.80 2.98 2.57 3.21 2.91 3.01 2.48 2.68 6.01 2.97 3.19\n1965 5.00 2.42 6.76 3.04 3.54 2.94 2.52 3.14 2.80 2.94 2.42 2.61 5.84 2.89 2.91\n1966 4.80 2.37 6.77 2.90 3.41 2.90 2.34 3.12 2.67 2.99 2.36 2.52 5.66 2.79 2.72\n1967 4.66 2.25 6.79 2.81 3.35 2.81 2.33 3.08 2.49 3.03 2.27 2.41 5.49 2.69 2.56\n1968 4.52 2.13 6.81 2.72 3.34 2.75 2.24 3.00 2.40 2.96 2.07 2.30 5.31 2.60 2.46\n1969 4.53 2.02 6.83 2.75 3.28 2.69 2.20 2.95 2.43 2.93 1.93 2.19 5.18 2.51 2.46\n1970 4.53 1.98 6.83 2.57 3.17 2.50 2.20 2.83 2.40 2.90 1.94 2.10 5.00 2.43 2.48\n1971 4.54 1.96 6.79 2.36 3.18 2.49 2.25 2.78 2.43 2.88 1.96 2.04 5.00 2.40 2.27\n1972 4.12 1.75 6.70 2.15 3.00 2.38 2.24 2.69 2.49 2.86 1.91 1.91 5.00 2.20 2.01\n1973 4.07 1.58 6.56 1.90 2.76 2.23 2.26 2.65 2.56 2.84 1.86 1.81 5.59 2.04 1.88\n1974 3.77 1.58 6.37 1.77 2.58 2.13 2.26 2.60 2.60 2.89 1.87 1.73 5.46 1.92 1.84\n1975 3.43 1.55 6.13 1.66 2.37 1.98 2.27 2.58 2.53 2.80 1.77 1.61 5.32 1.81 1.77\n1976 3.00 1.48 5.86 1.63 2.27 1.86 2.30 2.58 2.52 2.80 1.68 1.55 5.19 1.74 1.74\n1977 2.99 1.49 5.59 1.58 2.21 1.75 2.23 2.48 2.47 2.67 1.64 1.53 4.90 1.69 1.79\n1978 2.64 1.47 5.32 1.58 2.07 1.77 2.21 2.28 2.45 2.55 1.60 1.51 5.05 1.75 1.76\n1979 2.90 1.47 5.06 1.56 2.12 1.75 2.28 2.17 2.44 2.37 1.66 1.52 4.84 1.86 1.81\n1980 2.82 1.50 4.84 1.60 2.03 1.72 2.28 2.18 2.31 2.22 1.68 1.55 4.63 1.90 1.84\n1981 2.57 1.55 4.64 1.56 2.01 1.70 2.24 2.13 2.28 2.04 1.63 1.55 4.41 1.82 1.81\n1982 2.39 1.49 4.46 1.50 1.95 1.71 2.34 2.07 2.27 1.94 1.62 1.56 4.20 1.78 1.83\n1983 2.06 1.44 4.30 1.47 1.92 1.66 2.42 1.95 2.27 1.80 1.61 1.52 4.11 1.77 1.80\n1984 1.74 1.42 4.15 1.49 1.93 1.66 2.37 1.90 2.25 1.73 1.65 1.53 3.93 1.77 1.81\n1985 1.66 1.38 4.02 1.51 1.93 1.68 2.33 1.72 2.25 1.64 1.73 1.52 3.76 1.79 1.84\n1986 1.58 1.44 3.90 1.55 1.96 1.71 2.22 1.66 2.20 1.56 1.79 1.53 3.58 1.78 1.84\n1987 1.53 1.39 3.79 1.56 2.03 1.75 2.15 1.62 2.14 1.50 1.84 1.52 3.40 1.81 1.87\n1988 1.55 1.51 3.68 1.55 2.10 1.84 2.13 1.62 2.15 1.45 1.96 1.57 3.29 1.82 1.93\n1989 1.56 1.52 3.57 1.55 2.12 1.89 2.07 1.58 2.08 1.40 2.02 1.56 3.39 1.79 2.01\n1990 1.57 1.62 3.47 1.62 2.18 1.93 1.99 1.56 2.09 1.36 2.14 1.59 3.07 1.83 2.08\n1991 1.71 1.60 3.37 1.61 2.09 1.92 1.98 1.56 2.05 1.33 2.12 1.58 3.00 1.82 2.06\n1992 1.76 1.67 3.27 1.59 2.06 1.89 1.85 1.53 1.99 1.32 2.09 1.58 2.93 1.79 2.05\n1993 1.65 1.69 3.18 1.57 2.04 1.86 1.77 1.51 1.93 1.27 2.00 1.51 2.87 1.76 2.02\n1994 1.66 1.72 3.10 1.57 1.98 1.87 1.72 1.44 1.67 1.20 1.89 1.49 2.81 1.74 2.00\n1995 1.63 1.67 3.02 1.53 1.98 1.87 1.55 1.41 1.52 1.17 1.74 1.48 2.75 1.71 1.98\n1996 1.57 1.76 2.95 1.53 1.96 1.89 1.53 1.44 1.47 1.16 1.61 1.50 2.69 1.73 1.98\n1997 1.54 1.71 2.88 1.56 1.96 1.86 1.47 1.47 1.43 1.18 1.53 1.48 2.63 1.72 1.97\n1998 1.46 1.67 2.82 1.63 1.89 1.81 1.41 1.48 1.37 1.16 1.51 1.47 2.56 1.71 2.00\n1999 1.43 1.71 2.77 1.65 1.97 1.85 1.37 1.51 1.33 1.19 1.50 1.48 2.48 1.68 2.01\n2000 1.48 1.78 2.72 1.72 1.98 1.85 1.37 1.56 1.29 1.23 1.55 1.50 2.27 1.64 2.06\n2001 1.31 1.66 2.67 1.71 1.97 1.78 1.32 1.46 1.20 1.24 1.57 1.38 2.37 1.63 2.03\n2002 1.18 1.63 2.62 1.73 1.89 1.75 1.25 1.47 1.19 1.25 1.65 1.39 2.17 1.63 2.01\n2003 1.19 1.62 2.58 1.75 1.93 1.80 1.22 1.44 1.20 1.30 1.72 1.39 2.09 1.70 2.04\n2004 1.16 1.66 2.54 1.73 1.98 1.83 1.23 1.41 1.24 1.31 1.75 1.42 2.11 1.75 2.05\n2005 1.09 1.62 2.50 1.71 1.97 1.84 1.24 1.42 1.25 1.33 1.77 1.42 2.12 1.76 2.06\n2006 1.13 1.64 2.46 1.72 2.01 1.90 1.27 1.38 1.24 1.36 1.85 1.44 2.12 1.82 2.11\n2007 1.26 1.61 2.42 1.72 2.18 1.90 1.31 1.35 1.25 1.38 1.88 1.46 2.16 1.86 2.12\n2008 1.19 1.60 2.39 1.77 2.19 1.96 1.39 1.40 1.32 1.45 1.91 1.48 2.15 1.91 2.07\n2009 1.15 1.59 2.36 1.79 2.13 1.98 1.40 1.35 1.41 1.38 1.94 1.50 2.10 1.89 2.00\n2010 1.23 1.63 2.34 1.80 2.17 1.95 1.38 1.39 1.40 1.37 1.98 1.54 2.08 1.92 1.93\n2011 1.24 1.51 2.32 1.76 2.09 1.88 1.30 1.35 1.45 1.34 1.90 1.52 2.05 1.91 1.89\n2012 1.30 1.57 2.29 1.72 2.10 1.85 1.30 1.29 1.34 1.32 1.91 1.53 2.11 1.92 1.88\n2013 1.19 1.55 2.27 1.68 2.01 1.78 1.26 1.21 1.34 1.27 1.89 1.52 2.11 1.83 1.86\n2014 1.21 1.50 2.21 1.71 1.92 1.76 1.29 1.23 1.37 1.32 1.88 1.54 2.18 1.81 1.86\n2015 1.24 1.47 2.14 1.66 1.99 1.73 1.29 1.31 1.40 1.33 1.85 1.54 2.15 1.80 1.84\n2016 1.17 1.41 2.09 1.66 1.87 1.71 1.36 1.36 1.48 1.34 1.85 1.54 2.11 1.79 1.82\n2017 1.05 1.39 2.04 1.62 1.81 1.62 1.45 1.38 1.52 1.31 1.78 1.52 2.07 1.74 1.77\n2018 0.98 1.38 2.00 1.59 1.71 1.56 1.44 1.42 1.54 1.26 1.75 1.52 1.99 1.68 1.73\n2019 0.92 1.34 1.92 1.57 1.72 1.53 1.42 1.43 1.57 1.23 1.70 1.48 1.88 1.63 1.71\n2020 0.84 1.36 1.91 1.54 1.61 1.48 1.39 1.41 1.59 1.19 1.66 1.46 1.76 1.56 1.64\n2021 0.81 1.38 1.82 1.62 1.64 1.55 1.33 1.35 1.63 1.19 1.67 1.51 1.70 1.53 1.66\n2022   NA   NA   NA   NA   NA 1.41   NA   NA   NA   NA   NA   NA   NA   NA   NA\n      BRA  CHL  CHN  EST  IND  IDN  ISR  RUS  SVN  ZAF  COL  LVA  LTU  ARG  BGR\n1960 6.06 4.70 4.45 1.98 5.91 5.55 3.95 2.52 2.18 6.16 6.74 1.94 2.40 3.11 2.31\n1961 6.03 4.66 3.86 1.98 5.90 5.57 3.80 2.45 2.26 6.14 6.71 1.94 2.40 3.10 2.29\n1962 5.98 4.60 6.09 1.95 5.89 5.59 3.77 2.36 2.27 6.11 6.66 1.91 2.40 3.09 2.24\n1963 5.91 4.54 7.51 1.89 5.88 5.60 3.81 2.27 2.28 6.08 6.58 1.85 2.40 3.08 2.21\n1964 5.82 4.46 6.67 1.94 5.86 5.61 3.93 2.18 2.32 6.03 6.48 1.79 2.40 3.07 2.19\n1965 5.70 4.36 6.61 1.88 5.83 5.62 3.99 2.13 2.45 5.97 6.33 1.74 2.40 3.06 2.09\n1966 5.57 4.26 6.31 1.87 5.79 5.60 3.89 2.10 2.48 5.91 6.16 1.76 2.40 3.05 2.03\n1967 5.42 4.14 5.81 1.90 5.75 5.58 3.64 2.04 2.38 5.85 5.96 1.80 2.40 3.05 2.02\n1968 5.27 4.03 6.51 2.03 5.70 5.54 3.82 1.99 2.28 5.78 5.74 1.83 2.40 3.05 2.27\n1969 5.12 3.90 6.18 2.13 5.65 5.51 3.83 1.97 2.17 5.72 5.51 1.88 2.40 3.06 2.27\n1970 4.97 3.78 6.09 2.17 5.59 5.45 3.97 1.99 2.21 5.63 5.28 2.02 2.40 3.08 2.17\n1971 4.84 3.65 5.52 2.19 5.52 5.36 3.94 2.03 2.16 5.57 5.06 2.04 2.41 3.11 2.10\n1972 4.71 3.53 5.11 2.13 5.44 5.29 3.71 2.04 2.14 5.49 4.86 2.03 2.34 3.15 2.03\n1973 4.60 3.41 4.73 2.06 5.36 5.22 3.68 2.01 2.18 5.41 4.68 1.96 2.22 3.20 2.15\n1974 4.50 3.29 4.17 2.07 5.28 5.09 3.71 2.00 2.10 5.30 4.53 2.00 2.21 3.25 2.29\n1975 4.42 3.18 3.57 2.04 5.19 5.04 3.68 1.98 2.16 5.19 4.40 1.97 2.18 3.30 2.23\n1976 4.34 3.08 3.24 2.07 5.11 4.92 3.70 1.97 2.17 5.07 4.29 1.93 2.18 3.34 2.24\n1977 4.27 2.98 2.84 2.06 5.03 4.81 3.47 1.95 2.16 4.94 4.18 1.89 2.14 3.36 2.21\n1978 4.20 2.89 2.72 2.02 4.96 4.72 3.28 1.92 2.19 4.85 4.07 1.87 2.08 3.36 2.15\n1979 4.12 2.81 2.75 2.00 4.89 4.61 3.21 1.90 2.22 4.82 3.97 1.87 2.05 3.34 2.16\n1980 4.04 2.74 2.74 2.02 4.83 4.49 3.14 1.89 2.11 4.78 3.86 1.90 1.99 3.30 2.05\n1981 3.94 2.69 2.79 2.07 4.77 4.36 3.06 1.91 1.96 4.71 3.74 1.90 1.98 3.25 2.00\n1982 3.84 2.65 2.97 2.08 4.70 4.25 3.12 2.04 1.93 4.70 3.63 1.98 1.97 3.20 2.01\n1983 3.72 2.62 2.56 2.16 4.64 4.10 3.21 2.11 1.82 4.63 3.53 2.13 2.10 3.16 2.01\n1984 3.60 2.60 2.61 2.17 4.56 3.94 3.13 2.06 1.75 4.57 3.43 2.15 2.07 3.12 2.01\n1985 3.47 2.59 2.63 2.12 4.48 3.71 3.12 2.05 1.72 4.50 3.34 2.09 2.08 3.10 1.97\n1986 3.34 2.59 2.72 2.17 4.40 3.53 3.09 2.15 1.65 4.41 3.27 2.21 2.12 3.08 2.02\n1987 3.23 2.59 2.76 2.26 4.31 3.42 3.05 2.22 1.64 4.35 3.21 2.21 2.11 3.06 1.96\n1988 3.11 2.59 2.54 2.26 4.22 3.33 3.06 2.12 1.63 4.18 3.16 2.16 2.02 3.04 1.97\n1989 3.01 2.59 2.52 2.22 4.13 3.22 3.03 2.01 1.52 3.98 3.12 2.05 1.98 3.02 1.90\n1990 2.91 2.58 2.51 2.05 4.05 3.10 3.02 1.89 1.46 3.72 3.08 2.01 2.03 3.00 1.82\n1991 2.82 2.56 1.93 1.80 3.96 3.06 2.91 1.73 1.42 3.62 3.05 1.86 2.01 2.97 1.66\n1992 2.72 2.53 1.78 1.71 3.88 2.94 2.93 1.55 1.34 3.48 3.01 1.73 1.97 2.93 1.55\n1993 2.67 2.48 1.69 1.49 3.80 2.88 2.92 1.39 1.33 3.37 2.97 1.51 1.74 2.88 1.46\n1994 2.62 2.43 1.63 1.42 3.72 2.84 2.90 1.40 1.32 3.26 2.92 1.39 1.57 2.83 1.37\n1995 2.58 2.37 1.59 1.38 3.65 2.80 2.88 1.34 1.29 3.17 2.86 1.26 1.55 2.77 1.23\n1996 2.52 2.31 1.55 1.37 3.58 2.77 2.94 1.27 1.28 2.99 2.80 1.16 1.49 2.72 1.23\n1997 2.47 2.24 1.53 1.32 3.51 2.74 2.93 1.22 1.25 2.73 2.74 1.11 1.47 2.67 1.09\n1998 2.41 2.17 1.52 1.28 3.45 2.66 2.98 1.23 1.23 2.63 2.68 1.10 1.46 2.62 1.11\n1999 2.33 2.11 1.53 1.30 3.38 2.58 2.94 1.16 1.21 2.56 2.63 1.18 1.46 2.58 1.23\n2000 2.26 2.06 1.63 1.36 3.35 2.54 2.95 1.20 1.26 2.41 2.57 1.25 1.39 2.54 1.26\n2001 2.18 2.01 1.56 1.32 3.30 2.50 2.89 1.22 1.21 2.37 2.52 1.22 1.29 2.51 1.21\n2002 2.10 1.97 1.57 1.36 3.22 2.46 2.89 1.29 1.21 2.32 2.46 1.26 1.23 2.49 1.21\n2003 2.02 1.94 1.57 1.36 3.12 2.43 2.95 1.32 1.20 2.36 2.40 1.32 1.26 2.46 1.23\n2004 2.00 1.92 1.61 1.47 3.05 2.42 2.90 1.34 1.25 2.44 2.33 1.29 1.27 2.44 1.29\n2005 1.97 1.91 1.62 1.52 2.96 2.43 2.84 1.29 1.26 2.51 2.26 1.39 1.29 2.42 1.32\n2006 1.93 1.90 1.64 1.58 2.86 2.45 2.88 1.31 1.31 2.55 2.20 1.46 1.33 2.40 1.38\n2007 1.88 1.90 1.67 1.69 2.78 2.49 2.90 1.42 1.38 2.55 2.14 1.54 1.36 2.38 1.49\n2008 1.84 1.90 1.70 1.72 2.72 2.48 2.96 1.50 1.53 2.68 2.08 1.58 1.45 2.37 1.56\n2009 1.83 1.89 1.71 1.70 2.67 2.46 2.96 1.54 1.53 2.50 2.03 1.46 1.50 2.36 1.66\n2010 1.81 1.88 1.69 1.72 2.60 2.45 3.03 1.57 1.57 2.44 1.99 1.36 1.50 2.35 1.57\n2011 1.80 1.82 1.67 1.61 2.54 2.50 3.00 1.58 1.56 2.44 1.96 1.33 1.55 2.34 1.51\n2012 1.77 1.80 1.80 1.56 2.47 2.49 3.05 1.69 1.58 2.45 1.93 1.44 1.60 2.33 1.50\n2013 1.75 1.79 1.71 1.52 2.41 2.43 3.03 1.71 1.55 2.43 1.91 1.52 1.59 2.32 1.48\n2014 1.77 1.77 1.77 1.54 2.31 2.39 3.08 1.75 1.58 2.42 1.88 1.65 1.63 2.31 1.53\n2015 1.78 1.74 1.67 1.58 2.29 2.35 3.09 1.78 1.57 2.36 1.86 1.70 1.70 2.30 1.53\n2016 1.71 1.68 1.77 1.60 2.27 2.31 3.11 1.76 1.58 2.26 1.84 1.74 1.69 2.24 1.54\n2017 1.74 1.60 1.81 1.59 2.20 2.26 3.11 1.62 1.62 2.33 1.82 1.69 1.63 2.17 1.56\n2018 1.75 1.56 1.55 1.67 2.18 2.23 3.09 1.58 1.60 2.42 1.79 1.60 1.63 2.04 1.56\n2019 1.70 1.55 1.50 1.66 2.11 2.22 3.01 1.50 1.61 2.48 1.77 1.61 1.61 1.99 1.58\n2020 1.65 1.54 1.28 1.58 2.05 2.19 2.90 1.51 1.59 2.40 1.74 1.55 1.48 1.91 1.56\n2021 1.64 1.54 1.16 1.61 2.03 2.17 3.00 1.49 1.64 2.37 1.72 1.57 1.36 1.89 1.58\n2022   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA\n      HRV  CYP  MLT  ROU  SAU  PER  CRI   EU\n1960 2.20   NA   NA   NA 7.63 6.94 6.71 2.62\n1961 2.19   NA   NA   NA 7.63 6.92 6.65 2.62\n1962 2.17   NA   NA   NA 7.64 6.90 6.54 2.61\n1963 2.12   NA   NA   NA 7.65 6.86 6.39 2.65\n1964 2.12   NA   NA   NA 7.67 6.81 6.19 2.67\n1965 2.21   NA   NA   NA 7.66 6.75 5.96 2.62\n1966 2.21   NA   NA   NA 7.66 6.68 5.70 2.58\n1967 2.07   NA   NA   NA 7.66 6.60 5.42 2.53\n1968 1.99   NA   NA   NA 7.63 6.51 5.03 2.45\n1969 1.91   NA   NA   NA 7.60 6.42 4.84 2.41\n1970 1.83   NA   NA   NA 7.58 6.32 4.59 2.37\n1971 1.95   NA   NA   NA 7.56 6.21 4.36 2.36\n1972 1.97   NA   NA   NA 7.54 6.09 4.16 2.29\n1973 1.98   NA   NA   NA 7.48 5.97 3.99 2.23\n1974 1.95   NA   NA   NA 7.43 5.84 3.89 2.23\n1975 1.92   NA   NA 2.59 7.37 5.71 3.80 2.18\n1976 1.90   NA   NA 2.54 7.33 5.58 3.75 2.14\n1977 1.91   NA   NA 2.57 7.30 5.44 3.70 2.09\n1978 1.92   NA   NA 2.52 7.26 5.31 3.66 2.05\n1979 1.94   NA   NA 2.49 7.23 5.17 3.65 2.03\n1980 1.92   NA 1.99 2.43 7.19 5.04 3.59 2.00\n1981 1.91   NA 1.87 2.36 7.13 4.92 3.56 1.95\n1982 1.90 2.48 2.04 2.17 7.05 4.80 3.54 1.93\n1983 1.88 2.50 1.97 2.06 6.95 4.68 3.53 1.89\n1984 1.87 2.52 1.97 2.26 6.84 4.57 3.52 1.87\n1985 1.81 2.43 1.99 2.31 6.70 4.46 3.51 1.84\n1986 1.76 2.46 1.94 2.39 6.55 4.35 3.46 1.84\n1987 1.74 2.38 1.97 2.38 6.36 4.25 3.40 1.82\n1988 1.74 2.49 2.10 2.30 6.17 4.14 3.34 1.83\n1989 1.67 2.36 2.11 2.22 6.00 4.03 3.27 1.79\n1990 1.67 2.41 2.05 1.83 5.83 3.91 3.21 1.78\n1991 1.55 2.32 2.10 1.59 5.66 3.79 3.12 1.73\n1992 1.39 2.48 2.12 1.51 5.49 3.67 3.04 1.70\n1993 1.43 2.24 2.01 1.43 5.32 3.55 2.96 1.62\n1994 1.43 2.17 1.89 1.40 5.14 3.43 2.89 1.56\n1995 1.50 2.03 1.82 1.33 4.95 3.32 2.80 1.51\n1996 1.64 1.95 2.01 1.30 4.77 3.20 2.71 1.50\n1997 1.69 1.86 1.95 1.32 4.59 3.10 2.64 1.48\n1998 1.45 1.76 1.81 1.32 4.42 3.00 2.53 1.45\n1999 1.38 1.67 1.72 1.30 4.25 2.92 2.48 1.45\n2000 1.39 1.64 1.69 1.31 4.12 2.85 2.41 1.47\n2001 1.46 1.57 1.50 1.27 3.91 2.74 2.33 1.44\n2002 1.42 1.49 1.45 1.27 3.71 2.69 2.20 1.43\n2003 1.41 1.51 1.48 1.30 3.50 2.66 2.14 1.45\n2004 1.43 1.52 1.40 1.33 3.34 2.67 2.08 1.47\n2005 1.50 1.48 1.38 1.40 3.24 2.69 2.04 1.48\n2006 1.47 1.52 1.36 1.42 3.21 2.69 2.01 1.51\n2007 1.48 1.44 1.35 1.45 3.18 2.67 2.01 1.53\n2008 1.55 1.48 1.43 1.60 3.06 2.63 2.02 1.59\n2009 1.58 1.47 1.42 1.66 2.95 2.61 1.98 1.59\n2010 1.55 1.44 1.36 1.59 2.85 2.57 1.93 1.58\n2011 1.48 1.35 1.45 1.47 2.81 2.54 1.90 1.54\n2012 1.51 1.39 1.42 1.52 2.78 2.49 1.88 1.54\n2013 1.46 1.30 1.36 1.41 2.74 2.43 1.84 1.51\n2014 1.46 1.31 1.38 1.52 2.69 2.38 1.82 1.54\n2015 1.40 1.32 1.37 1.58 2.64 2.34 1.79 1.54\n2016 1.42 1.37 1.37 1.64 2.59 2.31 1.75 1.56\n2017 1.42 1.32 1.26 1.71 2.58 2.28 1.74 1.55\n2018 1.47 1.32 1.23 1.76 2.55 2.26 1.71 1.54\n2019 1.47 1.33 1.14 1.77 2.50 2.24 1.63 1.52\n2020 1.48 1.36 1.13 1.80 2.47 2.22 1.56 1.50\n2021 1.58 1.39 1.13 1.81 2.43 2.19 1.53 1.53\n2022   NA   NA   NA   NA   NA   NA   NA   NA\n\n\n\n\nLater, we will discuss the meanings of each step and the fact that there are much simpler solutions available.\n\nfertility_df &lt;- readr::read_csv(\"https://stats.oecd.org/sdmx-json/data/DP_LIVE/.FERTILITY.../OECD?contentType=csv&detail=code&separator=comma&csv-lang=en\")\nfertility_df &lt;- fertility_df[, c(\"LOCATION\", \"TIME\", \"Value\")]  \n\nfertility_df &lt;- tidyr::pivot_wider(fertility_df, names_from = \"LOCATION\", \n                   values_from = \"Value\")\n\nfertility_df &lt;- tibble::column_to_rownames(fertility_df, \"TIME\")\nfertility_df &lt;- fertility_df[, - 51] # OECD average\n\nfertility_df &lt;- data.frame(fertility_df)\n\n\n\n\nAt the end of these steps, we have a table, each column of which shows the time series of the fertility rate of a country.\nApply - MARGIN = 1\n\napply(fertility_df, MARGIN = 1, above_replacement_prop)\n\n      1960       1961       1962       1963       1964       1965       1966 \n0.92000000 0.92000000 0.92000000 0.92000000 0.92000000 0.92000000 0.88000000 \n      1967       1968       1969       1970       1971       1972       1973 \n0.86000000 0.86000000 0.82000000 0.80000000 0.78000000 0.68000000 0.66000000 \n      1974       1975       1976       1977       1978       1979       1980 \n0.66000000 0.62745098 0.60784314 0.56862745 0.50980392 0.52941176 0.48076923 \n      1981       1982       1983       1984       1985       1986       1987 \n0.42307692 0.39622642 0.43396226 0.39622642 0.37735849 0.43396226 0.43396226 \n      1988       1989       1990       1991       1992       1993       1994 \n0.47169811 0.37735849 0.35849057 0.32075472 0.30188679 0.28301887 0.28301887 \n      1995       1996       1997       1998       1999       2000       2001 \n0.24528302 0.26415094 0.24528302 0.24528302 0.24528302 0.22641509 0.22641509 \n      2002       2003       2004       2005       2006       2007       2008 \n0.22641509 0.18867925 0.18867925 0.18867925 0.20754717 0.22641509 0.20754717 \n      2009       2010       2011       2012       2013       2014       2015 \n0.20754717 0.18867925 0.15094340 0.18867925 0.16981132 0.16981132 0.16981132 \n      2016       2017       2018       2019       2020       2021       2022 \n0.15094340 0.13207547 0.11320755 0.11320755 0.09433962 0.09433962 0.00000000 \n\n\nIf MARGIN == 1, the apply calculates the proportion of observation above 2.1 in each row. The returned value is a named vector, which means that you can refer to its values by index, or by name.\nApply - MARGIN = 1\n\napply(fertility_df, MARGIN = 1, above_replacement_prop)[1]\n\n1960 \n0.92 \n\napply(fertility_df, MARGIN = 1, above_replacement_prop)[\"2020\"]\n\n      2020 \n0.09433962 \n\n\nApply - MARGIN = 2\n\napply(fertility_df, MARGIN = 2, above_replacement_prop)\n\n       AUS        AUT        BEL        CAN        CZE        DNK        FIN \n0.25806452 0.19354839 0.19354839 0.19354839 0.22580645 0.14285714 0.14516129 \n       FRA        DEU        GRC        HUN        ISL        IRL        ITA \n0.24193548 0.16129032 0.35483871 0.06349206 0.56451613 0.48387097 0.27419355 \n       JPN        KOR        LUX        MEX        NLD        NZL        NOR \n0.12903226 0.37096774 0.14516129 0.90322581 0.20967742 0.43548387 0.23809524 \n       POL        PRT        SVK        ESP        SWE        CHE        TUR \n0.46774194 0.35483871 0.46774194 0.33870968 0.16129032 0.17741935 0.87096774 \n       GBR        USA        BRA        CHL        CHN        EST        IND \n0.20967742 0.22580645 0.69354839 0.64516129 0.50000000 0.17741935 0.96774194 \n       IDN        ISR        RUS        SVN        ZAF        COL        LVA \n1.00000000 1.00000000 0.17741935 0.33870968 1.00000000 0.77419355 0.08064516 \n       LTU        ARG        BGR        HRV        CYP        MLT        ROU \n0.33870968 0.93548387 0.25806452 0.11290323 0.32500000 0.09523810 0.29787234 \n       SAU        PER        CRI         EU \n1.00000000 1.00000000 0.70967742 0.27419355 \n\n\nIf MARGIN == 2, the apply calculates the proportion of observation above 2.1 in each column."
  },
  {
    "objectID": "content/02-content.html",
    "href": "content/02-content.html",
    "title": "Statistics in R",
    "section": "",
    "text": "Statistical analysis serves as an information-compressing mechanism. The primary objectives can be broadly categorized into:\n\nThese tasks aim to forecast future outcomes based on historical data and current conditions.\n\nThese tasks focus on summarizing the main aspects of the data to provide an informative overview.\n\nThis involves identifying patterns, relationships, or anomalies in the data without a prior hypothesis.\n\nThis involves testing predefined hypotheses to confirm or refute them."
  },
  {
    "objectID": "content/02-content.html#objectives-of-statistical-analysis-in-economics",
    "href": "content/02-content.html#objectives-of-statistical-analysis-in-economics",
    "title": "Statistics in R",
    "section": "",
    "text": "Statistical analysis serves as an information-compressing mechanism. The primary objectives can be broadly categorized into:\n\nThese tasks aim to forecast future outcomes based on historical data and current conditions.\n\nThese tasks focus on summarizing the main aspects of the data to provide an informative overview.\n\nThis involves identifying patterns, relationships, or anomalies in the data without a prior hypothesis.\n\nThis involves testing predefined hypotheses to confirm or refute them."
  },
  {
    "objectID": "content/02-content.html#types-of-data",
    "href": "content/02-content.html#types-of-data",
    "title": "Statistics in R",
    "section": "Types of Data",
    "text": "Types of Data\nClassification Based on Structure\nUnstructured Data\nData that does not have a predefined format or organization.\nStructured Data\nData that is organized in a specific manner, often in tabular form.\nTypes of Structured Data\n\nCross-sectional\nTime-series\nLongitudinal\nSpatial\nNetwork\n\nThe type of data dictates the statistical tools and techniques that can be employed for analysis.\n\nAttribute Types\nDifferent types of attributes require different statistical techniques for effective analysis.\n\n\n\n\n\n\n\nAttribute type\n      Description\n      Examples\n      Operations\n    \n\n\nNominal\nNominal values provide only enough information to distinguish one object from another. (=, ‚â†)\nZip codes, employee ID numbers, eye color, gender\nMode, entropy, contingency correlation, chi2 test\n\n\nOrdinal\nThe values of an ordinal attribute provide enough information to order objects. (&lt;, &gt;)\nHardness of minerals, {good, better, best}, grades, street numbers\nQuantiles, rank correlation\n\n\nInterval\nDifferences between values are meaningful, i.e., a unit of measurement exists. (+, -)\nCalendar dates, temperature in Celsius or Fahrenheit\nmean, standard deviation, Pearson's correlation\n\n\nRatio\nFor ratio variables, both differences and ratios are meaningful. (*, /)\nTemperature in Kelvin, monetary quantities, counts, age, mass, length, electrical current\nGeometric mean, harmonic mean, percent variation"
  },
  {
    "objectID": "content/02-content.html#quality-of-data",
    "href": "content/02-content.html#quality-of-data",
    "title": "Statistics in R",
    "section": "Quality of Data",
    "text": "Quality of Data\n\n\n\n\nCommon Issues\n\nMissing data\nOutliers\nDuplication\nInconsistent data\n\n\n\nSome examples\n\n\nLet us examine a sample data from the modeldata R package.\n\n# load the data\ndata(attrition, package = \"modeldata\") \n\n\n\n\n\n\n\n\n\ntibble(attrition)\n\n# A tibble: 1,470 √ó 31\n     Age Attrition BusinessTravel    DailyRate Department       DistanceFromHome\n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;                 &lt;int&gt; &lt;fct&gt;                       &lt;int&gt;\n 1    41 Yes       Travel_Rarely          1102 Sales                           1\n 2    49 No        Travel_Frequently       279 Research_Develo‚Ä¶                8\n 3    37 Yes       Travel_Rarely          1373 Research_Develo‚Ä¶                2\n 4    33 No        Travel_Frequently      1392 Research_Develo‚Ä¶                3\n 5    27 No        Travel_Rarely           591 Research_Develo‚Ä¶                2\n 6    32 No        Travel_Frequently      1005 Research_Develo‚Ä¶                2\n 7    59 No        Travel_Rarely          1324 Research_Develo‚Ä¶                3\n 8    30 No        Travel_Rarely          1358 Research_Develo‚Ä¶               24\n 9    38 No        Travel_Frequently       216 Research_Develo‚Ä¶               23\n10    36 No        Travel_Rarely          1299 Research_Develo‚Ä¶               27\n# ‚Ñπ 1,460 more rows\n# ‚Ñπ 25 more variables: Education &lt;ord&gt;, EducationField &lt;fct&gt;,\n#   EnvironmentSatisfaction &lt;ord&gt;, Gender &lt;fct&gt;, HourlyRate &lt;int&gt;,\n#   JobInvolvement &lt;ord&gt;, JobLevel &lt;int&gt;, JobRole &lt;fct&gt;, JobSatisfaction &lt;ord&gt;,\n#   MaritalStatus &lt;fct&gt;, MonthlyIncome &lt;int&gt;, MonthlyRate &lt;int&gt;,\n#   NumCompaniesWorked &lt;int&gt;, OverTime &lt;fct&gt;, PercentSalaryHike &lt;int&gt;,\n#   PerformanceRating &lt;ord&gt;, RelationshipSatisfaction &lt;ord&gt;, ‚Ä¶\n\n\nAfter loading data that we would like to work with, it is worthwhile to examine and create descriptive statistics. The summary function exists in almost every language, but in R, I suggest using a dedicated package for this purpose: skmir (although there are countless others available).\n\nattrition |&gt; \n  skimr::skim()\n\n\nData summary\n\n\nName\nattrition\n\n\nNumber of rows\n1470\n\n\nNumber of columns\n31\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n15\n\n\nnumeric\n16\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nAttrition\n0\n1\nFALSE\n2\nNo: 1233, Yes: 237\n\n\nBusinessTravel\n0\n1\nFALSE\n3\nTra: 1043, Tra: 277, Non: 150\n\n\nDepartment\n0\n1\nFALSE\n3\nRes: 961, Sal: 446, Hum: 63\n\n\nEducation\n0\n1\nTRUE\n5\nBac: 572, Mas: 398, Col: 282, Bel: 170\n\n\nEducationField\n0\n1\nFALSE\n6\nLif: 606, Med: 464, Mar: 159, Tec: 132\n\n\nEnvironmentSatisfaction\n0\n1\nTRUE\n4\nHig: 453, Ver: 446, Med: 287, Low: 284\n\n\nGender\n0\n1\nFALSE\n2\nMal: 882, Fem: 588\n\n\nJobInvolvement\n0\n1\nTRUE\n4\nHig: 868, Med: 375, Ver: 144, Low: 83\n\n\nJobRole\n0\n1\nFALSE\n9\nSal: 326, Res: 292, Lab: 259, Man: 145\n\n\nJobSatisfaction\n0\n1\nTRUE\n4\nVer: 459, Hig: 442, Low: 289, Med: 280\n\n\nMaritalStatus\n0\n1\nFALSE\n3\nMar: 673, Sin: 470, Div: 327\n\n\nOverTime\n0\n1\nFALSE\n2\nNo: 1054, Yes: 416\n\n\nPerformanceRating\n0\n1\nTRUE\n2\nExc: 1244, Out: 226, Low: 0, Goo: 0\n\n\nRelationshipSatisfaction\n0\n1\nTRUE\n4\nHig: 459, Ver: 432, Med: 303, Low: 276\n\n\nWorkLifeBalance\n0\n1\nTRUE\n4\nBet: 893, Goo: 344, Bes: 153, Bad: 80\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nAge\n0\n1\n36.92\n9.14\n18\n30\n36.0\n43.00\n60\n‚ñÇ‚ñá‚ñá‚ñÉ‚ñÇ\n\n\nDailyRate\n0\n1\n802.49\n403.51\n102\n465\n802.0\n1157.00\n1499\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\nDistanceFromHome\n0\n1\n9.19\n8.11\n1\n2\n7.0\n14.00\n29\n‚ñá‚ñÖ‚ñÇ‚ñÇ‚ñÇ\n\n\nHourlyRate\n0\n1\n65.89\n20.33\n30\n48\n66.0\n83.75\n100\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\nJobLevel\n0\n1\n2.06\n1.11\n1\n1\n2.0\n3.00\n5\n‚ñá‚ñá‚ñÉ‚ñÇ‚ñÅ\n\n\nMonthlyIncome\n0\n1\n6502.93\n4707.96\n1009\n2911\n4919.0\n8379.00\n19999\n‚ñá‚ñÖ‚ñÇ‚ñÅ‚ñÇ\n\n\nMonthlyRate\n0\n1\n14313.10\n7117.79\n2094\n8047\n14235.5\n20461.50\n26999\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\nNumCompaniesWorked\n0\n1\n2.69\n2.50\n0\n1\n2.0\n4.00\n9\n‚ñá‚ñÉ‚ñÇ‚ñÇ‚ñÅ\n\n\nPercentSalaryHike\n0\n1\n15.21\n3.66\n11\n12\n14.0\n18.00\n25\n‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñÅ\n\n\nStockOptionLevel\n0\n1\n0.79\n0.85\n0\n0\n1.0\n1.00\n3\n‚ñá‚ñá‚ñÅ‚ñÇ‚ñÅ\n\n\nTotalWorkingYears\n0\n1\n11.28\n7.78\n0\n6\n10.0\n15.00\n40\n‚ñá‚ñá‚ñÇ‚ñÅ‚ñÅ\n\n\nTrainingTimesLastYear\n0\n1\n2.80\n1.29\n0\n2\n3.0\n3.00\n6\n‚ñÇ‚ñá‚ñá‚ñÇ‚ñÉ\n\n\nYearsAtCompany\n0\n1\n7.01\n6.13\n0\n3\n5.0\n9.00\n40\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nYearsInCurrentRole\n0\n1\n4.23\n3.62\n0\n2\n3.0\n7.00\n18\n‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÅ\n\n\nYearsSinceLastPromotion\n0\n1\n2.19\n3.22\n0\n0\n1.0\n3.00\n15\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nYearsWithCurrManager\n0\n1\n4.12\n3.57\n0\n2\n3.0\n7.00\n17\n‚ñá‚ñÇ‚ñÖ‚ñÅ‚ñÅ"
  },
  {
    "objectID": "content/02-content.html#measures-of-central-tendency",
    "href": "content/02-content.html#measures-of-central-tendency",
    "title": "Statistics in R",
    "section": "Measures of Central Tendency ü¶î",
    "text": "Measures of Central Tendency ü¶î\nArithmetic Mean\nThe arithmetic mean is calculated as:\n\\[\\bar{Y}=\\frac{Y_1+Y_2+\\ldots+Y_N}{N}=\\frac{\\sum_{i=1}^N Y_i}{N}\\]\nGeometric Mean\nThe geometric mean is calculated as:\n\\[\\bar{Y}_g=\\sqrt[N]{\\prod_{i=1}^N Y_i}\\]\nExample\nThe average can also be interpreted as meaning that if all elements were replaced by this value, the sum of the elements would remain the same.\nLet us see the case of indexes:\nThe GDP growth in one year was 5 percent and 5 percent in the following and 10 percent in the 3rd.\nWhat was the annual growth in the 3 years?\n\\[1.05 \\times 1.05 \\times 1.10 = \\sqrt[3]{1.21}=1.0656\\]\nHowever, the arithmetic mean equals 20/3 = 6.66.\nHarmonic Mean\nThe harmonic mean is calculated as:\n\\[\\bar{Y}_h=\\frac{N}{\\sum_{i=1}^N \\frac{1}{Y_i}}\\]\nExample\nConsider two car owners who seek to reduce their costs:\n\nAdam switches from a gas-guzzler of 12 mpg to a slightly less voracious guzzler that runs at 14 mpg.\nThe environmentally virtuous Beth switches from a Bon ss es from 30 mpg car to one that runs at 40 mpg.\n\nSuppose both drivers travel equal distances over a year. Who will save more gas by switching?\nSquared Mean\nThe squared mean is calculated as:\n\\[\\bar{Y_q}=\\sqrt{\\frac{\\sum_{i=1}^N Y_i^2}{N}}\\]"
  },
  {
    "objectID": "content/02-content.html#quantiles",
    "href": "content/02-content.html#quantiles",
    "title": "Statistics in R",
    "section": "Quantiles",
    "text": "Quantiles\nQuantiles are values that divide a dataset into equal groups. The types of quantiles include:\n\n\n\n\n\n\n\n# Groups\n      NAME\n    \n\n\n2\nMedian\n\n\n3\nTertiles\n\n\n4\nQuartiles\n\n\n5\nQuintiles\n\n\n10\nDeciles\n\n\n20\nVentiles\n\n\n100\nPercentiles\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThere will often be occasions when you need to reproduce a complete study or methodology for research or any other purpose. Generally, it‚Äôs not possible, and even the calculation of quantiles prevents precise reproduction, as everyone considers it completely obvious how to calculate the median or a quartile (and the choosen method is never documented). However, depending on the chosen methodology, we will obtain a very minimal but still different result.\n\nx &lt;- c(231, 45342, 2313, 213, 564, 874543, 92713, 3941, 31297, 2654, 4324, 6542) # random numbers\n\n\nquantile(x, probs = c(.25, .5, .75))\n\n     25%      50%      75% \n 1875.75  4132.50 34808.25 \n\n\n\nquantile(x, probs = c(.25, .5, .75), type = 2)\n\n    25%     50%     75% \n 1438.5  4132.5 38319.5"
  },
  {
    "objectID": "content/02-content.html#measures-of-variability",
    "href": "content/02-content.html#measures-of-variability",
    "title": "Statistics in R",
    "section": "Measures of Variability",
    "text": "Measures of Variability\nStandard Deviation\nStandard deviation is the average difference between the observations and the mean??\nIt was already mentioned that the sum of the deviations from the average is zero. If we divide zero by the number of observations it is still zero‚Ä¶\nHere comes the squared mean!\n\\[\\sigma=\\sqrt{\\frac{\\sum_{i=1}^N\\left(Y_i-\\bar{Y}\\right)^2}{N}}\\]\nVariance\nVariance is simply the square of standard deviation.\n\\[\\sigma^2=\\frac{\\sum_{i=1}^N\\left(Y_i-\\bar{Y}\\right)^2}{N}\\]\n\n\n\n\n\n\nNote\n\n\n\nIn many cases, we often use variance instead of standard deviation and not just for fun. If we recall what we learned about the ANOVA, variances are additive, while standard deviations are not."
  },
  {
    "objectID": "content/02-content.html#range",
    "href": "content/02-content.html#range",
    "title": "Statistics in R",
    "section": "Range",
    "text": "Range\nThe spread of your data from the lowest to the highest value.\nInterquartile range (IQR)\n\\[\\text{IQR}=Q_3-Q1\\]\nInterdecile range (IDR)\n\\[\\text{IDR}=D_9-D1\\]\nBoxplot\n\nboxplot(attrition$MonthlyIncome)\n\n\n\n\n\n\nThe boxplot is highly popular for describing a variable, as it displays numerous variables and effectively illustrates the differences based on a categorical feature (but we will discuss this further when we get there)."
  },
  {
    "objectID": "content/03-content.html",
    "href": "content/03-content.html",
    "title": "Dplyr & Tidyr basics",
    "section": "",
    "text": "Tidyverse contains the most important packages that you‚Äôre likely to use in everyday data analyses:\n\n\n\nggplot2, for data visualisation. (Later on, as we direct our attention towards the process of visualization)\ndplyr, for data manipulation. (now)\ntidyr, for tidy data. (today)\nreadr, for data import.\npurrr, for functional programming.\ntibble, for tibbles (modernized data frames).\nstringr, for strings.\nforcats, for factors.\n\n\n\n\n\nTidyverse works similarly to any other package, but when activated, 8 packages are activated at the same time. The following message should appear then on the console:\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.2 ‚îÄ‚îÄ\n‚úî ggplot2 3.4.3     ‚úî purrr   1.0.2\n‚úî tibble  3.2.1     ‚úî dplyr   1.1.2\n‚úî tidyr   1.3.0     ‚úî stringr 1.5.0\n‚úî readr   2.1.4     ‚úî forcats 1.0.0\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "content/03-content.html#the-tidyverse",
    "href": "content/03-content.html#the-tidyverse",
    "title": "Dplyr & Tidyr basics",
    "section": "",
    "text": "Tidyverse contains the most important packages that you‚Äôre likely to use in everyday data analyses:\n\n\n\nggplot2, for data visualisation. (Later on, as we direct our attention towards the process of visualization)\ndplyr, for data manipulation. (now)\ntidyr, for tidy data. (today)\nreadr, for data import.\npurrr, for functional programming.\ntibble, for tibbles (modernized data frames).\nstringr, for strings.\nforcats, for factors.\n\n\n\n\n\nTidyverse works similarly to any other package, but when activated, 8 packages are activated at the same time. The following message should appear then on the console:\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.2 ‚îÄ‚îÄ\n‚úî ggplot2 3.4.3     ‚úî purrr   1.0.2\n‚úî tibble  3.2.1     ‚úî dplyr   1.1.2\n‚úî tidyr   1.3.0     ‚úî stringr 1.5.0\n‚úî readr   2.1.4     ‚úî forcats 1.0.0\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "content/03-content.html#dplyr-functions",
    "href": "content/03-content.html#dplyr-functions",
    "title": "Dplyr & Tidyr basics",
    "section": "Dplyr functions",
    "text": "Dplyr functions\nselect() Extract the specified columns as a table.\n\nselect(.data = fertility_df, LOCATION, TIME, Value)\n\n# A tibble: 3,294 √ó 3\n   LOCATION  TIME Value\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 AUS       1960  3.45\n 2 AUS       1961  3.55\n 3 AUS       1962  3.43\n 4 AUS       1963  3.34\n 5 AUS       1964  3.15\n 6 AUS       1965  2.97\n 7 AUS       1966  2.89\n 8 AUS       1967  2.85\n 9 AUS       1968  2.89\n10 AUS       1969  2.89\n# ‚Ñπ 3,284 more rows\n\n\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, Value) # using col names\n\n# A tibble: 3,294 √ó 3\n   LOCATION  TIME Value\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 AUS       1960  3.45\n 2 AUS       1961  3.55\n 3 AUS       1962  3.43\n 4 AUS       1963  3.34\n 5 AUS       1964  3.15\n 6 AUS       1965  2.97\n 7 AUS       1966  2.89\n 8 AUS       1967  2.85\n 9 AUS       1968  2.89\n10 AUS       1969  2.89\n# ‚Ñπ 3,284 more rows\n\n\nIdentical outcome\n\nfertility_df %&gt;% \n  select(1, 6:7) # by column numbers\n\n# A tibble: 3,294 √ó 3\n   LOCATION  TIME Value\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 AUS       1960  3.45\n 2 AUS       1961  3.55\n 3 AUS       1962  3.43\n 4 AUS       1963  3.34\n 5 AUS       1964  3.15\n 6 AUS       1965  2.97\n 7 AUS       1966  2.89\n 8 AUS       1967  2.85\n 9 AUS       1968  2.89\n10 AUS       1969  2.89\n# ‚Ñπ 3,284 more rows\n\n\n\nfertility_df %&gt;% \n  select(- (2:5), -`Flag Codes`) # drop these\n\n# A tibble: 3,294 √ó 3\n   LOCATION  TIME Value\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 AUS       1960  3.45\n 2 AUS       1961  3.55\n 3 AUS       1962  3.43\n 4 AUS       1963  3.34\n 5 AUS       1964  3.15\n 6 AUS       1965  2.97\n 7 AUS       1966  2.89\n 8 AUS       1967  2.85\n 9 AUS       1968  2.89\n10 AUS       1969  2.89\n# ‚Ñπ 3,284 more rows\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn this case, ‚Äú\" had to be used when referencing the variable namedflag code. The reason for this is that in dplyr expressions, variables containing spaces or special characters always need to be enclosed in \"‚Äù To overcome the difficulties arising from special characters in the headers, the clean_names function in the janitor package provides a helpful solution, allowing all variables of a table to be renamed at once.\n\n\n\nrename() renames columns.\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, Value) %&gt;% \n  rename(geo = LOCATION, fertility = Value)\n\n# A tibble: 3,294 √ó 3\n   geo    TIME fertility\n   &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 AUS    1960      3.45\n 2 AUS    1961      3.55\n 3 AUS    1962      3.43\n 4 AUS    1963      3.34\n 5 AUS    1964      3.15\n 6 AUS    1965      2.97\n 7 AUS    1966      2.89\n 8 AUS    1967      2.85\n 9 AUS    1968      2.89\n10 AUS    1969      2.89\n# ‚Ñπ 3,284 more rows\n\n\n.content-box-green[ HINT: ]\n\n\n\n\n\n\nTip\n\n\n\nYou can do it in 1 single step.\n\nfertility_df %&gt;% \n  select(\n    geo = LOCATION, # select and rename\n    TIME,\n    fertility = Value\n  )\n\n# A tibble: 3,294 √ó 3\n   geo    TIME fertility\n   &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 AUS    1960      3.45\n 2 AUS    1961      3.55\n 3 AUS    1962      3.43\n 4 AUS    1963      3.34\n 5 AUS    1964      3.15\n 6 AUS    1965      2.97\n 7 AUS    1966      2.89\n 8 AUS    1967      2.85\n 9 AUS    1968      2.89\n10 AUS    1969      2.89\n# ‚Ñπ 3,284 more rows\n\n\n\n\nMutate\nmutate() computes new column(s).\ncountrycode::countrycode() Translates from one country naming scheme to another.\n\nlibrary(countrycode)\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, Value) %&gt;% \n  mutate(\n    country_name = countrycode(LOCATION, \"iso3c\", \"country.name\"),\n    continent = countrycode(LOCATION, \"iso3c\", \"continent\")\n  )\n\n# A tibble: 3,294 √ó 5\n   LOCATION  TIME Value country_name continent\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;    \n 1 AUS       1960  3.45 Australia    Oceania  \n 2 AUS       1961  3.55 Australia    Oceania  \n 3 AUS       1962  3.43 Australia    Oceania  \n 4 AUS       1963  3.34 Australia    Oceania  \n 5 AUS       1964  3.15 Australia    Oceania  \n 6 AUS       1965  2.97 Australia    Oceania  \n 7 AUS       1966  2.89 Australia    Oceania  \n 8 AUS       1967  2.85 Australia    Oceania  \n 9 AUS       1968  2.89 Australia    Oceania  \n10 AUS       1969  2.89 Australia    Oceania  \n# ‚Ñπ 3,284 more rows\n\n\n\nlibrary(countrycode)\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, Value) %&gt;% \n  mutate(\n    country_name = countrycode(LOCATION, \"iso3c\", \"country.name\"),\n    continent = countrycode(LOCATION, \"iso3c\", \"continent\"),\n    LOCATION = countrycode(LOCATION, \"iso3c\", \"iso2c\") # modify the original column\n  )\n\n# A tibble: 3,294 √ó 5\n   LOCATION  TIME Value country_name continent\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;    \n 1 AU        1960  3.45 Australia    Oceania  \n 2 AU        1961  3.55 Australia    Oceania  \n 3 AU        1962  3.43 Australia    Oceania  \n 4 AU        1963  3.34 Australia    Oceania  \n 5 AU        1964  3.15 Australia    Oceania  \n 6 AU        1965  2.97 Australia    Oceania  \n 7 AU        1966  2.89 Australia    Oceania  \n 8 AU        1967  2.85 Australia    Oceania  \n 9 AU        1968  2.89 Australia    Oceania  \n10 AU        1969  2.89 Australia    Oceania  \n# ‚Ñπ 3,284 more rows\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you specify a single value instead of a vector, then the values will be repeated by the number of rows.\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, Value) %&gt;% \n  mutate(avg_fertility = mean(Value))\n\n# A tibble: 3,294 √ó 4\n   LOCATION  TIME Value avg_fertility\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n 1 AUS       1960  3.45          2.34\n 2 AUS       1961  3.55          2.34\n 3 AUS       1962  3.43          2.34\n 4 AUS       1963  3.34          2.34\n 5 AUS       1964  3.15          2.34\n 6 AUS       1965  2.97          2.34\n 7 AUS       1966  2.89          2.34\n 8 AUS       1967  2.85          2.34\n 9 AUS       1968  2.89          2.34\n10 AUS       1969  2.89          2.34\n# ‚Ñπ 3,284 more rows\n\n\n\n\nTips for mutate\nifelse\nifelse() returns a vector filled with elements selected from either yes or no depending on the condition.\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, Value) %&gt;% \n  mutate(\n    avg_fertility = mean(Value),\n    above_avg = Value &gt; avg_fertility,\n    high = ifelse(above_avg, \"yes, above the avg\", \"no\")\n  )\n\n# A tibble: 3,294 √ó 6\n   LOCATION  TIME Value avg_fertility above_avg high              \n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt; &lt;lgl&gt;     &lt;chr&gt;             \n 1 AUS       1960  3.45          2.34 TRUE      yes, above the avg\n 2 AUS       1961  3.55          2.34 TRUE      yes, above the avg\n 3 AUS       1962  3.43          2.34 TRUE      yes, above the avg\n 4 AUS       1963  3.34          2.34 TRUE      yes, above the avg\n 5 AUS       1964  3.15          2.34 TRUE      yes, above the avg\n 6 AUS       1965  2.97          2.34 TRUE      yes, above the avg\n 7 AUS       1966  2.89          2.34 TRUE      yes, above the avg\n 8 AUS       1967  2.85          2.34 TRUE      yes, above the avg\n 9 AUS       1968  2.89          2.34 TRUE      yes, above the avg\n10 AUS       1969  2.89          2.34 TRUE      yes, above the avg\n# ‚Ñπ 3,284 more rows\n\n\nifelse() returns a vector filled with elements selected from either yes or no depending on the condition.\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, Value) %&gt;% \n  mutate(\n    avg_fertility = mean(Value),\n    above_avg = Value &gt; avg_fertility,\n    high = ifelse(above_avg, \"yes, above the avg\", \"no\")\n  )\n\n# A tibble: 3,294 √ó 6\n   LOCATION  TIME Value avg_fertility above_avg high              \n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt; &lt;lgl&gt;     &lt;chr&gt;             \n 1 AUS       1960  3.45          2.34 TRUE      yes, above the avg\n 2 AUS       1961  3.55          2.34 TRUE      yes, above the avg\n 3 AUS       1962  3.43          2.34 TRUE      yes, above the avg\n 4 AUS       1963  3.34          2.34 TRUE      yes, above the avg\n 5 AUS       1964  3.15          2.34 TRUE      yes, above the avg\n 6 AUS       1965  2.97          2.34 TRUE      yes, above the avg\n 7 AUS       1966  2.89          2.34 TRUE      yes, above the avg\n 8 AUS       1967  2.85          2.34 TRUE      yes, above the avg\n 9 AUS       1968  2.89          2.34 TRUE      yes, above the avg\n10 AUS       1969  2.89          2.34 TRUE      yes, above the avg\n# ‚Ñπ 3,284 more rows\n\n\nWith a single step:\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, Value) %&gt;% \n  mutate(\n    high = ifelse(Value &gt; mean(Value), \"yes, above the avg\", \"no\")\n  )\n\n# A tibble: 3,294 √ó 4\n   LOCATION  TIME Value high              \n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;             \n 1 AUS       1960  3.45 yes, above the avg\n 2 AUS       1961  3.55 yes, above the avg\n 3 AUS       1962  3.43 yes, above the avg\n 4 AUS       1963  3.34 yes, above the avg\n 5 AUS       1964  3.15 yes, above the avg\n 6 AUS       1965  2.97 yes, above the avg\n 7 AUS       1966  2.89 yes, above the avg\n 8 AUS       1967  2.85 yes, above the avg\n 9 AUS       1968  2.89 yes, above the avg\n10 AUS       1969  2.89 yes, above the avg\n# ‚Ñπ 3,284 more rows\n\n\ncase_when\ncase_when() Vectorise multiple if_else() statements (return the first value associated with TRUE condition)\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, Value) %&gt;% \n  mutate(\n     fertility_level = case_when(\n      Value &gt;= 5 ~ \"very high\",\n      Value &gt;= 3 ~ \"high\", \n      Value &gt;= 2.1 ~ \"above replacement r\",\n      Value &gt;= 1.3 ~ \"below replacemnt r\",\n      TRUE ~ \"very low\"\n    )\n  )\n\n# A tibble: 3,294 √ó 4\n   LOCATION  TIME Value fertility_level    \n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              \n 1 AUS       1960  3.45 high               \n 2 AUS       1961  3.55 high               \n 3 AUS       1962  3.43 high               \n 4 AUS       1963  3.34 high               \n 5 AUS       1964  3.15 high               \n 6 AUS       1965  2.97 above replacement r\n 7 AUS       1966  2.89 above replacement r\n 8 AUS       1967  2.85 above replacement r\n 9 AUS       1968  2.89 above replacement r\n10 AUS       1969  2.89 above replacement r\n# ‚Ñπ 3,284 more rows\n\n\nFrom the latest release:\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, Value) %&gt;% \n  mutate(\n     fertility_level = case_when(\n      Value &gt;= 5 ~ \"very high\",\n      Value &gt;= 3 ~ \"high\", \n      Value &gt;= 2.1 ~ \"above replacement r\",\n      Value &gt;= 1.3 ~ \"below replacemnt r\",\n      .default =  \"very low\"\n    )\n  )\n\n# A tibble: 3,294 √ó 4\n   LOCATION  TIME Value fertility_level    \n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              \n 1 AUS       1960  3.45 high               \n 2 AUS       1961  3.55 high               \n 3 AUS       1962  3.43 high               \n 4 AUS       1963  3.34 high               \n 5 AUS       1964  3.15 high               \n 6 AUS       1965  2.97 above replacement r\n 7 AUS       1966  2.89 above replacement r\n 8 AUS       1967  2.85 above replacement r\n 9 AUS       1968  2.89 above replacement r\n10 AUS       1969  2.89 above replacement r\n# ‚Ñπ 3,284 more rows\n\n\ntransmute() Compute new column(s), drop others. (= select + mutate)\n\nfertility_df %&gt;% \n  transmute(LOCATION, TIME, \n              fertility_level = case_when(\n                Value &gt;= 5 ~ \"very high\",\n                Value &gt;= 3 ~ \"high\", \n                Value &gt;= 2.1 ~ \"above replacement r\",\n                Value &gt;= 1.3 ~ \"below replacemnt r\",\n                TRUE ~ \"very low\" \n              )\n  )\n\nfilter() Extract rows that meet logical criteria.\n\nfertility_df %&gt;% \n  filter(LOCATION != \"EU\" & LOCATION != \"OAVG\")\n\n# A tibble: 3,170 √ó 8\n   LOCATION INDICATOR SUBJECT MEASURE   FREQUENCY  TIME Value `Flag Codes`\n   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;       \n 1 AUS      FERTILITY TOT     CHD_WOMAN A          1960  3.45 NA          \n 2 AUS      FERTILITY TOT     CHD_WOMAN A          1961  3.55 NA          \n 3 AUS      FERTILITY TOT     CHD_WOMAN A          1962  3.43 NA          \n 4 AUS      FERTILITY TOT     CHD_WOMAN A          1963  3.34 NA          \n 5 AUS      FERTILITY TOT     CHD_WOMAN A          1964  3.15 NA          \n 6 AUS      FERTILITY TOT     CHD_WOMAN A          1965  2.97 NA          \n 7 AUS      FERTILITY TOT     CHD_WOMAN A          1966  2.89 NA          \n 8 AUS      FERTILITY TOT     CHD_WOMAN A          1967  2.85 NA          \n 9 AUS      FERTILITY TOT     CHD_WOMAN A          1968  2.89 NA          \n10 AUS      FERTILITY TOT     CHD_WOMAN A          1969  2.89 NA          \n# ‚Ñπ 3,160 more rows\n\n\nslice() Select rows by position\n\nfertility_df %&gt;% \n  slice(2:5)\n\n# A tibble: 4 √ó 8\n  LOCATION INDICATOR SUBJECT MEASURE   FREQUENCY  TIME Value `Flag Codes`\n  &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;       \n1 AUS      FERTILITY TOT     CHD_WOMAN A          1961  3.55 NA          \n2 AUS      FERTILITY TOT     CHD_WOMAN A          1962  3.43 NA          \n3 AUS      FERTILITY TOT     CHD_WOMAN A          1963  3.34 NA          \n4 AUS      FERTILITY TOT     CHD_WOMAN A          1964  3.15 NA          \n\n\nsample_n Select n random rows.\n\nfertility_df %&gt;% \n  sample_n(6)\n\n# A tibble: 6 √ó 8\n  LOCATION INDICATOR SUBJECT MEASURE   FREQUENCY  TIME Value `Flag Codes`\n  &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;       \n1 PRT      FERTILITY TOT     CHD_WOMAN A          1999  1.51 NA          \n2 ROU      FERTILITY TOT     CHD_WOMAN A          2003  1.3  NA          \n3 ESP      FERTILITY TOT     CHD_WOMAN A          2020  1.19 NA          \n4 USA      FERTILITY TOT     CHD_WOMAN A          1977  1.79 NA          \n5 ISR      FERTILITY TOT     CHD_WOMAN A          1988  3.06 NA          \n6 GRC      FERTILITY TOT     CHD_WOMAN A          1971  2.32 NA          \n\n\n\n\n\n\n\n\nTip\n\n\n\nIn order to ensure that R consistently generates the same ‚Äúrandom‚Äù sample, you can utilize the set.seed function. This function has the ability to influence all sources of randomness within your ongoing session. Use it for your research project, if reproducibility is important.\n\n\narrange() Order rows by values of a column or columns (low to high), use with desc() to order from high to low.\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, \n         Value) %&gt;% \n  arrange(Value)\n\n# A tibble: 3,294 √ó 3\n   LOCATION  TIME Value\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 KOR       2021  0.81\n 2 KOR       2020  0.84\n 3 KOR       2019  0.92\n 4 KOR       2018  0.98\n 5 KOR       2017  1.05\n 6 KOR       2005  1.09\n 7 BGR       1997  1.09\n 8 LVA       1998  1.1 \n 9 LVA       1997  1.11\n10 BGR       1998  1.11\n# ‚Ñπ 3,284 more rows\n\n\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, \n         Value) %&gt;% \n  arrange(desc(Value))\n\n# A tibble: 3,294 √ó 3\n   LOCATION  TIME Value\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 SAU       1964  7.67\n 2 SAU       1965  7.66\n 3 SAU       1966  7.66\n 4 SAU       1967  7.66\n 5 SAU       1963  7.65\n 6 SAU       1962  7.64\n 7 SAU       1960  7.63\n 8 SAU       1961  7.63\n 9 SAU       1968  7.63\n10 SAU       1969  7.6 \n# ‚Ñπ 3,284 more rows\n\n\nGrouping\ngroup_by() Create a ‚Äúgrouped‚Äù copy of a table.\nsummarise() Compute table of summaries.\n\nfertility_df %&gt;% \n  group_by(LOCATION)\n\n# A tibble: 3,294 √ó 8\n# Groups:   LOCATION [54]\n   LOCATION INDICATOR SUBJECT MEASURE   FREQUENCY  TIME Value `Flag Codes`\n   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;       \n 1 AUS      FERTILITY TOT     CHD_WOMAN A          1960  3.45 NA          \n 2 AUS      FERTILITY TOT     CHD_WOMAN A          1961  3.55 NA          \n 3 AUS      FERTILITY TOT     CHD_WOMAN A          1962  3.43 NA          \n 4 AUS      FERTILITY TOT     CHD_WOMAN A          1963  3.34 NA          \n 5 AUS      FERTILITY TOT     CHD_WOMAN A          1964  3.15 NA          \n 6 AUS      FERTILITY TOT     CHD_WOMAN A          1965  2.97 NA          \n 7 AUS      FERTILITY TOT     CHD_WOMAN A          1966  2.89 NA          \n 8 AUS      FERTILITY TOT     CHD_WOMAN A          1967  2.85 NA          \n 9 AUS      FERTILITY TOT     CHD_WOMAN A          1968  2.89 NA          \n10 AUS      FERTILITY TOT     CHD_WOMAN A          1969  2.89 NA          \n# ‚Ñπ 3,284 more rows\n\n\n\nfertility_df %&gt;% \n  group_by(LOCATION) %&gt;% \n  summarise(avg_fertility = mean(Value))\n\n# A tibble: 54 √ó 2\n   LOCATION avg_fertility\n   &lt;chr&gt;            &lt;dbl&gt;\n 1 ARG               2.79\n 2 AUS               2.13\n 3 AUT               1.72\n 4 BEL               1.84\n 5 BGR               1.76\n 6 BRA               3.29\n 7 CAN               1.91\n 8 CHE               1.70\n 9 CHL               2.68\n10 CHN               2.91\n# ‚Ñπ 44 more rows\n\n\nExample: number of observations\n\nfertility_df %&gt;% \n  group_by(LOCATION) %&gt;% \n  summarise(n_obs = n())\n\n# A tibble: 54 √ó 2\n   LOCATION n_obs\n   &lt;chr&gt;    &lt;int&gt;\n 1 ARG         62\n 2 AUS         62\n 3 AUT         62\n 4 BEL         62\n 5 BGR         62\n 6 BRA         62\n 7 CAN         62\n 8 CHE         62\n 9 CHL         62\n10 CHN         62\n# ‚Ñπ 44 more rows\n\n\n\n\n\n\n\n\nWarning\n\n\n\nEvery other manipulation will behave differently if the data frame is grouped (We frequently utilise this).\n\n\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, Value) %&gt;% \n  group_by(LOCATION) %&gt;% \n  mutate(avg_fertility = mean(Value))\n\n# A tibble: 3,294 √ó 4\n# Groups:   LOCATION [54]\n   LOCATION  TIME Value avg_fertility\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n 1 AUS       1960  3.45          2.13\n 2 AUS       1961  3.55          2.13\n 3 AUS       1962  3.43          2.13\n 4 AUS       1963  3.34          2.13\n 5 AUS       1964  3.15          2.13\n 6 AUS       1965  2.97          2.13\n 7 AUS       1966  2.89          2.13\n 8 AUS       1967  2.85          2.13\n 9 AUS       1968  2.89          2.13\n10 AUS       1969  2.89          2.13\n# ‚Ñπ 3,284 more rows\n\n\nExercise\nIn which decade did the average fertility rate of European countries decrease the most? (in absolute magnitude)\n\n\n\n\n\n\nTip\n\n\n\nUse the first() and the last() function to calculate difference in a period.\n\n\n‚Ä¶if, ‚Ä¶at, ‚Ä¶all (deprecated)\nSome of the previously listed functions have extended versions.\nSelect all columns where the condition is TRUE\n\nfertility_df %&gt;% \n  select_if(is.numeric)\n\n# A tibble: 3,294 √ó 2\n    TIME Value\n   &lt;dbl&gt; &lt;dbl&gt;\n 1  1960  3.45\n 2  1961  3.55\n 3  1962  3.43\n 4  1963  3.34\n 5  1964  3.15\n 6  1965  2.97\n 7  1966  2.89\n 8  1967  2.85\n 9  1968  2.89\n10  1969  2.89\n# ‚Ñπ 3,284 more rows\n\n\nModify all columns where the condition is true\n\n\n\n\n\n\nImportant\n\n\n\nYou will utilize lambda-style functions in this context. This implies that after the tilde (~), you may reference the current element using the dot operator (.).\n\n\n\nfertility_df %&gt;% \n  mutate_if(is.numeric, ~ . * 10)\n\n# A tibble: 3,294 √ó 8\n   LOCATION INDICATOR SUBJECT MEASURE   FREQUENCY  TIME Value `Flag Codes`\n   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;       \n 1 AUS      FERTILITY TOT     CHD_WOMAN A         19600  34.5 NA          \n 2 AUS      FERTILITY TOT     CHD_WOMAN A         19610  35.5 NA          \n 3 AUS      FERTILITY TOT     CHD_WOMAN A         19620  34.3 NA          \n 4 AUS      FERTILITY TOT     CHD_WOMAN A         19630  33.4 NA          \n 5 AUS      FERTILITY TOT     CHD_WOMAN A         19640  31.5 NA          \n 6 AUS      FERTILITY TOT     CHD_WOMAN A         19650  29.7 NA          \n 7 AUS      FERTILITY TOT     CHD_WOMAN A         19660  28.9 NA          \n 8 AUS      FERTILITY TOT     CHD_WOMAN A         19670  28.5 NA          \n 9 AUS      FERTILITY TOT     CHD_WOMAN A         19680  28.9 NA          \n10 AUS      FERTILITY TOT     CHD_WOMAN A         19690  28.9 NA          \n# ‚Ñπ 3,284 more rows\n\n\nAcross/where\nThe concepts of ‚Äúacross‚Äù and ‚Äúwhere‚Äù represent relatively new functions that offer an alternative to the previously used if/at/all notations. These functions provide us with a more efficient and reliable approach, eliminating the need for dependence on the aforementioned notations.\n\nfertility_df |&gt; \n  select(LOCATION, where(is.numeric))\n\n# A tibble: 3,294 √ó 3\n   LOCATION  TIME Value\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 AUS       1960  3.45\n 2 AUS       1961  3.55\n 3 AUS       1962  3.43\n 4 AUS       1963  3.34\n 5 AUS       1964  3.15\n 6 AUS       1965  2.97\n 7 AUS       1966  2.89\n 8 AUS       1967  2.85\n 9 AUS       1968  2.89\n10 AUS       1969  2.89\n# ‚Ñπ 3,284 more rows\n\n\n\nfertility_df |&gt; \n  select(LOCATION, where(is.numeric)) |&gt; \n  mutate(\n    across(is.numeric, mean)\n  )\n\n# A tibble: 3,294 √ó 3\n   LOCATION  TIME Value\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 AUS      1991.  2.34\n 2 AUS      1991.  2.34\n 3 AUS      1991.  2.34\n 4 AUS      1991.  2.34\n 5 AUS      1991.  2.34\n 6 AUS      1991.  2.34\n 7 AUS      1991.  2.34\n 8 AUS      1991.  2.34\n 9 AUS      1991.  2.34\n10 AUS      1991.  2.34\n# ‚Ñπ 3,284 more rows\n\n\n\nfertility_df |&gt; \n  select(LOCATION, where(is.numeric)) |&gt; \n  mutate(\n    across(is.numeric, list(avg = mean), .names = \"{.col}_{.fn}\")\n  )\n\n# A tibble: 3,294 √ó 5\n   LOCATION  TIME Value TIME_avg Value_avg\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 AUS       1960  3.45    1991.      2.34\n 2 AUS       1961  3.55    1991.      2.34\n 3 AUS       1962  3.43    1991.      2.34\n 4 AUS       1963  3.34    1991.      2.34\n 5 AUS       1964  3.15    1991.      2.34\n 6 AUS       1965  2.97    1991.      2.34\n 7 AUS       1966  2.89    1991.      2.34\n 8 AUS       1967  2.85    1991.      2.34\n 9 AUS       1968  2.89    1991.      2.34\n10 AUS       1969  2.89    1991.      2.34\n# ‚Ñπ 3,284 more rows"
  },
  {
    "objectID": "content/03-content.html#tidyr-functions",
    "href": "content/03-content.html#tidyr-functions",
    "title": "Dplyr & Tidyr basics",
    "section": "Tidyr functions",
    "text": "Tidyr functions\nThe goal of tidyr is to help you create tidy data. Tidy data is data where:\n\nEvery column is variable.\nEvery row is an observation.\nEvery cell is a single value.\n\n\n\n\n\n\n\nCHEATSHEETS\n\n\n\n\n\n\n\nWide/long\nDatasets are in long on wide format (but never in the currently desired one) üòÜ\nThe following example is the one we saw last week to present the apply functions. In excel you probably operated with tables in wide format, but in R we always prefer the long format.\n\nfertility_wide &lt;- fertility_df %&gt;% \n  select(LOCATION, TIME, Value) %&gt;% \n  pivot_wider(names_from = LOCATION, values_from = Value)\n\nfertility_wide\n\n# A tibble: 63 √ó 55\n    TIME   AUS   AUT   BEL   CAN   CZE   DNK   FIN   FRA   DEU   GRC   HUN   ISL\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  1960  3.45  2.69  2.54  3.9   2.11  2.54  2.71  2.74  2.37  2.23  2.02  4.26\n 2  1961  3.55  2.78  2.63  3.84  2.13  2.55  2.65  2.82  2.44  2.13  1.94  3.88\n 3  1962  3.43  2.8   2.59  3.76  2.14  2.54  2.66  2.8   2.44  2.16  1.79  3.98\n 4  1963  3.34  2.82  2.68  3.67  2.33  2.64  2.66  2.9   2.51  2.14  1.82  3.98\n 5  1964  3.15  2.79  2.71  3.5   2.36  2.6   2.58  2.91  2.53  2.24  1.8   3.86\n 6  1965  2.97  2.7   2.61  3.15  2.18  2.61  2.46  2.85  2.5   2.25  1.81  3.71\n 7  1966  2.89  2.66  2.52  2.81  2.01  2.62  2.4   2.8   2.51  2.32  1.88  3.58\n 8  1967  2.85  2.62  2.41  2.6   1.9   2.35  2.32  2.67  2.45  2.45  2.01  3.28\n 9  1968  2.89  2.58  2.31  2.45  1.83  2.12  2.15  2.59  2.36  2.42  2.06  3.07\n10  1969  2.89  2.49  2.27  2.4   1.86  2     1.94  2.53  2.21  2.36  2.04  2.99\n# ‚Ñπ 53 more rows\n# ‚Ñπ 42 more variables: IRL &lt;dbl&gt;, ITA &lt;dbl&gt;, JPN &lt;dbl&gt;, KOR &lt;dbl&gt;, LUX &lt;dbl&gt;,\n#   MEX &lt;dbl&gt;, NLD &lt;dbl&gt;, NZL &lt;dbl&gt;, NOR &lt;dbl&gt;, POL &lt;dbl&gt;, PRT &lt;dbl&gt;,\n#   SVK &lt;dbl&gt;, ESP &lt;dbl&gt;, SWE &lt;dbl&gt;, CHE &lt;dbl&gt;, TUR &lt;dbl&gt;, GBR &lt;dbl&gt;,\n#   USA &lt;dbl&gt;, BRA &lt;dbl&gt;, CHL &lt;dbl&gt;, CHN &lt;dbl&gt;, EST &lt;dbl&gt;, IND &lt;dbl&gt;,\n#   IDN &lt;dbl&gt;, ISR &lt;dbl&gt;, RUS &lt;dbl&gt;, SVN &lt;dbl&gt;, ZAF &lt;dbl&gt;, COL &lt;dbl&gt;,\n#   LVA &lt;dbl&gt;, LTU &lt;dbl&gt;, ARG &lt;dbl&gt;, BGR &lt;dbl&gt;, HRV &lt;dbl&gt;, CYP &lt;dbl&gt;, ‚Ä¶\n\n\nYes, pivot_longer is to convert back the table into long format.\n\nfertility_wide %&gt;% \n  pivot_longer(\n    cols = - 1, # all except the 1st\n    # 2:last_col()\n    # AUS, AUT, ... also work\n    names_to = \"LOCATION\",\n    values_to = \"fertility\"\n  )\n\n# A tibble: 3,402 √ó 3\n    TIME LOCATION fertility\n   &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1  1960 AUS           3.45\n 2  1960 AUT           2.69\n 3  1960 BEL           2.54\n 4  1960 CAN           3.9 \n 5  1960 CZE           2.11\n 6  1960 DNK           2.54\n 7  1960 FIN           2.71\n 8  1960 FRA           2.74\n 9  1960 DEU           2.37\n10  1960 GRC           2.23\n# ‚Ñπ 3,392 more rows"
  },
  {
    "objectID": "content/03-content.html#mutating-joins",
    "href": "content/03-content.html#mutating-joins",
    "title": "Dplyr & Tidyr basics",
    "section": "Mutating joins",
    "text": "Mutating joins\nJoins are fundamental operations that merge rows from two or more tables based on the common columns (keys). These operations play a crucial role in data manipulation and analysis, especially when dealing with multiple data sources (which is almost always the case).\nThe difference among the functions is how they handle values from the shared columns that appear in only one of the two tables.\n\n# Create two example data frames\ndf1 &lt;- data.frame(ID = c(1, 2, 3), Name = c(\"Alice\", \"Bob\", \"Charlie\"))\ndf2 &lt;- data.frame(ID = c(2, 3, 4), Score = c(90, 85, 88))\n\n\ninner_join(df1, df2, by = \"ID\")\n\n  ID    Name Score\n1  2     Bob    90\n2  3 Charlie    85\n\nleft_join(df1, df2, by = \"ID\")\n\n  ID    Name Score\n1  1   Alice    NA\n2  2     Bob    90\n3  3 Charlie    85\n\nright_join(df1, df2, by = \"ID\")\n\n  ID    Name Score\n1  2     Bob    90\n2  3 Charlie    85\n3  4    &lt;NA&gt;    88\n\nfull_join(df1, df2, by = \"ID\")\n\n  ID    Name Score\n1  1   Alice    NA\n2  2     Bob    90\n3  3 Charlie    85\n4  4    &lt;NA&gt;    88\n\nsemi_join(df1, df2, by = \"ID\")\n\n  ID    Name\n1  2     Bob\n2  3 Charlie\n\nanti_join(df1, df2, by = \"ID\")\n\n  ID  Name\n1  1 Alice"
  },
  {
    "objectID": "content/04-content.html",
    "href": "content/04-content.html",
    "title": "A whole game",
    "section": "",
    "text": "You can access an Excel spreadsheet on infant mortality downloaded from Eurostat. We discovered last week that utilizing the Eurostat data is significantly facilitated by the eurostat package; however, for the purpose of practical demonstration, we now need to showcase the multitude of cleaning procedures required to establish links between series originating from diverse sources.\n\nLets download the infant mortality rates from Eurostat (in Excel format) and link it with the fertility_df\n\ninfant_mortality_df &lt;- readxl::read_excel(\"../data/demo_minfind.xls\")\n\nThe issue here is that there are multiple tables on the first sheet (not a rare thing).\n\ninfant_mortality_df\n\n# A tibble: 339 √ó 62\n   Infant mortality rate‚Ä¶¬π ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9  ...10\n   &lt;chr&gt;                   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 &lt;NA&gt;                    &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 2 Last update             4474‚Ä¶ &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 3 Extracted on            4482‚Ä¶ &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 4 Source of data          Euro‚Ä¶ &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 5 &lt;NA&gt;                    &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 6 INDIC_DE                Earl‚Ä¶ &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 7 UNIT                    Rate  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 8 &lt;NA&gt;                    &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 9 GEO/TIME                1960  1961  1962  1963  1964  1965  1966  1967  1968 \n10 European Union - 27 co‚Ä¶ :     :     :     :     :     :     :     :     :    \n# ‚Ñπ 329 more rows\n# ‚Ñπ abbreviated name: ¬π‚Äã`Infant mortality rates [demo_minfind]`\n# ‚Ñπ 52 more variables: ...11 &lt;chr&gt;, ...12 &lt;chr&gt;, ...13 &lt;chr&gt;, ...14 &lt;chr&gt;,\n#   ...15 &lt;chr&gt;, ...16 &lt;chr&gt;, ...17 &lt;chr&gt;, ...18 &lt;chr&gt;, ...19 &lt;chr&gt;,\n#   ...20 &lt;chr&gt;, ...21 &lt;chr&gt;, ...22 &lt;chr&gt;, ...23 &lt;chr&gt;, ...24 &lt;chr&gt;,\n#   ...25 &lt;chr&gt;, ...26 &lt;chr&gt;, ...27 &lt;chr&gt;, ...28 &lt;chr&gt;, ...29 &lt;chr&gt;,\n#   ...30 &lt;chr&gt;, ...31 &lt;chr&gt;, ...32 &lt;chr&gt;, ...33 &lt;chr&gt;, ...34 &lt;chr&gt;, ‚Ä¶\n\n\nAfter exploring the data, you may realize that the name of the data is always in the second column. Our table starts after where find the ‚ÄúInfant mortality rate‚Äù.\n\npull(infant_mortality_df, 2) # 2nd column as vector\n\n  [1] NA                              \"44742.521828703699\"           \n  [3] \"44822.86385927083\"             \"Eurostat\"                     \n  [5] NA                              \"Early neonatal mortality rate\"\n  [7] \"Rate\"                          NA                             \n  [9] \"1960\"                          \":\"                            \n [11] \":\"                             \":\"                            \n [13] \":\"                             \":\"                            \n [15] \"17.100000000000001\"            \"10.9\"                         \n [17] \"10.699999999999999\"            \"13.9\"                         \n [19] \"20.800000000000001\"            \"19.699999999999999\"           \n [21] \"9.5999999999999996\"            \"16.100000000000001\"           \n [23] \"12.300000000000001\"            \"15.9\"                         \n [25] \":\"                             \"14.6\"                         \n [27] \"21\"                            \"17.800000000000001\"           \n [29] \":\"                             \":\"                            \n [31] \"7.2999999999999998\"            \"16.300000000000001\"           \n [33] \"22.100000000000001\"            \":\"                            \n [35] \"11.9\"                          \"20.199999999999999\"           \n [37] \":\"                             \"15\"                           \n [39] \":\"                             \"15.6\"                         \n [41] \"10.300000000000001\"            \"12.6\"                         \n [43] \"11.800000000000001\"            \":\"                            \n [45] \":\"                             \":\"                            \n [47] \":\"                             \"7.0999999999999996\"           \n [49] \":\"                             \"9.9000000000000004\"           \n [51] \"14.4\"                          \"13.699999999999999\"           \n [53] \":\"                             \"18.600000000000001\"           \n [55] \":\"                             \":\"                            \n [57] \":\"                             \":\"                            \n [59] \":\"                             \"17.300000000000001\"           \n [61] \":\"                             \":\"                            \n [63] \":\"                             \":\"                            \n [65] \":\"                             \":\"                            \n [67] \":\"                             \":\"                            \n [69] NA                              NA                             \n [71] \"not available\"                 NA                             \n [73] \"Infant mortality rate\"         \"Rate\"                         \n [75] NA                              \"1960\"                         \n [77] \":\"                             \":\"                            \n [79] \":\"                             \":\"                            \n [81] \":\"                             \"31.399999999999999\"           \n [83] \"45.100000000000001\"            \"20\"                           \n [85] \"21.5\"                          \"33.799999999999997\"           \n [87] \"35\"                            \"31.100000000000001\"           \n [89] \"29.300000000000001\"            \"40.100000000000001\"           \n [91] \"35.399999999999999\"            \":\"                            \n [93] \"27.699999999999999\"            \"70.400000000000006\"           \n [95] \"43.899999999999999\"            \":\"                            \n [97] \"27\"                            \"38\"                           \n [99] \"31.5\"                          \"47.600000000000001\"           \n[101] \"38.299999999999997\"            \"16.5\"                         \n[103] \"37.5\"                          \"56.100000000000001\"           \n[105] \"77.5\"                          \"75.700000000000003\"           \n[107] \"35.100000000000001\"            \"28.600000000000001\"           \n[109] \"21\"                            \"16.600000000000001\"           \n[111] \":\"                             \":\"                            \n[113] \":\"                             \"18.899999999999999\"           \n[115] \"13\"                            \"21.100000000000001\"           \n[117] \"16\"                            \"21.100000000000001\"           \n[119] \"22.5\"                          \":\"                            \n[121] \"114.59999999999999\"            \"83\"                           \n[123] \":\"                             \":\"                            \n[125] \":\"                             \":\"                            \n[127] \"107\"                           \"132.5\"                        \n[129] \":\"                             \":\"                            \n[131] \":\"                             \":\"                            \n[133] \":\"                             \":\"                            \n[135] \":\"                             NA                             \n[137] NA                              \"not available\"                \n[139] NA                              \"Late foetal mortality rate\"   \n[141] \"Rate\"                          NA                             \n[143] \"1960\"                          \":\"                            \n[145] \":\"                             \":\"                            \n[147] \":\"                             \":\"                            \n[149] \"15.300000000000001\"            \"12.199999999999999\"           \n[151] \"9.8000000000000007\"            \"12.4\"                         \n[153] \"15.300000000000001\"            \"15.5\"                         \n[155] \"13\"                            \"21.899999999999999\"           \n[157] \"14.300000000000001\"            \"27.300000000000001\"           \n[159] \":\"                             \"17\"                           \n[161] \"12.4\"                          \"24.5\"                         \n[163] \":\"                             \"10.699999999999999\"           \n[165] \"8.9000000000000004\"            \"16.100000000000001\"           \n[167] \"13.199999999999999\"            \":\"                            \n[169] \"14.9\"                          \"15\"                           \n[171] \"12.5\"                          \"26.5\"                         \n[173] \"15.9\"                          \"14.199999999999999\"           \n[175] \"10.9\"                          \"15.1\"                         \n[177] \"13.699999999999999\"            \":\"                            \n[179] \":\"                             \":\"                            \n[181] \"12.4\"                          \"12.699999999999999\"           \n[183] \"10.4\"                          \"13.9\"                         \n[185] \"11.4\"                          \"20.100000000000001\"           \n[187] \":\"                             \"9.8000000000000007\"           \n[189] \":\"                             \":\"                            \n[191] \":\"                             \":\"                            \n[193] \":\"                             \"8.6999999999999993\"           \n[195] \":\"                             \":\"                            \n[197] \":\"                             \":\"                            \n[199] \":\"                             \":\"                            \n[201] \":\"                             \":\"                            \n[203] NA                              NA                             \n[205] \"not available\"                 NA                             \n[207] \"Neonatal mortality rate\"       \"Rate\"                         \n[209] NA                              \"1960\"                         \n[211] \":\"                             \":\"                            \n[213] \":\"                             \":\"                            \n[215] \":\"                             \"20.5\"                         \n[217] \"19.399999999999999\"            \"13.1\"                         \n[219] \"16.100000000000001\"            \"23.899999999999999\"           \n[221] \"23.199999999999999\"            \":\"                            \n[223] \"20.399999999999999\"            \"19.5\"                         \n[225] \"20.199999999999999\"            \":\"                            \n[227] \"17.699999999999999\"            \"35.100000000000001\"           \n[229] \"23.899999999999999\"            \":\"                            \n[231] \"10.9\"                          \"13.4\"                         \n[233] \"19.100000000000001\"            \"27\"                           \n[235] \":\"                             \"13.5\"                         \n[237] \"24.600000000000001\"            \":\"                            \n[239] \"27.899999999999999\"            \":\"                            \n[241] \"20.399999999999999\"            \"14.1\"                         \n[243] \"14.4\"                          \"13.4\"                         \n[245] \":\"                             \":\"                            \n[247] \":\"                             \":\"                            \n[249] \"9.1999999999999993\"            \":\"                            \n[251] \"11.699999999999999\"            \"16.100000000000001\"           \n[253] \"16\"                            \":\"                            \n[255] \"41.399999999999999\"            \":\"                            \n[257] \":\"                             \":\"                            \n[259] \":\"                             \":\"                            \n[261] \"32.799999999999997\"            \":\"                            \n[263] \":\"                             \":\"                            \n[265] \":\"                             \":\"                            \n[267] \":\"                             \":\"                            \n[269] \":\"                             NA                             \n[271] NA                              \"not available\"                \n[273] NA                              \"Perinatal mortality rate\"     \n[275] \"Rate\"                          NA                             \n[277] \"1960\"                          \":\"                            \n[279] \":\"                             \":\"                            \n[281] \":\"                             \":\"                            \n[283] \"32.100000000000001\"            \"23\"                           \n[285] \"20.5\"                          \"26.199999999999999\"           \n[287] \"35.799999999999997\"            \"34.899999999999999\"           \n[289] \"22.5\"                          \"37.700000000000003\"           \n[291] \"26.399999999999999\"            \"42.799999999999997\"           \n[293] \":\"                             \"31.399999999999999\"           \n[295] \"33.100000000000001\"            \"41.899999999999999\"           \n[297] \":\"                             \":\"                            \n[299] \"16.100000000000001\"            \"32.200000000000003\"           \n[301] \"35\"                            \":\"                            \n[303] \"26.600000000000001\"            \"34.899999999999999\"           \n[305] \":\"                             \"41.100000000000001\"           \n[307] \":\"                             \"29.600000000000001\"           \n[309] \"21\"                            \"27.5\"                         \n[311] \"25.399999999999999\"            \":\"                            \n[313] \":\"                             \":\"                            \n[315] \":\"                             \"19.699999999999999\"           \n[317] \":\"                             \"23.699999999999999\"           \n[319] \"25.600000000000001\"            \"33.5\"                         \n[321] \":\"                             \"28.199999999999999\"           \n[323] \":\"                             \":\"                            \n[325] \":\"                             \":\"                            \n[327] \":\"                             \"25.800000000000001\"           \n[329] \":\"                             \":\"                            \n[331] \":\"                             \":\"                            \n[333] \":\"                             \":\"                            \n[335] \":\"                             \":\"                            \n[337] NA                              NA                             \n[339] \"not available\"                \n\n\nAfter exploring the data, you may realise that the name of the data is always in the second column. Our data starts where find we the ‚ÄúInfant mortality rate‚Äù. The first row of the table where we find ‚ÄúGEO/TIME‚Äù in the first column\n\ndata_start_index &lt;- pull(infant_mortality_df, 2) %&gt;%\n  purrr::detect_index(~ . == \"Infant mortality rate\" & !is.na(.))\n\ndata_start_index\n\n[1] 73\n\n\n\nstart_index &lt;- pull(infant_mortality_df, 1) %&gt;% \n  {. == \"GEO/TIME\" & !is.na(.)} %&gt;% \n  which() %&gt;% \n  detect(~ . &gt; data_start_index)\n\nAnd ends at the next empty cell (not ‚Äú:‚Äù)\n\nend_index &lt;- pull(infant_mortality_df, 2) %&gt;% \n  is.na() %&gt;%\n  which() %&gt;% \n  detect(~ . &gt; start_index)\n\n\ninfant_mortality_df %&gt;% \n  slice(start_index:end_index)\n\n# A tibble: 61 √ó 62\n   Infant mortality rate‚Ä¶¬π ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9  ...10\n   &lt;chr&gt;                   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 GEO/TIME                1960  1961  1962  1963  1964  1965  1966  1967  1968 \n 2 European Union - 27 co‚Ä¶ :     38.2‚Ä¶ 36.3‚Ä¶ 34.2‚Ä¶ 31.8‚Ä¶ 30    29.1‚Ä¶ 28.6‚Ä¶ 29.1‚Ä¶\n 3 European Union - 28 co‚Ä¶ :     36.2‚Ä¶ 34.6‚Ä¶ 32.7‚Ä¶ 30.5  28.6‚Ä¶ 28    27.5  27.8‚Ä¶\n 4 European Union - 27 co‚Ä¶ :     36    34.3‚Ä¶ 32.5  30.3‚Ä¶ 28.5  27.8‚Ä¶ 27.3‚Ä¶ 27.8‚Ä¶\n 5 Euro area - 19 countri‚Ä¶ :     34.7‚Ä¶ 33.2‚Ä¶ 31.8‚Ä¶ 29.6‚Ä¶ 28.3‚Ä¶ 27.6‚Ä¶ 26.1‚Ä¶ 25.6‚Ä¶\n 6 Euro area - 18 countri‚Ä¶ :     34.7‚Ä¶ 33.2‚Ä¶ 31.8‚Ä¶ 29.6‚Ä¶ 28.3‚Ä¶ 27.6‚Ä¶ 26.1‚Ä¶ 25.6‚Ä¶\n 7 Belgium                 31.3‚Ä¶ 28.1‚Ä¶ 27.5  27.1‚Ä¶ 25.3‚Ä¶ 23.6‚Ä¶ 24.6‚Ä¶ 22.8‚Ä¶ 21.6‚Ä¶\n 8 Bulgaria                45.1‚Ä¶ 37.7‚Ä¶ 37.2‚Ä¶ 35.7‚Ä¶ 32.8‚Ä¶ 30.8‚Ä¶ 32.2‚Ä¶ 33.1‚Ä¶ 28.3‚Ä¶\n 9 Czechia                 20    19.3‚Ä¶ 21.1‚Ä¶ 19.6‚Ä¶ 19.1‚Ä¶ 23.6‚Ä¶ 21.8‚Ä¶ 21.5  21.6‚Ä¶\n10 Denmark                 21.5  21.8‚Ä¶ 20.1‚Ä¶ 19.1‚Ä¶ 18.6‚Ä¶ 18.6‚Ä¶ 16.8‚Ä¶ 15.8‚Ä¶ 16.3‚Ä¶\n# ‚Ñπ 51 more rows\n# ‚Ñπ abbreviated name: ¬π‚Äã`Infant mortality rates [demo_minfind]`\n# ‚Ñπ 52 more variables: ...11 &lt;chr&gt;, ...12 &lt;chr&gt;, ...13 &lt;chr&gt;, ...14 &lt;chr&gt;,\n#   ...15 &lt;chr&gt;, ...16 &lt;chr&gt;, ...17 &lt;chr&gt;, ...18 &lt;chr&gt;, ...19 &lt;chr&gt;,\n#   ...20 &lt;chr&gt;, ...21 &lt;chr&gt;, ...22 &lt;chr&gt;, ...23 &lt;chr&gt;, ...24 &lt;chr&gt;,\n#   ...25 &lt;chr&gt;, ...26 &lt;chr&gt;, ...27 &lt;chr&gt;, ...28 &lt;chr&gt;, ...29 &lt;chr&gt;,\n#   ...30 &lt;chr&gt;, ...31 &lt;chr&gt;, ...32 &lt;chr&gt;, ...33 &lt;chr&gt;, ...34 &lt;chr&gt;, ‚Ä¶\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf the colnames are not tidy or they are threated as observation, then use the janitor package.\n\n\n\ninfant_mortality_df %&gt;% \n  slice(start_index:end_index) %&gt;% \n  janitor::row_to_names(1)\n\n# A tibble: 60 √ó 62\n   `GEO/TIME`     `1960` `1961` `1962` `1963` `1964` `1965` `1966` `1967` `1968`\n   &lt;chr&gt;          &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; \n 1 European Unio‚Ä¶ :      38.20‚Ä¶ 36.39‚Ä¶ 34.29‚Ä¶ 31.89‚Ä¶ 30     29.19‚Ä¶ 28.69‚Ä¶ 29.19‚Ä¶\n 2 European Unio‚Ä¶ :      36.20‚Ä¶ 34.60‚Ä¶ 32.70‚Ä¶ 30.5   28.69‚Ä¶ 28     27.5   27.89‚Ä¶\n 3 European Unio‚Ä¶ :      36     34.39‚Ä¶ 32.5   30.30‚Ä¶ 28.5   27.80‚Ä¶ 27.39‚Ä¶ 27.80‚Ä¶\n 4 Euro area - 1‚Ä¶ :      34.79‚Ä¶ 33.29‚Ä¶ 31.80‚Ä¶ 29.60‚Ä¶ 28.30‚Ä¶ 27.60‚Ä¶ 26.19‚Ä¶ 25.69‚Ä¶\n 5 Euro area - 1‚Ä¶ :      34.79‚Ä¶ 33.20‚Ä¶ 31.80‚Ä¶ 29.60‚Ä¶ 28.30‚Ä¶ 27.60‚Ä¶ 26.19‚Ä¶ 25.69‚Ä¶\n 6 Belgium        31.39‚Ä¶ 28.10‚Ä¶ 27.5   27.19‚Ä¶ 25.30‚Ä¶ 23.69‚Ä¶ 24.69‚Ä¶ 22.89‚Ä¶ 21.69‚Ä¶\n 7 Bulgaria       45.10‚Ä¶ 37.79‚Ä¶ 37.29‚Ä¶ 35.70‚Ä¶ 32.89‚Ä¶ 30.80‚Ä¶ 32.20‚Ä¶ 33.10‚Ä¶ 28.30‚Ä¶\n 8 Czechia        20     19.30‚Ä¶ 21.10‚Ä¶ 19.69‚Ä¶ 19.10‚Ä¶ 23.69‚Ä¶ 21.89‚Ä¶ 21.5   21.60‚Ä¶\n 9 Denmark        21.5   21.80‚Ä¶ 20.10‚Ä¶ 19.10‚Ä¶ 18.69‚Ä¶ 18.69‚Ä¶ 16.89‚Ä¶ 15.80‚Ä¶ 16.39‚Ä¶\n10 Germany (unti‚Ä¶ 33.79‚Ä¶ 31.69‚Ä¶ 29.30‚Ä¶ 27     25.30‚Ä¶ 23.89‚Ä¶ 23.60‚Ä¶ 22.89‚Ä¶ 22.80‚Ä¶\n# ‚Ñπ 50 more rows\n# ‚Ñπ 52 more variables: `1969` &lt;chr&gt;, `1970` &lt;chr&gt;, `1971` &lt;chr&gt;, `1972` &lt;chr&gt;,\n#   `1973` &lt;chr&gt;, `1974` &lt;chr&gt;, `1975` &lt;chr&gt;, `1976` &lt;chr&gt;, `1977` &lt;chr&gt;,\n#   `1978` &lt;chr&gt;, `1979` &lt;chr&gt;, `1980` &lt;chr&gt;, `1981` &lt;chr&gt;, `1982` &lt;chr&gt;,\n#   `1983` &lt;chr&gt;, `1984` &lt;chr&gt;, `1985` &lt;chr&gt;, `1986` &lt;chr&gt;, `1987` &lt;chr&gt;,\n#   `1988` &lt;chr&gt;, `1989` &lt;chr&gt;, `1990` &lt;chr&gt;, `1991` &lt;chr&gt;, `1992` &lt;chr&gt;,\n#   `1993` &lt;chr&gt;, `1994` &lt;chr&gt;, `1995` &lt;chr&gt;, `1996` &lt;chr&gt;, `1997` &lt;chr&gt;, ‚Ä¶\n\n\nNext issue: All columns are in character format\n\ninfant_mortality_df %&gt;% \n  slice(start_index:end_index) %&gt;% \n  janitor::row_to_names(1) %&gt;% \n* mutate_at(- 1, as.numeric) # all except the 1st col\n\n\n\n# A tibble: 60 √ó 62\n   `GEO/TIME`     `1960` `1961` `1962` `1963` `1964` `1965` `1966` `1967` `1968`\n   &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 European Unio‚Ä¶   NA     38.2   36.4   34.3   31.9   30     29.2   28.7   29.2\n 2 European Unio‚Ä¶   NA     36.2   34.6   32.7   30.5   28.7   28     27.5   27.9\n 3 European Unio‚Ä¶   NA     36     34.4   32.5   30.3   28.5   27.8   27.4   27.8\n 4 Euro area - 1‚Ä¶   NA     34.8   33.3   31.8   29.6   28.3   27.6   26.2   25.7\n 5 Euro area - 1‚Ä¶   NA     34.8   33.2   31.8   29.6   28.3   27.6   26.2   25.7\n 6 Belgium          31.4   28.1   27.5   27.2   25.3   23.7   24.7   22.9   21.7\n 7 Bulgaria         45.1   37.8   37.3   35.7   32.9   30.8   32.2   33.1   28.3\n 8 Czechia          20     19.3   21.1   19.7   19.1   23.7   21.9   21.5   21.6\n 9 Denmark          21.5   21.8   20.1   19.1   18.7   18.7   16.9   15.8   16.4\n10 Germany (unti‚Ä¶   33.8   31.7   29.3   27     25.3   23.9   23.6   22.9   22.8\n# ‚Ñπ 50 more rows\n# ‚Ñπ 52 more variables: `1969` &lt;dbl&gt;, `1970` &lt;dbl&gt;, `1971` &lt;dbl&gt;, `1972` &lt;dbl&gt;,\n#   `1973` &lt;dbl&gt;, `1974` &lt;dbl&gt;, `1975` &lt;dbl&gt;, `1976` &lt;dbl&gt;, `1977` &lt;dbl&gt;,\n#   `1978` &lt;dbl&gt;, `1979` &lt;dbl&gt;, `1980` &lt;dbl&gt;, `1981` &lt;dbl&gt;, `1982` &lt;dbl&gt;,\n#   `1983` &lt;dbl&gt;, `1984` &lt;dbl&gt;, `1985` &lt;dbl&gt;, `1986` &lt;dbl&gt;, `1987` &lt;dbl&gt;,\n#   `1988` &lt;dbl&gt;, `1989` &lt;dbl&gt;, `1990` &lt;dbl&gt;, `1991` &lt;dbl&gt;, `1992` &lt;dbl&gt;,\n#   `1993` &lt;dbl&gt;, `1994` &lt;dbl&gt;, `1995` &lt;dbl&gt;, `1996` &lt;dbl&gt;, `1997` &lt;dbl&gt;, ‚Ä¶\n\n\nAnd now lets transform it into long format.\n\ninfant_mortality_df %&gt;% \n  slice(start_index:end_index) %&gt;% \n  janitor::row_to_names(1) %&gt;% \n  mutate_at(- 1, as.numeric) %&gt;% \n* pivot_longer(- 1, names_to = \"TIME\", \n                 values_to = \"infant_mortality\")\n\n\n\n# A tibble: 3,660 √ó 3\n   `GEO/TIME`                                TIME  infant_mortality\n   &lt;chr&gt;                                     &lt;chr&gt;            &lt;dbl&gt;\n 1 European Union - 27 countries (from 2020) 1960              NA  \n 2 European Union - 27 countries (from 2020) 1961              38.2\n 3 European Union - 27 countries (from 2020) 1962              36.4\n 4 European Union - 27 countries (from 2020) 1963              34.3\n 5 European Union - 27 countries (from 2020) 1964              31.9\n 6 European Union - 27 countries (from 2020) 1965              30  \n 7 European Union - 27 countries (from 2020) 1966              29.2\n 8 European Union - 27 countries (from 2020) 1967              28.7\n 9 European Union - 27 countries (from 2020) 1968              29.2\n10 European Union - 27 countries (from 2020) 1969              28  \n# ‚Ñπ 3,650 more rows\n\n\nLets filter out aggregates and irrelevant.\n\ninfant_mortality_df %&gt;% \n  slice(start_index:end_index) %&gt;% \n  janitor::row_to_names(1) %&gt;% \n  mutate_at(- 1, as.numeric) %&gt;% \n  pivot_longer(- 1, names_to = \"TIME\", \n               values_to = \"infant_mortality\") %&gt;% \n  rename(geo = `GEO/TIME`) %&gt;% \n  mutate(\n    geo = countrycode(geo, \"country.name\", \"iso3c\"),\n  ) %&gt;% \n  filter(!is.na(geo))\n\nThis code filters out the rows where geo is in:\nEnough?\ncount() Quickly count the unique values of one or more variables\n\ninfant_mortality_df %&gt;% \n  slice(start_index:end_index) %&gt;% \n  janitor::row_to_names(1) %&gt;% \n  mutate_at(- 1, as.numeric) %&gt;% \n  pivot_longer(- 1, names_to = \"TIME\", \n               values_to = \"infant_mortality\") %&gt;% \n  rename(geo = `GEO/TIME`) %&gt;% \n  mutate(\n    geo = countrycode(geo, \"country.name\", \"iso3c\"),\n  ) %&gt;% \n  filter(!is.na(geo)) %&gt;% \n  count(geo, sort = TRUE)\n\n\n\n# A tibble: 47 √ó 2\n   geo       n\n   &lt;chr&gt; &lt;int&gt;\n 1 DEU     122\n 2 FRA     122\n 3 ALB      61\n 4 AND      61\n 5 ARM      61\n 6 AUT      61\n 7 AZE      61\n 8 BEL      61\n 9 BGR      61\n10 BIH      61\n# ‚Ñπ 37 more rows\n\n\nRemove the irrelevant duplication & convert TIME to numeric\n\ninfant_mortality_df %&gt;% \n  slice(start_index:end_index) %&gt;% \n  janitor::row_to_names(1) %&gt;% \n  mutate_at(- 1, as.numeric) %&gt;% \n  pivot_longer(- 1, names_to = \"TIME\", \n               values_to = \"infant_mortality\") %&gt;% \n  rename(geo = `GEO/TIME`) %&gt;% \n  filter(\n    geo != \"France (metropolitan)\" &\n      geo != \"Germany (until 1990 former territory of the FRG)\"\n  ) %&gt;% \nmutate(\n  geo = countrycode(geo, \"country.name\", \"iso3c\"),\n  TIME = as.numeric(TIME)\n) %&gt;% \n  filter(!is.na(geo)) \n\n\n\n# A tibble: 2,867 √ó 3\n   geo    TIME infant_mortality\n   &lt;chr&gt; &lt;dbl&gt;            &lt;dbl&gt;\n 1 BEL    1960             31.4\n 2 BEL    1961             28.1\n 3 BEL    1962             27.5\n 4 BEL    1963             27.2\n 5 BEL    1964             25.3\n 6 BEL    1965             23.7\n 7 BEL    1966             24.7\n 8 BEL    1967             22.9\n 9 BEL    1968             21.7\n10 BEL    1969             21.2\n# ‚Ñπ 2,857 more rows\n\n\nAssign it with the same name (re-write the table)\n\ninfant_mortality_df &lt;- infant_mortality_df %&gt;% \n  slice(start_index:end_index) %&gt;% \n  janitor::row_to_names(1) %&gt;% \n  mutate_at(- 1, as.numeric) %&gt;% \n  pivot_longer(- 1, names_to = \"TIME\", \n               values_to = \"infant_mortality\") %&gt;% \n  rename(geo = `GEO/TIME`) %&gt;% \n  filter(\n    geo != \"France (metropolitan)\" &\n      geo != \"Germany (until 1990 former territory of the FRG)\"\n  ) %&gt;% \nmutate(\n  geo = countrycode(geo, \"country.name\", \"iso3c\"),\n  TIME = as.numeric(TIME)\n) %&gt;% \n  filter(!is.na(geo)) \n\nLet‚Äôs transform fertility_df to a similar format\n\nfertility_df &lt;- read_csv(\"https://stats.oecd.org/sdmx-json/data/DP_LIVE/.FERTILITY.../OECD?contentType=csv&detail=code&separator=comma&csv-lang=en\")\n\n\nfertility_df &lt;- fertility_df %&gt;% \n  select(\n    geo = LOCATION, TIME, fertility = Value\n  )\n\nNow we need to link the two tables.\n\ndf &lt;- full_join(x = fertility_df, y = infant_mortality_df)\n\ndf\n\n# A tibble: 4,205 √ó 4\n   geo    TIME fertility infant_mortality\n   &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt;            &lt;dbl&gt;\n 1 AUS    1960      3.45               NA\n 2 AUS    1961      3.55               NA\n 3 AUS    1962      3.43               NA\n 4 AUS    1963      3.34               NA\n 5 AUS    1964      3.15               NA\n 6 AUS    1965      2.97               NA\n 7 AUS    1966      2.89               NA\n 8 AUS    1967      2.85               NA\n 9 AUS    1968      2.89               NA\n10 AUS    1969      2.89               NA\n# ‚Ñπ 4,195 more rows\n\n\nAnd now‚Ä¶ Let‚Äôs see what we have here. Last time we saw that we can generate summary statistics with skimr function.\n\nskimr::skim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n4205\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\ngeo\n0\n1\n2\n4\n0\n68\n0\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nTIME\n0\n1.00\n1990.42\n17.85\n1960.00\n1975.00\n1990.00\n2006.00\n2022.00\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\nfertility\n911\n0.78\n2.34\n1.20\n0.81\n1.59\n1.94\n2.59\n7.67\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\n\n\ninfant_mortality\n1890\n0.55\n14.33\n15.03\n0.00\n4.90\n9.70\n17.90\n137.60\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\nWe can also generate some pots and statistics easily with the powerful radiant package.\n\nradiant::radiant() # start a shiny app on your machine\n\nBut life is never so easy. Let‚Äôs suppose we want to visualise only a few countries to tell a story.\n\nbeveridge_plot &lt;- function(data, x, y, group, time, n_label = 5) {\n  data |&gt; \n    arrange({{ time }}) |&gt; \n    ggplot() +\n    aes({{ x }}, {{ y }}, color = {{ group }}) +\n    geom_path() +\n    geom_point(size = 2, shape = 16) +\n    ggrepel::geom_text_repel(data = ~ group_by(., {{ group }}) |&gt; \n                               slice(unique(floor(seq(from = 1, to = n(), \n                                                      length.out = n_label)))),\n                             aes(label = {{ time }}),\n                             show.legend = FALSE\n                               )\n}\n\n\ndf |&gt; \n  filter(geo %in% c(\"SVK\", \"HUN\", \"AUT\")) |&gt; \n  beveridge_plot(fertility, infant_mortality, geo, TIME)"
  },
  {
    "objectID": "content/05-content.html",
    "href": "content/05-content.html",
    "title": "Regex",
    "section": "",
    "text": "Motivation\n\n‚ÄúStrings are not glamorous, high-profile components of R, but they do play a big role in many data cleaning and preparation tasks. The stringr package provides a cohesive set of functions designed to make working with strings as easy as possible.‚Äù\n\nSource: Package description\n\n\n\n\n\n\nCHEATSHEETS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAll functions within stringr are prefixed with str_ and require a vector of strings as the primary argument. This design choice facilitates the effortless identification of the desired string manipulation function (just type ‚Äústr_‚Äù and use the TAB to browse).\n\n\n\ncourses &lt;- c(\"Big data\", \"Behavioral economics\", \"Dynamic macroeconomics 2\", \"Communication\", \"Economic instituions\")\n\ncourses\n\n[1] \"Big data\"                 \"Behavioral economics\"    \n[3] \"Dynamic macroeconomics 2\" \"Communication\"           \n[5] \"Economic instituions\"    \n\n\nBasics\nstringr is also part of the tidyverse, so you do not have to load it individually.\n\nlibrary(tidyverse)\n\nCombine strings:\n\nstr_c(courses, \" 2\")\n\n[1] \"Big data 2\"                 \"Behavioral economics 2\"    \n[3] \"Dynamic macroeconomics 2 2\" \"Communication 2\"           \n[5] \"Economic instituions 2\"    \n\n\n\nWhich subject is about economics?\n\nstr_detect(courses, \"economics\")\n\n[1] FALSE  TRUE  TRUE FALSE FALSE\n\n\nOf course, these functions can also be used in the structure seen earlier (in a tidy format).\n\ntibble(courses)\n\n# A tibble: 5 √ó 1\n  courses                 \n  &lt;chr&gt;                   \n1 Big data                \n2 Behavioral economics    \n3 Dynamic macroeconomics 2\n4 Communication           \n5 Economic instituions    \n\n\n\ntibble(courses) %&gt;% \n  mutate(\n    about_economics = str_detect(courses, \"economic\")\n  )\n\n# A tibble: 5 √ó 2\n  courses                  about_economics\n  &lt;chr&gt;                    &lt;lgl&gt;          \n1 Big data                 FALSE          \n2 Behavioral economics     TRUE           \n3 Dynamic macroeconomics 2 TRUE           \n4 Communication            FALSE          \n5 Economic instituions     FALSE          \n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you look carefully at the outcome, you can see that these functions are cAsE sENsItIVE (the FALSE value in the last row).\n\n\nSolution 1. - convert everything to lower case\n\ntibble(courses) %&gt;% \n  mutate(\n    courses = str_to_lower(courses),\n    about_economics = str_detect(courses, \"economic\")\n  )\n\n# A tibble: 5 √ó 2\n  courses                  about_economics\n  &lt;chr&gt;                    &lt;lgl&gt;          \n1 big data                 FALSE          \n2 behavioral economics     TRUE           \n3 dynamic macroeconomics 2 TRUE           \n4 communication            FALSE          \n5 economic instituions     TRUE           \n\n\nSolution 2. - detect with lower and upper case\n\ntibble(courses) %&gt;% \n  mutate(\n    about_economics = str_detect(courses, \"economic|Economic\")\n  )\n\n# A tibble: 5 √ó 2\n  courses                  about_economics\n  &lt;chr&gt;                    &lt;lgl&gt;          \n1 Big data                 FALSE          \n2 Behavioral economics     TRUE           \n3 Dynamic macroeconomics 2 TRUE           \n4 Communication            FALSE          \n5 Economic instituions     TRUE           \n\n\n\nSolution 3. - detect with lower and upper first letter\n\ntibble(courses) %&gt;% \n  mutate(\n    about_economics = str_detect(courses, \"[eE]conomic\")\n  )\n\n# A tibble: 5 √ó 2\n  courses                  about_economics\n  &lt;chr&gt;                    &lt;lgl&gt;          \n1 Big data                 FALSE          \n2 Behavioral economics     TRUE           \n3 Dynamic macroeconomics 2 TRUE           \n4 Communication            FALSE          \n5 Economic instituions     TRUE           \n\n\nRegex\n\nMost string functions work with regular expressions, a concise language for describing patterns of text.\n\n[eE]conomic was an example to regular expressions (regex): ‚Äúe‚Äù or ‚ÄúE‚Äù\nRegex has a great number of special characters that we can use to describe the patterns we are looking for\n\nFor example: \\\\d is for any numeric character\n\n\ntibble(courses) %&gt;% \n  mutate(\n    about_economics = str_detect(courses, \"economic\"),\n    not_single_course = str_detect(courses, \"\\\\d\")\n  )\n\n# A tibble: 5 √ó 3\n  courses                  about_economics not_single_course\n  &lt;chr&gt;                    &lt;lgl&gt;           &lt;lgl&gt;            \n1 Big data                 FALSE           FALSE            \n2 Behavioral economics     TRUE            FALSE            \n3 Dynamic macroeconomics 2 TRUE            TRUE             \n4 Communication            FALSE           FALSE            \n5 Economic instituions     FALSE           FALSE            \n\n\n\\\\s is for whitespaces (space/new line/tabulator)\n\ntibble(courses) %&gt;% \n  mutate(\n    contain_spaces = str_detect(courses, \"\\\\s\")\n  )\n\n# A tibble: 5 √ó 2\n  courses                  contain_spaces\n  &lt;chr&gt;                    &lt;lgl&gt;         \n1 Big data                 TRUE          \n2 Behavioral economics     TRUE          \n3 Dynamic macroeconomics 2 TRUE          \n4 Communication            FALSE         \n5 Economic instituions     TRUE          \n\n\n\\\\w is for letters (but all of them contain letters)\n\ntibble(courses) %&gt;% \n  mutate(\n    contain_letter = str_detect(courses, \"\\\\w\")\n  )\n\n# A tibble: 5 √ó 2\n  courses                  contain_letter\n  &lt;chr&gt;                    &lt;lgl&gt;         \n1 Big data                 TRUE          \n2 Behavioral economics     TRUE          \n3 Dynamic macroeconomics 2 TRUE          \n4 Communication            TRUE          \n5 Economic instituions     TRUE          \n\n\n\n\n\n\n\n\nTip\n\n\n\nEach of the regex expressions presented previously has an opposite. The same code in upper case. For instance, \\\\W is for non-letter characters (numbers or white-spaces)\n\ntibble(courses) %&gt;% \n  mutate(\n    contain_non_letter = str_detect(courses, \"\\\\W\")\n  )\n\n# A tibble: 5 √ó 2\n  courses                  contain_non_letter\n  &lt;chr&gt;                    &lt;lgl&gt;             \n1 Big data                 TRUE              \n2 Behavioral economics     TRUE              \n3 Dynamic macroeconomics 2 TRUE              \n4 Communication            FALSE             \n5 Economic instituions     TRUE              \n\n\n\n\nThere are several other functions in the {stringr} package. We will cover a few in the following examples.\n\ntibble(courses) %&gt;% \n  mutate(\n    n_non_letter = str_count(courses, \"\\\\W\"),\n    n_character = str_length(courses)\n  )\n\n# A tibble: 5 √ó 3\n  courses                  n_non_letter n_character\n  &lt;chr&gt;                           &lt;int&gt;       &lt;int&gt;\n1 Big data                            1           8\n2 Behavioral economics                1          20\n3 Dynamic macroeconomics 2            2          24\n4 Communication                       0          13\n5 Economic instituions                1          20\n\n\nExtract date from url\nhttps://economaniablog.hu/2022/09/14/how-to-forecast-the-business-cycle-sentiment-speaks/\n\nx &lt;- \"https://economaniablog.hu/2022/09/14/how-to-forecast-the-business-cycle-sentiment-speaks/\"\n\n\nstr_extract(x, \"20\\\\d\\\\d/\\\\d\\\\d/\\\\d\\\\d\")\n\n[1] \"2022/09/14\"\n\n\nAn alternative solution:\n\nstr_extract(x, \"[\\\\d/-]{3,}\") %&gt;% # digit, / or - and more than 3\n  str_remove(\"[/-]$\") %&gt;% # if it is at the end\n  str_remove(\"^[/-]\") # if it is at the beginning\n\n[1] \"2022/09/14\"\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThose who want to work with webscraping and/or text analysis tools will really need to learn how to use the {stringr} functions!\n\n\nIs it a website?\n\nstr_starts(x, \"https://\")\n\n[1] TRUE\n\n\nRemove the base url, assuming that its length is always the same\n\nstr_sub(x, end = 26)\n\n[1] \"https://economaniablog.hu/\"\n\n\n\nhttps://economaniablog.hu/2022/09/14/how-to-forecast-the-business-cycle-sentiment-speaks/\nRemove the base url, assuming it lasts until the date\n\nstr_replace(x, \".*20\\\\d\\\\d/\\\\d\\\\d/\\\\d\\\\d/\", \"\")\n\n[1] \"how-to-forecast-the-business-cycle-sentiment-speaks/\"\n\n\nHere the . refers to anything, and * denotes any repetition. Thus .* before the pattern means anything before the pattern, and .* after the pattern means anything after the pattern."
  },
  {
    "objectID": "content/06-content.html",
    "href": "content/06-content.html",
    "title": "Functional programming",
    "section": "",
    "text": "‚Äú{purrr} enhances R‚Äôs functional programming (FP) toolkit by providing a complete and consistent set of tools for working with functions and vectors. If you‚Äôve never heard of FP before, the best place to start is the family of map() functions which allow you to replace many for loops with code that is both more succinct and easier to read. The best place to learn about the map() functions is the iteration chapter in R for data science.‚Äù\nSource: Package description\nThe purpose of functional programming, as it is written in description of the package, is to implement iterations (recursions) in a readable manner in our code. It is going to be just as a huge advantage of R programming as the dplyr package for tabular data.\nThe approach is very similar to what we have seen with the apply family, where there is an input object and we apply the specified function to each of its elements. This was the lapply function we encountered earlier, as we discussed previously."
  },
  {
    "objectID": "content/06-content.html#nested-tibbles",
    "href": "content/06-content.html#nested-tibbles",
    "title": "Functional programming",
    "section": "Nested tibbles",
    "text": "Nested tibbles\nThe above seen functionality, that we can store a list as a column of a tibble is great, but what if we need the whole tables as one df. Well, we can simple unnest the columns.\n\ntibble(file_names) %&gt;% \n  mutate(\n    data = map(file_names, read_csv),\n    file_names = str_remove(file_names, \".*/\"), # remove the path\n    file_names = str_remove(file_names, \".csv\"),\n    avg_cite = map_dbl(data, ~ mean(.$Cites, na.rm = TRUE)) #&lt;\n  ) %&gt;% \n  unnest(data) \n\n# A tibble: 1,978 √ó 28\n   file_names     Cites Authors Title  Year Source Publisher ArticleURL CitesURL\n   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;   \n 1 daily-inflati‚Ä¶  3295 GP Com‚Ä¶ The ‚Ä¶  2011 Quart‚Ä¶ Wiley On‚Ä¶ https://r‚Ä¶ https:/‚Ä¶\n 2 daily-inflati‚Ä¶   885 E Naka‚Ä¶ High‚Ä¶  2018 The Q‚Ä¶ academic‚Ä¶ https://a‚Ä¶ https:/‚Ä¶\n 3 daily-inflati‚Ä¶  2708 E Cast‚Ä¶ Synt‚Ä¶  2008 Synth‚Ä¶ degruyte‚Ä¶ https://w‚Ä¶ https:/‚Ä¶\n 4 daily-inflati‚Ä¶   660 RC Cor‚Ä¶ An e‚Ä¶  2018 Journ‚Ä¶ Wiley On‚Ä¶ https://a‚Ä¶ https:/‚Ä¶\n 5 daily-inflati‚Ä¶   164 MC Med‚Ä¶ Fore‚Ä¶  2021 Journ‚Ä¶ Taylor &‚Ä¶ https://w‚Ä¶ https:/‚Ä¶\n 6 daily-inflati‚Ä¶  5219 GW Sch‚Ä¶ Why ‚Ä¶  1989 The j‚Ä¶ Wiley On‚Ä¶ https://o‚Ä¶ https:/‚Ä¶\n 7 daily-inflati‚Ä¶  1701 PR Han‚Ä¶ The ‚Ä¶  2011 Econo‚Ä¶ Wiley On‚Ä¶ https://o‚Ä¶ https:/‚Ä¶\n 8 daily-inflati‚Ä¶  2573 LJ Chr‚Ä¶ The ‚Ä¶  2003 inter‚Ä¶ Wiley On‚Ä¶ https://o‚Ä¶ https:/‚Ä¶\n 9 daily-inflati‚Ä¶  4813 F Black Noise  1986 The j‚Ä¶ Wiley On‚Ä¶ https://o‚Ä¶ https:/‚Ä¶\n10 daily-inflati‚Ä¶   220 C Bind‚Ä¶ Coro‚Ä¶  2020 Revie‚Ä¶ direct.m‚Ä¶ https://d‚Ä¶ https:/‚Ä¶\n# ‚Ñπ 1,968 more rows\n# ‚Ñπ 19 more variables: GSRank &lt;dbl&gt;, QueryDate &lt;dttm&gt;, Type &lt;chr&gt;, DOI &lt;chr&gt;,\n#   ISSN &lt;lgl&gt;, CitationURL &lt;lgl&gt;, Volume &lt;lgl&gt;, Issue &lt;lgl&gt;, StartPage &lt;lgl&gt;,\n#   EndPage &lt;lgl&gt;, ECC &lt;dbl&gt;, CitesPerYear &lt;dbl&gt;, CitesPerAuthor &lt;dbl&gt;,\n#   AuthorCount &lt;dbl&gt;, Age &lt;dbl&gt;, Abstract &lt;chr&gt;, FullTextURL &lt;chr&gt;,\n#   RelatedURL &lt;chr&gt;, avg_cite &lt;dbl&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou may realize that the original unnested columns are copied to each corresponding observation.\n\n\nWe can simply use the nest function if we want to achieve the opposite.\n\nmap_dfr(file_names, read_csv, .id = \"keyword\") %&gt;% \n  mutate(\n    keyword = file_names[as.numeric(keyword)],\n    keyword = str_remove(keyword, \".*/\"), # remove the path\n    keyword = str_remove(keyword, \".csv\") # remove extension\n  ) %&gt;%\n  nest(\n    data = - keyword, # everything except \"keyword\" to the \"data\" column\n    .by = keyword\n    )\n\n# A tibble: 2 √ó 2\n  keyword                         data               \n  &lt;chr&gt;                           &lt;list&gt;             \n1 daily-inflation-online          &lt;tibble [998 √ó 26]&gt;\n2 inflation-expectations-forecast &lt;tibble [980 √ó 26]&gt;\n\n\n\nmap_dfr(file_names, read_csv, .id = \"keyword\") %&gt;% \n  mutate(\n    keyword = file_names[as.numeric(keyword)],\n    keyword = str_remove(keyword, \".*/\"), # remove the path\n    keyword = str_remove(keyword, \".csv\") # remove extension\n  ) %&gt;%\n  group_by(keyword) %&gt;%\n  nest()\n\n# A tibble: 2 √ó 2\n# Groups:   keyword [2]\n  keyword                         data               \n  &lt;chr&gt;                           &lt;list&gt;             \n1 daily-inflation-online          &lt;tibble [998 √ó 26]&gt;\n2 inflation-expectations-forecast &lt;tibble [980 √ó 26]&gt;"
  },
  {
    "objectID": "content/06-content.html#exercise-1",
    "href": "content/06-content.html#exercise-1",
    "title": "Functional programming",
    "section": "Exercise 1",
    "text": "Exercise 1\nLets open the url of the 5 most cited articles by the 2 topics, which is newer than 10 years, and the abstarct is about the US.\n\n\n\n\n\n\nTip\n\n\n\nThe walk function works similarly like map, but it does not return any value, it is useful if you want to generate side-effects (like opening something in your browser, with the browseURL).\n\n\nSolution:\n\nmap_dfr(file_names, read_csv, .id = \"keyword\") %&gt;% \n  mutate(\n    keyword = file_names[as.numeric(keyword)],\n    keyword = str_remove(keyword, \".*/\"), # remove the path\n    keyword = str_remove(keyword, \".csv\") # remove extension\n  ) %&gt;%\n  filter(Year &gt;= lubridate::year(Sys.Date()) - 10) %&gt;% \n  filter(str_detect(Abstract, \" US | USA\")) %&gt;% \n  group_by() %&gt;% \n  slice_max(Cites, n = 10, by = keyword) %&gt;% \n  arrange(Cites) %&gt;% \n  pull(ArticleURL) %&gt;% \n  walk(browseURL)"
  },
  {
    "objectID": "content/06-content.html#exercise-2",
    "href": "content/06-content.html#exercise-2",
    "title": "Functional programming",
    "section": "Exercise 2",
    "text": "Exercise 2\nLet us create a simulation to determine the optimal investment ratio (\\(f\\)) given a probability, (\\(p\\)), of doubling our invested money and a probability of \\(1-p\\) of losing it. We will play this game for a total of 200 rounds. What should be the value of \\(f\\), given a specific value of \\(p\\), in order to achieve maximum return?\n\nlibrary(tidyverse)\n\ncoins &lt;- function(n = 1000, p = .55) {\n  ifelse(runif(n - 1) &gt; (1 - p), 2, 0)\n}\n\nret &lt;- function(p = .5, f = .5, n = 200, m = 10000) {\n  accumulate(\n    .x = coins(n, p),\n    .f = ~ {\n      keep = floor(.x * (1 - f))\n      play = .x - keep\n      earned = floor(keep + play * .y)\n      # message(\"K:\", round(keep), \"p\", round(play), \"x\", round(.y, 4), \"e\", round(earned))\n      earned\n    },\n    .init = m\n  )\n}\n\n\nparams_df &lt;- crossing(\n  p = seq(from = .5, to = .6, length.out = 3),\n  f = seq(from = 0, to = .7, length.out = 10)\n)\n\nparams_df\n\n# A tibble: 30 √ó 2\n       p      f\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1   0.5 0     \n 2   0.5 0.0778\n 3   0.5 0.156 \n 4   0.5 0.233 \n 5   0.5 0.311 \n 6   0.5 0.389 \n 7   0.5 0.467 \n 8   0.5 0.544 \n 9   0.5 0.622 \n10   0.5 0.7   \n# ‚Ñπ 20 more rows\n\n\n\nresults_df &lt;- params_df %&gt;% \n  mutate(\n    r = map2(p, f, ~ {\n      replicate(10000, last(ret(p = .x, f = .y)), simplify = TRUE)\n    }, .progress = TRUE), # .progress: show progress line\n    avg_r = map_dbl(r, mean)\n  )\n\n ‚ñ†‚ñ†                                 3% |  ETA:  2m\n\n\n ‚ñ†‚ñ†‚ñ†                                7% |  ETA:  2m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†                              10% |  ETA:  2m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                             13% |  ETA:  2m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                            17% |  ETA:  2m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                           20% |  ETA:  2m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                          23% |  ETA:  2m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                         27% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                        30% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                       33% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                      37% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                     40% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                    43% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                   47% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                  50% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                 53% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                57% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†               60% |  ETA: 49s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†              63% |  ETA: 44s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†             67% |  ETA: 40s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†            70% |  ETA: 37s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†           73% |  ETA: 33s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†          77% |  ETA: 29s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†         80% |  ETA: 25s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†        83% |  ETA: 20s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†       87% |  ETA: 16s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†      90% |  ETA: 12s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†     93% |  ETA:  8s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†    97% |  ETA:  4s\n\nresults_df\n\n# A tibble: 30 √ó 4\n       p      f r                  avg_r\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;list&gt;             &lt;dbl&gt;\n 1   0.5 0      &lt;dbl [10,000]&gt; 10000    \n 2   0.5 0.0778 &lt;dbl [10,000]&gt; 10483.   \n 3   0.5 0.156  &lt;dbl [10,000]&gt;  9619.   \n 4   0.5 0.233  &lt;dbl [10,000]&gt; 15987.   \n 5   0.5 0.311  &lt;dbl [10,000]&gt;  2677.   \n 6   0.5 0.389  &lt;dbl [10,000]&gt;   517.   \n 7   0.5 0.467  &lt;dbl [10,000]&gt;  1707.   \n 8   0.5 0.544  &lt;dbl [10,000]&gt;     0.319\n 9   0.5 0.622  &lt;dbl [10,000]&gt;     0    \n10   0.5 0.7    &lt;dbl [10,000]&gt;     0    \n# ‚Ñπ 20 more rows\n\n\n\nlibrary(gt)\n\nresults_df %&gt;%\n  select(-r) %&gt;%\n  mutate(p = format(p, digits = 2)) %&gt;%\n  pivot_wider(names_from = p, values_from = avg_r) %&gt;%\n  gt() %&gt;%\n  fmt_number(-f, decimals = 0) %&gt;%\n  fmt_number(f, decimals = 2) %&gt;%\n  tab_spanner(\"p\", -1) %&gt;% \n  data_color(2) %&gt;%\n  data_color(3) %&gt;%\n  data_color(4)\n\n\n\n\n\n\n\nf\n      \n        p\n      \n    \n\n0.50\n      0.55\n      0.60\n    \n\n\n\n0.00\n10,000\n10,000\n10,000\n\n\n0.08\n10,067\n46,030\n212,650\n\n\n0.16\n10,287\n210,835\n4,184,787\n\n\n0.23\n6,046\n919,002\n87,429,495\n\n\n0.31\n2,259\n1,936,808\n1,425,716,604\n\n\n0.39\n610\n1,594,350\n5,149,530,456\n\n\n0.47\n51\n354,241\n19,601,120,279\n\n\n0.54\n0\n55,296\n34,492,225,580\n\n\n0.62\n0\n370\n803,305,667\n\n\n0.70\n0\n0\n309,720"
  },
  {
    "objectID": "content/07-content.html",
    "href": "content/07-content.html",
    "title": "A whole game",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lubridate)\n\n\n\n\n\n\ndate_to_colname &lt;- function(.data) {\n  # * wide panel format &gt; header are the dates started at 2nd col from 2017\n  dates &lt;- seq.Date(\n    from = as.Date(\"2007-01-01\"),\n    by = \"days\", \n    to = Sys.Date()\n  ) |&gt; \n    keep(~ lubridate::wday(., week_start = 1) %in% 1:5) |&gt; \n    head(ncol(.data) - 1) |&gt; \n    as.character()\n  \n  .data |&gt; \n    set_names(c(\"ticker\", dates))\n}\n\n\nbloomberg_raw &lt;- list.files(\"../data/bloomberg\", full.names = TRUE) |&gt; \n  keep(str_detect, \"/bloomberg_scores\\\\d{1,2}.xlsx\") |&gt; \n  map(.progress = \"reading raw data\", \\(x) {\n    list(\n      news_heat = readxl::read_xlsx(x, sheet = 1, progress = FALSE) |&gt; \n        date_to_colname(),\n      sentiment_avg = readxl::read_xlsx(x, sheet = 2, progress = FALSE) |&gt; \n        date_to_colname()\n    )\n  })\n\nreading raw data ‚ñ†‚ñ†‚ñ†‚ñ†                              10% |  ETA:  1m\n\n\nNew names:\nNew names:\nreading raw data ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 20% | ETA: 1m\nreading raw data ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 30% | ETA: 1m\nreading raw data ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 40% | ETA: 1m\nreading raw data ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 50% | ETA: 46s\nreading raw data ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 60% | ETA: 37s\nreading raw data ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 70% | ETA: 27s\nreading raw data ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 80% | ETA: 18s\nreading raw data ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 90% | ETA: 9s\n‚Ä¢ `` -&gt; `...4274`\n‚Ä¢ `` -&gt; `...4275`\n‚Ä¢ `` -&gt; `...4276`\n‚Ä¢ `` -&gt; `...4277`\n‚Ä¢ `` -&gt; `...4278`\n‚Ä¢ `` -&gt; `...4279`\n‚Ä¢ `` -&gt; `...4280`\n‚Ä¢ `` -&gt; `...4281`\n‚Ä¢ `` -&gt; `...4282`\n‚Ä¢ `` -&gt; `...4283`\n‚Ä¢ `` -&gt; `...4284`\n‚Ä¢ `` -&gt; `...4285`\n‚Ä¢ `` -&gt; `...4286`\n‚Ä¢ `` -&gt; `...4287`\n‚Ä¢ `` -&gt; `...4288`\n‚Ä¢ `` -&gt; `...4289`\n‚Ä¢ `` -&gt; `...4290`\n‚Ä¢ `` -&gt; `...4291`\n‚Ä¢ `` -&gt; `...4292`\n\n\n\nnews_heat_df &lt;- bloomberg_raw |&gt; \n  map_dfr(\\(x) {\n    x$news_heat |&gt; \n      pivot_longer(-1, \n                   names_to = \"time\",\n                   names_transform = ymd, \n                   values_to = \"news_heat\") |&gt; \n      mutate(\n        ticker = str_remove(ticker, \" .*\"),\n        news_heat = factor(news_heat, levels = 0:4, ordered = TRUE)\n      )\n  }) |&gt; \n  drop_na()\n\nnews_heat_df\n\n# A tibble: 22,129,018 √ó 3\n   ticker time       news_heat\n   &lt;chr&gt;  &lt;date&gt;     &lt;ord&gt;    \n 1 AAPL   2010-02-16 0        \n 2 AAPL   2010-02-17 2        \n 3 AAPL   2010-02-18 1        \n 4 AAPL   2010-02-19 0        \n 5 AAPL   2010-02-22 2        \n 6 AAPL   2010-02-23 3        \n 7 AAPL   2010-02-24 2        \n 8 AAPL   2010-02-25 4        \n 9 AAPL   2010-02-26 3        \n10 AAPL   2010-03-01 4        \n# ‚Ñπ 22,129,008 more rows\n\n\n\nsentiment_avg_df &lt;- bloomberg_raw |&gt; \n  map_dfr(\\(x) {\n    x$sentiment_avg |&gt; \n      pivot_longer(-1, \n                   names_to = \"time\",\n                   names_transform = ymd, \n                   values_to = \"sentiment_avg\") |&gt; \n      mutate(\n        ticker = str_remove(ticker, \" .*\"),\n        sentiment_avg = as.numeric(sentiment_avg)\n      )\n  }) |&gt; \n  drop_na()\n\nsentiment_avg_df\n\n# A tibble: 31,270,528 √ó 3\n   ticker time       sentiment_avg\n   &lt;chr&gt;  &lt;date&gt;             &lt;dbl&gt;\n 1 AMZN   2007-01-04       -0.500 \n 2 AMZN   2007-01-05       -0.500 \n 3 AMZN   2007-01-08       -0.500 \n 4 AMZN   2007-01-09       -0.500 \n 5 AMZN   2007-01-10        0.0555\n 6 AMZN   2007-01-11        0.0555\n 7 AMZN   2007-01-12        0.0555\n 8 AMZN   2007-01-15        0.0555\n 9 AMZN   2007-01-16        0.0555\n10 AMZN   2007-01-17        0.0555\n# ‚Ñπ 31,270,518 more rows\n\n\n\nbloomberg_df &lt;- list(\n  news_heat_df, \n  sentiment_avg_df\n) |&gt; \n  reduce(full_join, by = join_by(ticker, time)) |&gt; \n  arrange(ticker, time)\n\nbloomberg_df\n\n# A tibble: 34,499,579 √ó 4\n   ticker time       news_heat sentiment_avg\n   &lt;chr&gt;  &lt;date&gt;     &lt;ord&gt;             &lt;dbl&gt;\n 1 A      2007-01-05 &lt;NA&gt;             0.0506\n 2 A      2007-01-08 &lt;NA&gt;             0.0506\n 3 A      2007-01-09 &lt;NA&gt;             0.0506\n 4 A      2007-01-10 &lt;NA&gt;             0.0506\n 5 A      2007-01-11 &lt;NA&gt;             0.0506\n 6 A      2007-01-12 &lt;NA&gt;             0     \n 7 A      2007-01-15 &lt;NA&gt;             0     \n 8 A      2007-01-16 &lt;NA&gt;             0     \n 9 A      2007-01-17 &lt;NA&gt;             0.558 \n10 A      2007-01-18 &lt;NA&gt;             0.319 \n# ‚Ñπ 34,499,569 more rows"
  },
  {
    "objectID": "content/08-content.html",
    "href": "content/08-content.html",
    "title": "Regression, Classification",
    "section": "",
    "text": "This week, the material we will discuss is mainly theoretical (likely a review for many of you), so we will progress based on slide presentations."
  },
  {
    "objectID": "content/08-content.html#bivariate-and-multivariate-ols",
    "href": "content/08-content.html#bivariate-and-multivariate-ols",
    "title": "Regression, Classification",
    "section": "Bivariate and multivariate OLS",
    "text": "Bivariate and multivariate OLS\nThe bivariate or multivariate ordinary least squares (OLS) is undoubtedly the most widely used estimation procedure in the field of social sciences. It is essential for every economics student to be able to interpret it."
  },
  {
    "objectID": "content/08-content.html#assumptions-of-the-ols",
    "href": "content/08-content.html#assumptions-of-the-ols",
    "title": "Regression, Classification",
    "section": "Assumptions of the OLS",
    "text": "Assumptions of the OLS\nThe estimation procedure can sometimes be problematic. It is always necessary to check its assumptions, as drawing incorrect conclusions can occur in case of their violation."
  },
  {
    "objectID": "content/08-content.html#logit",
    "href": "content/08-content.html#logit",
    "title": "Regression, Classification",
    "section": "Logit",
    "text": "Logit\nThe logit method is a straightforward and widely used for classification purposes."
  },
  {
    "objectID": "content/09-content.html",
    "href": "content/09-content.html",
    "title": "Introduction to ggplot2\n",
    "section": "",
    "text": "Although it is possible to utilize base R itself (including diagrams created with the plot command here), this approach is seldom advised, except for its simplicity. Instead, it is highly recommended to employ ggplot2, which, despite being more intricate than base R, offers virtually unlimited possibilities for customization.\n\nIn addition to its enhanced visual aesthetics, the utilization of ggplot is motivated by a significantly more crucial factor. The package derives its name from the seminal work ‚ÄúGrammar of Graphics‚Äù authored by Leland Wilkinson, denoted by the abbreviation gg. This work laid the foundation for the fundamental structure of an ideal graphing algorithm. As ggplot2 adheres to these principles, one will observe that, while there may be numerous considerations to bear in mind, everything falls into place seamlessly. Once you have attained mastery in this subject matter (and engaged in further supplementary readings), you will possess the capability to generate any requisite diagrams. In my personal opinion, I firmly believe that constructing both simple and intricate plots serves as the optimal approach for comprehending your data and captivating the attention of others towards your research."
  },
  {
    "objectID": "content/09-content.html#motivation-for-ggplot",
    "href": "content/09-content.html#motivation-for-ggplot",
    "title": "Introduction to ggplot2\n",
    "section": "",
    "text": "Although it is possible to utilize base R itself (including diagrams created with the plot command here), this approach is seldom advised, except for its simplicity. Instead, it is highly recommended to employ ggplot2, which, despite being more intricate than base R, offers virtually unlimited possibilities for customization.\n\nIn addition to its enhanced visual aesthetics, the utilization of ggplot is motivated by a significantly more crucial factor. The package derives its name from the seminal work ‚ÄúGrammar of Graphics‚Äù authored by Leland Wilkinson, denoted by the abbreviation gg. This work laid the foundation for the fundamental structure of an ideal graphing algorithm. As ggplot2 adheres to these principles, one will observe that, while there may be numerous considerations to bear in mind, everything falls into place seamlessly. Once you have attained mastery in this subject matter (and engaged in further supplementary readings), you will possess the capability to generate any requisite diagrams. In my personal opinion, I firmly believe that constructing both simple and intricate plots serves as the optimal approach for comprehending your data and captivating the attention of others towards your research."
  },
  {
    "objectID": "content/09-content.html#introduction-to-ggplot",
    "href": "content/09-content.html#introduction-to-ggplot",
    "title": "Introduction to ggplot2\n",
    "section": "Introduction to ggplot",
    "text": "Introduction to ggplot\nGgplot2 can be installed standalone, or this package is part of a ‚Äúpackage collection‚Äù called tidyverse. I highly recommend you to always start your session with loading the tidyverse, since this collection contains many crucial function for data analyses. Thus, depending on whether the package is already downloaded, install install.packages (\"tidyverse\") or if this is not the first time you use tidyverse on your computer, then load the packages with library(tidyverse).\n\nlibrary(tidyverse)\n\nWith ggplot2 we get some additional data tables, for the sake of simplicity we now use diamonds of these. Let‚Äôs take a look at them:\n\ndiamonds\n\n# A tibble: 53,940 √ó 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ‚Ñπ 53,930 more rows\n\n\nLets see our first plot with ggplot.\n\nggplot(data = diamonds, mapping = aes(x = depth, y = price)) + \n  geom_point()\n\n\n\nour first plot with ggplot.\n\n\n\nThis is a typical syntax for creating a plot with ggplot.\n\n\n\n\n\n\nCaution\n\n\n\nEach plot consist of 3 parts:\n\nggplot function with two inputs: which data to use and how (mapping)\naesthetics: this is offen inserted into the ggplot function (the mapping argument). The code above means that values of depth columns on the x-axis and values of price column on the y-axis\ngeom: specifies the plot type, currently we create a point graph (scatter plot). Type geom and hit TAB to see all the possibilities.\n\n\n\nWe have many options to customise or plot. First, lets see what we can change inside the geom function.\nColor\n\nggplot(data = diamonds, mapping = aes(x = depth, y = price)) + \n  geom_point(color = \"blue\")\n\n\n\nChanging the color of points to blue.\n\n\n\nSize\n\nggplot(data = diamonds, mapping = aes(x = depth, y = price)) + \n  geom_point(size = 5)\n\n\n\nChanging the size of the points.\n\n\n\nShape\nOnly some geom functions have shape argument, but geom_point has. By that we can modify the shape type.\n\nggplot(data = diamonds, mapping = aes(x = depth, y = price)) + \n  geom_point(shape = 3) # change the shape of the points\n\n\n\nChanging the shape.\n\n\n\nThis is important because the default shape type has only color argument. If you want border to your points then you have to change the shape to 21.\n\nggplot(data = diamonds, mapping = aes(x = depth, y = price)) + \n  geom_point(shape = 21, color = \"blue\", fill = \"red\")\n\n\n\n\nAesthetics\nIn the examples above we used color, size and shape arguments in explicit way inside the geom function. But what if we want to add different colors based on the cut column or different shape (as this is usually required in academic papers). You have to add then these argument to the mapping with the aes function.\n\nggplot(data = diamonds, mapping = aes(x = depth, y = price, color = cut, shape = cut))  +\n  geom_point()\n\n\n\n\nEverything what is not x or y and is in the aes function appears on the legend. You can omit this by adding show.legend = FALSE:\n\nggplot(data = diamonds, mapping = aes(x = depth, y = price, color = cut, shape = cut))  +\n  geom_point(show.legend = FALSE) # hide legend"
  },
  {
    "objectID": "content/09-content.html#customizing",
    "href": "content/09-content.html#customizing",
    "title": "Introduction to ggplot2\n",
    "section": "customizing",
    "text": "customizing\nYou have to use the scale_ functions to specify the colors on the plot above. For academic work you must often use greyscale colors:\n\nggplot(data = diamonds, mapping = aes(x = depth, y = price, color = cut, shape = cut))  +\n  geom_point() +\n  scale_color_grey() # using greyscale colors\n\n\n\n\nYou also have to add labels to your plot by the labs function:\n\nggplot(data = diamonds, mapping = aes(x = depth, y = price, color = cut))  +\n  geom_point() +\n  labs(\n    x = \"Depth\",\n    y = \"Price in dollar\",\n    title = \"My awesome title\",\n    color = \"My legend title\",\n    subtitle = \"My awesome subtitle\",\n    caption = \"Details about the source.\"\n  )\n\n\n\n\nWe use theme for customizing how the plot looks.\n\nggplot(data = diamonds, mapping = aes(x = depth, y = price, color = cut, shape = cut))  +\n  geom_point() + \n  theme_minimal() + # theme with white backgroung and no axis line\n  theme(\n    axis.text = element_text(size = 30) # increase text size to unreasonable high\n  )\n\n\n\n\nSome available themes I recommend:\n\n\n\n\nThemes for ggplot\n\n\n\nYou can install packages to find even more themes, like {ggdark} for dark themes. I also add some other options on this example:\n\nggplot(data = diamonds) + \n  aes(x = depth, y = price, color = cut) + \n  geom_point() + \n  labs(x = \"X-axis\", y = \"Y-axis\", \n       title = \"Nice title\", \n       color = \"New legend title\",\n       subtitle = \"Unknown years\") + \n  scale_x_continuous(labels = function(x) str_c(x, \"%\"), # percentage on x-axis\n                     limits = c(50, 70), \n                     expand = c(0, 0) # axis line at the last observation\n                     ) + \n  scale_color_manual( # modify colors\n    values = c(\"red\", \"blue\", \"orange\", \"green\", \"grey\")\n    ) +\n  ggdark::dark_theme_classic() + # dark theme\n  theme(plot.title = element_text(color = \"red\", face = \"bold\")) # change title style\n\n\n\n\nIf you want to use the same theme, then you can make it as default:\n\ntheme_set(theme_minimal())\n\n\nggplot(diamonds, aes(depth, price)) +\n  geom_point()"
  },
  {
    "objectID": "content/09-content.html#geom",
    "href": "content/09-content.html#geom",
    "title": "Introduction to ggplot2\n",
    "section": "Geom",
    "text": "Geom\nLets see what other type of charts exist. For univariate continues data I recommend density, histogram and boxplot.\n\nggplot(diamonds, aes(price)) + \n  geom_density(color = \"red\", fill = \"yellow\")\n\n\n\nDensity plot\n\n\n\n\nggplot(diamonds, aes(price)) +\n  geom_histogram() + \n  scale_x_log10()\n\n\n\nHistogram\n\n\n\n\nggplot(diamonds, aes(price)) +\n  geom_boxplot()\n\n\n\nBoxplot\n\n\n\nWhat do we see on a boxplot?\n\n\n\n\nStructure of a boxplot\n\n\n\nIf we are interested in the outlier, and our plan is to remove them, then it is a good idea to use the base R boxplot function.\n\nboxplot(diamonds$price)\n\n\n\nBoxplot with basea R command.\n\n\n\nThis function has an advantage. Lets assign its output as bplot_output instead of simply plot it, while turning its plot argument to FALSE.\n\nbplot_output &lt;- boxplot(diamonds$price, plot = FALSE) # hide the plot\n\n\nbplot_output\n\n$stats\n        [,1]\n[1,]   326.0\n[2,]   950.0\n[3,]  2401.0\n[4,]  5324.5\n[5,] 11886.0\n\n$n\n[1] 53940\n\n$conf\n        [,1]\n[1,] 2371.24\n[2,] 2430.76\n\n$out\n   [1] 11888 11888 11888 11897 11899 11899 11901 11903 11904 11905 11906 11912\n  [13] 11913 11917 11917 11921 11922 11923 11923 11923 11924 11925 11926 11927\n  [25] 11927 11933 11934 11935 11939 11942 11943 11946 11946 11946 11946 11948\n  [37] 11948 11950 11951 11954 11955 11956 11957 11957 11957 11958 11962 11963\n  [49] 11965 11966 11966 11967 11968 11968 11969 11970 11971 11971 11973 11975\n  [61] 11975 11975 11976 11979 11982 11985 11986 11988 11988 11988 11988 11990\n  [73] 11998 11999 12000 12004 12005 12008 12009 12012 12013 12014 12016 12021\n  [85] 12028 12030 12030 12030 12030 12030 12030 12031 12032 12035 12036 12038\n  [97] 12044 12047 12047 12048 12048 12048 12048 12052 12053 12055 12058 12059\n [109] 12060 12061 12061 12063 12063 12066 12068 12069 12071 12071 12071 12075\n [121] 12078 12078 12079 12081 12081 12082 12084 12084 12084 12085 12085 12087\n [133] 12089 12092 12093 12094 12094 12095 12095 12098 12098 12098 12099 12099\n [145] 12100 12100 12100 12100 12105 12108 12108 12109 12111 12112 12117 12119\n [157] 12121 12121 12123 12127 12140 12141 12146 12148 12148 12150 12150 12151\n [169] 12152 12152 12152 12153 12154 12155 12156 12156 12157 12161 12165 12168\n [181] 12168 12168 12170 12171 12174 12175 12179 12179 12179 12179 12182 12182\n [193] 12183 12185 12186 12190 12193 12195 12196 12196 12196 12196 12196 12196\n [205] 12196 12199 12200 12202 12206 12207 12207 12209 12209 12209 12209 12210\n [217] 12210 12210 12210 12210 12215 12215 12219 12220 12221 12224 12224 12224\n [229] 12225 12226 12226 12228 12228 12229 12229 12230 12231 12232 12236 12237\n [241] 12238 12238 12242 12244 12244 12247 12248 12252 12253 12255 12257 12257\n [253] 12257 12260 12261 12261 12261 12261 12265 12267 12268 12268 12269 12271\n [265] 12271 12271 12273 12282 12283 12284 12284 12285 12286 12288 12291 12291\n [277] 12295 12295 12297 12300 12304 12305 12308 12308 12308 12308 12308 12308\n [289] 12308 12308 12308 12311 12311 12314 12315 12316 12317 12319 12319 12320\n [301] 12321 12327 12327 12332 12334 12336 12336 12338 12338 12338 12338 12338\n [313] 12339 12341 12342 12342 12342 12342 12342 12342 12343 12345 12349 12356\n [325] 12359 12360 12361 12361 12364 12364 12369 12369 12369 12369 12373 12375\n [337] 12377 12377 12377 12378 12378 12379 12379 12380 12380 12381 12386 12386\n [349] 12388 12389 12390 12392 12392 12392 12392 12394 12394 12396 12400 12401\n [361] 12401 12401 12401 12403 12406 12407 12407 12409 12415 12416 12416 12417\n [373] 12417 12418 12423 12423 12423 12429 12430 12431 12431 12432 12433 12437\n [385] 12437 12440 12440 12443 12447 12450 12451 12454 12454 12454 12455 12458\n [397] 12458 12459 12459 12459 12462 12465 12466 12467 12467 12468 12468 12474\n [409] 12477 12483 12485 12489 12490 12492 12492 12493 12494 12494 12495 12498\n [421] 12499 12499 12499 12500 12502 12506 12508 12509 12512 12515 12521 12522\n [433] 12526 12529 12530 12530 12531 12531 12539 12539 12541 12542 12543 12545\n [445] 12545 12545 12546 12547 12547 12549 12551 12551 12551 12551 12554 12554\n [457] 12554 12554 12554 12554 12555 12556 12559 12561 12565 12566 12571 12573\n [469] 12576 12576 12576 12581 12581 12581 12584 12587 12587 12587 12591 12592\n [481] 12592 12596 12598 12602 12603 12605 12606 12606 12607 12607 12608 12610\n [493] 12610 12612 12613 12614 12615 12616 12616 12617 12617 12617 12617 12618\n [505] 12620 12621 12621 12621 12622 12622 12626 12629 12631 12633 12633 12637\n [517] 12639 12641 12641 12642 12644 12644 12645 12646 12647 12648 12648 12648\n [529] 12654 12654 12654 12654 12655 12655 12655 12657 12670 12671 12671 12672\n [541] 12674 12674 12677 12677 12680 12680 12681 12681 12681 12681 12683 12686\n [553] 12687 12688 12690 12693 12693 12696 12696 12696 12697 12698 12700 12702\n [565] 12707 12707 12709 12712 12713 12713 12714 12716 12717 12717 12717 12720\n [577] 12720 12722 12723 12725 12725 12730 12731 12734 12736 12736 12737 12737\n [589] 12738 12738 12738 12738 12738 12743 12744 12745 12747 12748 12753 12754\n [601] 12755 12755 12756 12756 12756 12756 12762 12764 12765 12765 12766 12768\n [613] 12770 12770 12771 12773 12776 12778 12779 12779 12779 12787 12787 12787\n [625] 12787 12787 12787 12788 12791 12791 12792 12794 12795 12798 12798 12799\n [637] 12799 12800 12800 12809 12810 12811 12812 12814 12816 12818 12821 12821\n [649] 12821 12822 12823 12823 12823 12825 12825 12828 12829 12829 12829 12829\n [661] 12829 12830 12831 12831 12832 12832 12833 12839 12840 12840 12841 12842\n [673] 12843 12844 12844 12845 12846 12846 12848 12848 12851 12853 12857 12857\n [685] 12862 12862 12864 12865 12870 12870 12871 12872 12872 12872 12872 12874\n [697] 12874 12880 12883 12883 12884 12891 12891 12895 12896 12897 12898 12899\n [709] 12900 12905 12906 12906 12907 12907 12907 12907 12907 12910 12912 12915\n [721] 12916 12918 12921 12923 12927 12929 12931 12931 12932 12937 12939 12939\n [733] 12940 12943 12944 12945 12947 12948 12948 12951 12956 12956 12958 12958\n [745] 12958 12961 12963 12964 12967 12968 12968 12968 12970 12970 12971 12978\n [757] 12978 12979 12980 12981 12981 12981 12985 12985 12985 12987 12988 12989\n [769] 12990 12991 12992 12992 12996 12996 12998 12998 13001 13001 13003 13006\n [781] 13006 13007 13007 13007 13009 13010 13012 13014 13015 13016 13026 13027\n [793] 13029 13034 13034 13034 13034 13037 13037 13037 13038 13043 13046 13047\n [805] 13047 13049 13052 13060 13060 13061 13063 13063 13063 13065 13065 13068\n [817] 13068 13068 13069 13069 13074 13075 13077 13078 13079 13080 13081 13084\n [829] 13085 13088 13092 13092 13095 13096 13097 13097 13098 13099 13102 13104\n [841] 13107 13109 13109 13109 13110 13111 13112 13113 13115 13117 13117 13119\n [853] 13119 13120 13122 13127 13132 13132 13133 13134 13134 13134 13135 13135\n [865] 13135 13140 13144 13148 13152 13153 13154 13155 13156 13157 13157 13160\n [877] 13161 13162 13162 13163 13165 13168 13169 13171 13177 13178 13178 13182\n [889] 13182 13182 13182 13187 13189 13190 13194 13194 13196 13196 13198 13199\n [901] 13200 13200 13201 13203 13203 13205 13206 13206 13207 13211 13212 13214\n [913] 13215 13219 13221 13221 13225 13228 13228 13228 13229 13229 13229 13230\n [925] 13232 13233 13234 13239 13242 13247 13248 13248 13248 13248 13250 13250\n [937] 13250 13253 13253 13254 13254 13256 13257 13261 13263 13263 13267 13275\n [949] 13275 13278 13278 13278 13280 13282 13284 13286 13287 13287 13287 13288\n [961] 13289 13291 13292 13293 13298 13298 13298 13298 13299 13307 13307 13312\n [973] 13316 13317 13317 13317 13320 13320 13320 13320 13320 13321 13323 13324\n [985] 13325 13325 13326 13329 13329 13329 13333 13334 13337 13338 13340 13342\n [997] 13344 13348 13351 13355 13355 13357 13360 13363 13363 13365 13367 13367\n[1009] 13369 13369 13369 13370 13370 13372 13373 13375 13376 13377 13378 13379\n[1021] 13387 13387 13387 13387 13389 13393 13395 13397 13398 13398 13399 13400\n[1033] 13400 13403 13405 13406 13417 13420 13420 13421 13421 13423 13427 13428\n[1045] 13434 13437 13439 13442 13445 13445 13445 13445 13453 13453 13460 13462\n[1057] 13462 13464 13464 13465 13474 13474 13477 13477 13480 13483 13485 13485\n[1069] 13485 13486 13488 13495 13498 13499 13499 13499 13500 13500 13500 13502\n[1081] 13503 13506 13506 13508 13512 13513 13515 13528 13530 13530 13531 13532\n[1093] 13536 13537 13539 13540 13542 13542 13542 13543 13544 13550 13552 13553\n[1105] 13553 13553 13554 13554 13555 13556 13557 13560 13561 13563 13564 13572\n[1117] 13574 13574 13578 13579 13582 13587 13587 13587 13587 13588 13588 13588\n[1129] 13595 13595 13596 13596 13596 13597 13597 13598 13598 13599 13600 13603\n[1141] 13605 13606 13607 13609 13609 13609 13610 13615 13619 13621 13622 13622\n[1153] 13622 13622 13623 13623 13624 13626 13629 13629 13630 13631 13632 13638\n[1165] 13642 13642 13645 13646 13653 13653 13653 13654 13655 13659 13660 13661\n[1177] 13665 13666 13667 13669 13671 13675 13675 13675 13677 13678 13680 13681\n[1189] 13686 13686 13687 13691 13691 13693 13701 13701 13702 13703 13703 13710\n[1201] 13711 13711 13713 13714 13719 13720 13720 13720 13720 13721 13724 13725\n[1213] 13726 13728 13730 13731 13732 13733 13734 13735 13736 13737 13737 13744\n[1225] 13744 13746 13752 13753 13755 13756 13757 13757 13757 13760 13761 13761\n[1237] 13764 13764 13766 13767 13768 13768 13769 13771 13771 13777 13777 13779\n[1249] 13782 13782 13784 13786 13786 13786 13787 13790 13790 13790 13794 13796\n[1261] 13797 13799 13800 13803 13807 13809 13811 13811 13811 13811 13811 13812\n[1273] 13812 13813 13818 13818 13819 13820 13820 13823 13823 13824 13825 13825\n[1285] 13827 13828 13828 13828 13831 13833 13839 13844 13844 13846 13846 13846\n[1297] 13849 13849 13850 13853 13853 13853 13853 13858 13864 13865 13867 13869\n[1309] 13872 13872 13873 13873 13879 13880 13882 13884 13885 13887 13892 13892\n[1321] 13899 13903 13903 13904 13905 13907 13908 13908 13912 13912 13917 13919\n[1333] 13919 13919 13919 13921 13923 13926 13929 13929 13930 13931 13933 13938\n[1345] 13939 13940 13942 13945 13945 13945 13945 13948 13949 13950 13951 13953\n[1357] 13956 13963 13963 13965 13968 13970 13976 13978 13983 13986 13986 13986\n[1369] 13991 13991 13993 13993 13994 13994 13995 13995 13996 13996 13998 14014\n[1381] 14017 14022 14024 14026 14027 14027 14028 14031 14032 14032 14033 14033\n[1393] 14037 14038 14039 14040 14042 14042 14042 14044 14047 14057 14058 14061\n[1405] 14065 14065 14066 14067 14067 14068 14071 14071 14071 14071 14071 14074\n[1417] 14080 14083 14084 14092 14095 14095 14103 14103 14105 14105 14105 14106\n[1429] 14107 14108 14111 14112 14119 14120 14125 14126 14127 14129 14130 14137\n[1441] 14138 14139 14146 14146 14148 14150 14154 14156 14157 14165 14165 14165\n[1453] 14165 14165 14167 14171 14174 14177 14179 14180 14182 14184 14185 14185\n[1465] 14188 14190 14192 14192 14194 14196 14199 14199 14199 14199 14199 14199\n[1477] 14199 14201 14205 14208 14208 14209 14211 14214 14215 14217 14220 14220\n[1489] 14220 14224 14224 14229 14231 14234 14234 14236 14236 14237 14238 14239\n[1501] 14240 14242 14242 14242 14245 14247 14247 14249 14251 14256 14256 14258\n[1513] 14266 14266 14267 14268 14275 14277 14277 14278 14279 14281 14282 14283\n[1525] 14285 14292 14293 14294 14294 14294 14294 14294 14295 14298 14299 14300\n[1537] 14300 14300 14300 14304 14308 14308 14308 14319 14319 14321 14323 14328\n[1549] 14330 14330 14334 14338 14340 14341 14341 14341 14344 14348 14350 14350\n[1561] 14351 14352 14354 14359 14359 14359 14361 14362 14364 14364 14368 14372\n[1573] 14372 14372 14375 14375 14375 14375 14383 14383 14383 14386 14386 14386\n[1585] 14386 14386 14388 14388 14394 14394 14395 14399 14400 14402 14403 14404\n[1597] 14406 14407 14408 14410 14411 14412 14414 14414 14414 14416 14416 14421\n[1609] 14424 14424 14426 14426 14426 14426 14428 14428 14429 14429 14430 14430\n[1621] 14433 14438 14444 14445 14447 14451 14452 14452 14452 14453 14456 14462\n[1633] 14465 14474 14476 14477 14479 14481 14482 14482 14482 14482 14482 14482\n[1645] 14482 14483 14486 14488 14489 14490 14494 14495 14498 14498 14500 14502\n[1657] 14502 14502 14502 14503 14505 14507 14507 14507 14519 14525 14527 14527\n[1669] 14527 14529 14534 14540 14542 14542 14542 14542 14543 14543 14544 14545\n[1681] 14548 14556 14558 14558 14558 14561 14565 14574 14574 14577 14578 14579\n[1693] 14581 14581 14581 14584 14586 14588 14592 14593 14593 14593 14597 14603\n[1705] 14603 14603 14605 14611 14615 14616 14616 14618 14620 14623 14624 14625\n[1717] 14626 14634 14637 14637 14637 14638 14639 14646 14646 14648 14650 14652\n[1729] 14654 14654 14659 14660 14660 14662 14663 14666 14666 14667 14673 14674\n[1741] 14674 14674 14674 14675 14680 14683 14687 14691 14691 14692 14698 14699\n[1753] 14704 14709 14709 14709 14709 14711 14715 14715 14717 14719 14720 14724\n[1765] 14725 14725 14727 14731 14732 14733 14735 14737 14740 14744 14745 14745\n[1777] 14749 14749 14750 14750 14752 14759 14763 14763 14766 14768 14773 14775\n[1789] 14775 14775 14775 14775 14777 14779 14787 14787 14790 14790 14792 14795\n[1801] 14799 14801 14802 14803 14810 14811 14812 14813 14814 14817 14819 14824\n[1813] 14826 14830 14833 14837 14837 14838 14841 14842 14842 14844 14844 14844\n[1825] 14844 14847 14853 14855 14857 14859 14860 14863 14866 14867 14870 14882\n[1837] 14888 14889 14889 14889 14892 14893 14900 14904 14904 14915 14918 14918\n[1849] 14918 14918 14920 14925 14931 14933 14935 14936 14936 14937 14938 14939\n[1861] 14945 14947 14948 14948 14949 14952 14956 14957 14959 14961 14968 14968\n[1873] 14970 14973 14973 14975 14976 14982 14982 14982 14998 14998 15000 15002\n[1885] 15005 15007 15011 15013 15014 15014 15017 15022 15025 15025 15025 15025\n[1897] 15026 15030 15030 15031 15032 15032 15035 15035 15036 15038 15043 15046\n[1909] 15047 15052 15053 15055 15059 15064 15064 15065 15065 15067 15072 15073\n[1921] 15076 15079 15081 15081 15081 15083 15086 15091 15092 15092 15092 15093\n[1933] 15095 15096 15097 15100 15102 15102 15105 15105 15105 15109 15110 15110\n[1945] 15111 15116 15118 15119 15122 15124 15126 15132 15134 15134 15137 15140\n[1957] 15140 15143 15144 15145 15147 15147 15148 15151 15152 15153 15153 15162\n[1969] 15164 15164 15164 15166 15169 15169 15172 15175 15178 15184 15185 15185\n[1981] 15185 15188 15188 15189 15193 15197 15197 15198 15201 15210 15210 15214\n[1993] 15217 15217 15218 15219 15223 15223 15225 15226 15229 15231 15231 15231\n[2005] 15235 15238 15238 15239 15240 15241 15245 15246 15246 15247 15247 15248\n[2017] 15249 15249 15252 15253 15254 15255 15255 15258 15258 15259 15261 15265\n[2029] 15272 15275 15278 15281 15281 15282 15282 15287 15288 15291 15291 15291\n[2041] 15291 15293 15301 15303 15303 15306 15308 15309 15312 15312 15316 15316\n[2053] 15318 15320 15321 15322 15323 15323 15324 15330 15334 15334 15335 15338\n[2065] 15339 15339 15348 15351 15354 15364 15365 15365 15366 15370 15371 15375\n[2077] 15377 15378 15379 15384 15385 15386 15392 15393 15393 15394 15394 15394\n[2089] 15395 15395 15395 15397 15398 15404 15412 15413 15415 15415 15418 15420\n[2101] 15420 15424 15426 15426 15426 15428 15430 15433 15440 15444 15444 15446\n[2113] 15450 15451 15451 15451 15454 15458 15460 15461 15464 15465 15466 15467\n[2125] 15472 15474 15475 15475 15478 15485 15485 15486 15488 15494 15497 15498\n[2137] 15499 15505 15505 15508 15509 15510 15511 15512 15513 15515 15517 15520\n[2149] 15522 15528 15528 15528 15529 15530 15531 15535 15540 15540 15543 15544\n[2161] 15559 15562 15562 15562 15563 15568 15575 15579 15581 15584 15584 15584\n[2173] 15585 15587 15589 15592 15594 15600 15601 15602 15606 15606 15609 15609\n[2185] 15613 15615 15618 15618 15618 15619 15627 15646 15647 15649 15649 15651\n[2197] 15651 15651 15653 15654 15658 15662 15665 15666 15671 15671 15673 15675\n[2209] 15680 15683 15684 15686 15688 15690 15691 15694 15695 15696 15697 15701\n[2221] 15706 15707 15708 15708 15710 15714 15716 15717 15718 15721 15729 15729\n[2233] 15729 15729 15730 15740 15745 15745 15746 15751 15751 15756 15757 15760\n[2245] 15760 15760 15762 15765 15766 15767 15769 15773 15773 15773 15776 15776\n[2257] 15783 15785 15785 15788 15792 15792 15792 15792 15794 15797 15797 15801\n[2269] 15801 15801 15802 15802 15804 15804 15805 15805 15806 15808 15811 15813\n[2281] 15814 15818 15818 15819 15819 15819 15821 15823 15824 15825 15826 15826\n[2293] 15827 15829 15829 15832 15836 15837 15840 15840 15840 15841 15841 15842\n[2305] 15843 15845 15847 15847 15848 15848 15850 15851 15851 15851 15851 15851\n[2317] 15852 15857 15864 15873 15874 15874 15877 15878 15887 15888 15888 15889\n[2329] 15897 15897 15897 15898 15907 15908 15913 15915 15917 15917 15917 15919\n[2341] 15919 15919 15920 15922 15923 15928 15930 15930 15931 15934 15937 15938\n[2353] 15939 15939 15941 15941 15941 15942 15946 15948 15948 15949 15949 15952\n[2365] 15955 15957 15959 15959 15962 15964 15965 15966 15968 15970 15970 15974\n[2377] 15977 15977 15983 15984 15984 15984 15984 15987 15987 15990 15991 15992\n[2389] 15992 15992 15992 15992 15992 15993 15996 15996 16003 16004 16013 16018\n[2401] 16021 16023 16025 16031 16036 16037 16041 16043 16048 16049 16052 16055\n[2413] 16059 16062 16062 16064 16064 16064 16068 16068 16073 16073 16075 16077\n[2425] 16080 16082 16085 16086 16086 16087 16091 16092 16097 16098 16100 16104\n[2437] 16104 16111 16112 16112 16116 16123 16126 16128 16129 16130 16131 16137\n[2449] 16140 16146 16147 16148 16149 16149 16151 16169 16169 16169 16170 16171\n[2461] 16171 16174 16179 16181 16183 16187 16187 16188 16189 16190 16191 16192\n[2473] 16193 16195 16198 16198 16198 16206 16210 16215 16219 16220 16223 16224\n[2485] 16231 16231 16232 16234 16235 16235 16237 16239 16239 16239 16240 16240\n[2497] 16241 16241 16241 16242 16253 16253 16256 16256 16261 16262 16273 16274\n[2509] 16277 16278 16280 16280 16286 16287 16287 16287 16290 16291 16294 16294\n[2521] 16295 16297 16300 16300 16304 16304 16304 16309 16309 16311 16314 16316\n[2533] 16316 16316 16319 16319 16319 16323 16329 16336 16337 16339 16340 16340\n[2545] 16343 16353 16353 16357 16357 16358 16363 16364 16364 16368 16369 16370\n[2557] 16378 16380 16383 16384 16386 16389 16390 16392 16392 16395 16397 16397\n[2569] 16398 16400 16402 16404 16406 16407 16407 16409 16410 16412 16420 16422\n[2581] 16425 16426 16427 16427 16427 16431 16437 16439 16442 16446 16450 16451\n[2593] 16459 16462 16462 16465 16466 16466 16469 16472 16479 16479 16483 16484\n[2605] 16485 16492 16499 16499 16505 16506 16506 16507 16512 16512 16513 16518\n[2617] 16519 16520 16521 16530 16532 16533 16538 16544 16544 16545 16547 16547\n[2629] 16551 16558 16558 16558 16560 16562 16564 16565 16570 16575 16575 16580\n[2641] 16582 16582 16583 16587 16589 16592 16593 16599 16601 16603 16611 16613\n[2653] 16616 16617 16618 16624 16626 16626 16626 16626 16628 16628 16629 16629\n[2665] 16629 16632 16636 16641 16642 16643 16643 16650 16650 16650 16656 16657\n[2677] 16665 16669 16670 16677 16683 16687 16687 16688 16689 16690 16693 16694\n[2689] 16694 16700 16703 16704 16704 16707 16709 16709 16709 16715 16716 16716\n[2701] 16716 16717 16718 16718 16723 16723 16728 16731 16733 16733 16733 16733\n[2713] 16733 16733 16736 16737 16742 16747 16750 16754 16768 16769 16776 16776\n[2725] 16778 16778 16778 16778 16778 16778 16778 16779 16779 16779 16783 16783\n[2737] 16783 16783 16786 16787 16789 16789 16789 16790 16791 16792 16793 16793\n[2749] 16797 16800 16801 16803 16804 16805 16807 16808 16811 16813 16817 16819\n[2761] 16820 16823 16824 16826 16842 16842 16854 16857 16861 16872 16872 16874\n[2773] 16878 16879 16881 16881 16889 16896 16900 16900 16900 16901 16904 16914\n[2785] 16914 16914 16914 16915 16916 16921 16922 16929 16931 16934 16937 16941\n[2797] 16942 16944 16945 16948 16954 16955 16955 16956 16956 16957 16960 16960\n[2809] 16960 16969 16970 16970 16975 16985 16985 16987 16988 16992 16994 16996\n[2821] 17000 17001 17003 17005 17006 17009 17010 17012 17014 17016 17017 17019\n[2833] 17024 17024 17027 17028 17028 17028 17029 17036 17038 17039 17041 17042\n[2845] 17045 17045 17049 17049 17050 17051 17051 17052 17053 17057 17057 17062\n[2857] 17063 17065 17066 17068 17068 17068 17068 17068 17073 17073 17076 17078\n[2869] 17079 17081 17084 17094 17095 17095 17096 17099 17100 17103 17108 17111\n[2881] 17114 17114 17115 17116 17118 17123 17125 17126 17127 17136 17138 17141\n[2893] 17143 17143 17146 17149 17151 17153 17153 17156 17160 17162 17164 17166\n[2905] 17168 17168 17172 17172 17175 17176 17179 17179 17182 17184 17186 17191\n[2917] 17191 17192 17193 17193 17194 17197 17197 17203 17203 17204 17204 17206\n[2929] 17209 17209 17213 17214 17216 17218 17219 17219 17220 17221 17223 17224\n[2941] 17227 17228 17231 17233 17235 17235 17237 17240 17244 17245 17247 17250\n[2953] 17254 17256 17258 17262 17263 17263 17265 17265 17265 17267 17273 17278\n[2965] 17279 17294 17294 17294 17294 17297 17312 17313 17313 17315 17317 17323\n[2977] 17323 17327 17329 17329 17330 17334 17338 17339 17343 17347 17351 17353\n[2989] 17353 17357 17358 17360 17360 17365 17365 17366 17374 17377 17379 17381\n[3001] 17383 17392 17393 17393 17400 17403 17403 17403 17403 17405 17405 17407\n[3013] 17408 17414 17416 17422 17425 17433 17433 17434 17436 17441 17442 17447\n[3025] 17448 17449 17451 17452 17455 17458 17460 17469 17469 17472 17473 17474\n[3037] 17475 17476 17485 17489 17492 17492 17496 17497 17499 17504 17509 17513\n[3049] 17514 17515 17516 17521 17522 17523 17525 17530 17533 17534 17534 17535\n[3061] 17539 17545 17548 17552 17553 17554 17555 17569 17569 17570 17574 17579\n[3073] 17581 17582 17590 17591 17592 17595 17597 17597 17598 17600 17604 17605\n[3085] 17606 17607 17607 17608 17609 17614 17614 17617 17634 17640 17642 17649\n[3097] 17650 17658 17659 17662 17663 17666 17667 17672 17673 17673 17674 17676\n[3109] 17676 17685 17688 17688 17689 17689 17692 17694 17710 17712 17713 17714\n[3121] 17715 17716 17723 17724 17729 17730 17740 17742 17746 17747 17751 17752\n[3133] 17753 17759 17759 17760 17760 17760 17760 17760 17760 17761 17765 17766\n[3145] 17772 17773 17776 17778 17784 17798 17801 17803 17803 17804 17805 17805\n[3157] 17811 17816 17820 17825 17825 17826 17829 17835 17837 17838 17840 17841\n[3169] 17849 17849 17849 17849 17856 17864 17869 17871 17871 17877 17882 17887\n[3181] 17888 17889 17891 17891 17892 17892 17892 17893 17893 17893 17894 17895\n[3193] 17898 17902 17904 17904 17905 17909 17916 17917 17920 17923 17924 17930\n[3205] 17932 17934 17936 17936 17938 17949 17952 17952 17953 17955 17955 17957\n[3217] 17975 17983 17986 17987 17988 17989 17995 17996 17999 17999 18001 18002\n[3229] 18003 18005 18007 18014 18017 18018 18020 18023 18026 18026 18026 18026\n[3241] 18027 18027 18028 18029 18029 18034 18034 18034 18037 18041 18050 18055\n[3253] 18055 18057 18059 18062 18062 18066 18067 18068 18069 18071 18077 18077\n[3265] 18077 18077 18080 18090 18102 18104 18107 18107 18108 18112 18113 18114\n[3277] 18115 18115 18115 18117 18118 18119 18120 18120 18124 18124 18125 18127\n[3289] 18128 18128 18128 18128 18139 18139 18149 18149 18152 18153 18159 18164\n[3301] 18166 18168 18172 18172 18176 18178 18179 18179 18181 18183 18186 18186\n[3313] 18188 18188 18190 18193 18193 18193 18198 18206 18207 18207 18211 18215\n[3325] 18231 18231 18232 18236 18236 18236 18236 18239 18242 18242 18242 18242\n[3337] 18242 18251 18252 18252 18254 18255 18257 18259 18274 18275 18276 18279\n[3349] 18279 18281 18281 18286 18291 18293 18294 18295 18295 18296 18299 18304\n[3361] 18306 18308 18310 18312 18318 18318 18320 18324 18325 18325 18340 18342\n[3373] 18342 18343 18344 18358 18359 18363 18364 18369 18371 18371 18371 18371\n[3385] 18374 18374 18376 18377 18392 18394 18395 18398 18398 18398 18405 18407\n[3397] 18416 18419 18421 18423 18426 18426 18426 18429 18430 18430 18430 18431\n[3409] 18432 18435 18439 18440 18440 18442 18443 18445 18447 18447 18458 18462\n[3421] 18462 18468 18470 18472 18474 18475 18477 18480 18481 18483 18485 18487\n[3433] 18489 18493 18495 18500 18507 18507 18508 18508 18509 18515 18522 18524\n[3445] 18525 18526 18528 18531 18531 18532 18535 18541 18541 18542 18552 18557\n[3457] 18559 18559 18559 18561 18561 18565 18571 18572 18574 18575 18578 18593\n[3469] 18594 18598 18599 18604 18607 18611 18614 18615 18625 18630 18640 18640\n[3481] 18648 18653 18656 18659 18663 18674 18678 18678 18680 18682 18686 18691\n[3493] 18692 18692 18693 18700 18700 18701 18705 18706 18707 18709 18710 18710\n[3505] 18717 18718 18729 18730 18731 18735 18736 18741 18741 18741 18745 18756\n[3517] 18757 18759 18760 18766 18768 18777 18779 18780 18781 18784 18787 18788\n[3529] 18791 18791 18795 18795 18797 18803 18804 18806 18818 18823\n\n$group\n   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [186] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [297] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [334] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [371] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [408] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [445] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [482] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [519] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [556] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [593] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [630] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [667] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [704] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [741] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [778] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [815] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [852] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [889] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [926] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [963] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1000] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1037] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1074] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1111] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1148] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1185] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1222] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1259] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1296] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1333] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1370] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1407] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1444] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1481] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1518] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1555] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1592] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1629] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1666] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1703] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1740] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1777] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1814] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1851] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1888] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1925] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1962] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1999] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2036] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2073] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2110] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2147] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2184] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2221] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2258] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2295] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2332] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2369] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2406] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2443] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2480] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2517] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2554] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2591] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2628] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2665] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2702] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2739] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2776] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2813] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2850] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2887] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2924] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2961] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2998] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3035] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3072] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3109] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3146] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3183] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3220] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3257] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3294] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3331] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3368] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3405] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3442] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3479] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3516] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n$names\n[1] \"1\"\n\n\nbplot_output has an element named as out. This contains the values of those observations who are outliers based on boxplot. We can simply check if a given observation is in this vector and if it is, then we remove that observation [^It is always more complicated in real life tasks, but fine for now.]. Lets create a function for this step.\n\noutlier_filter &lt;- function(x) {\noutlier_value &lt;- boxplot(x, plot = FALSE)$out\nx %in% outlier_value # returns TRUE if the given value is an outlier\n}\n\n\ndiamonds %&gt;% \n  filter(!outlier_filter(price)) # keep the observation if price is not an outlier\n\n# A tibble: 50,402 √ó 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ‚Ñπ 50,392 more rows\n\n\nFor univariate discrete variables you can use column chart. You have two options for that:\n\ngeom_bar: just specify which discrete variable you want to use, and the number of observations will be returned by category.\ngeom_col: Add a discrete variable for categories and a continues variable for the corresponding values. You have to calculate the number of observation as an initial step.\n\n\nggplot(diamonds, aes(cut)) + \n  geom_bar()\n\n\n\nGeom_bar\n\n\n\n\ndiamonds %&gt;% \n  count(cut) %&gt;% # calculating number of observations by category\n  ggplot(aes(cut, n)) + \n  geom_col()\n\n\n\nGeom_col\n\n\n\nYou can also use geom_bar for two categorical variables:\n\nggplot(data = diamonds, aes(cut, fill = color))  +\n  geom_bar(color = \"black\") # color -&gt; add border\n\n\n\nBar chart with fill argument.\n\n\n\nOr with proportions:\n\nggplot(data = diamonds, aes(cut, fill = color))  +\n  geom_bar(color = \"black\", position = position_fill()) # fill to 100%\n\n\n\nPosition_fill\n\n\n\nFor any kind of matrix you can use the geom_tile for creating heatmaps. Lets see the correlations!\n\ndiamonds %&gt;% \n   select_if(is.numeric) # you can calculate cor only for numerical variables\n\n# A tibble: 53,940 √ó 7\n   carat depth table price     x     y     z\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23  61.5    55   326  3.95  3.98  2.43\n 2  0.21  59.8    61   326  3.89  3.84  2.31\n 3  0.23  56.9    65   327  4.05  4.07  2.31\n 4  0.29  62.4    58   334  4.2   4.23  2.63\n 5  0.31  63.3    58   335  4.34  4.35  2.75\n 6  0.24  62.8    57   336  3.94  3.96  2.48\n 7  0.24  62.3    57   336  3.95  3.98  2.47\n 8  0.26  61.9    55   337  4.07  4.11  2.53\n 9  0.22  65.1    61   337  3.87  3.78  2.49\n10  0.23  59.4    61   338  4     4.05  2.39\n# ‚Ñπ 53,930 more rows\n\ndiamonds %&gt;% \n   select_if(is.numeric) %&gt;%  # you can calculate cor only for numerical variables\n  cor() # correlation matrix\n\n           carat       depth      table      price           x           y\ncarat 1.00000000  0.02822431  0.1816175  0.9215913  0.97509423  0.95172220\ndepth 0.02822431  1.00000000 -0.2957785 -0.0106474 -0.02528925 -0.02934067\ntable 0.18161755 -0.29577852  1.0000000  0.1271339  0.19534428  0.18376015\nprice 0.92159130 -0.01064740  0.1271339  1.0000000  0.88443516  0.86542090\nx     0.97509423 -0.02528925  0.1953443  0.8844352  1.00000000  0.97470148\ny     0.95172220 -0.02934067  0.1837601  0.8654209  0.97470148  1.00000000\nz     0.95338738  0.09492388  0.1509287  0.8612494  0.97077180  0.95200572\n               z\ncarat 0.95338738\ndepth 0.09492388\ntable 0.15092869\nprice 0.86124944\nx     0.97077180\ny     0.95200572\nz     1.00000000\n\ndiamonds %&gt;% \n   select_if(is.numeric) %&gt;%  # you can calculate cor only for numerical variables\n  cor() %&gt;%  # correlation matrix\n  data.frame() %&gt;% \n  rownames_to_column(var = \"X\") # make rowname to \"X\" column\n\n      X      carat       depth      table      price           x           y\n1 carat 1.00000000  0.02822431  0.1816175  0.9215913  0.97509423  0.95172220\n2 depth 0.02822431  1.00000000 -0.2957785 -0.0106474 -0.02528925 -0.02934067\n3 table 0.18161755 -0.29577852  1.0000000  0.1271339  0.19534428  0.18376015\n4 price 0.92159130 -0.01064740  0.1271339  1.0000000  0.88443516  0.86542090\n5     x 0.97509423 -0.02528925  0.1953443  0.8844352  1.00000000  0.97470148\n6     y 0.95172220 -0.02934067  0.1837601  0.8654209  0.97470148  1.00000000\n7     z 0.95338738  0.09492388  0.1509287  0.8612494  0.97077180  0.95200572\n           z\n1 0.95338738\n2 0.09492388\n3 0.15092869\n4 0.86124944\n5 0.97077180\n6 0.95200572\n7 1.00000000\n\ndiamonds %&gt;% \n   select_if(is.numeric) %&gt;%  # you can calculate cor only for numerical variables\n  cor() %&gt;%  # correlation matrix\n  data.frame() %&gt;% \n  rownames_to_column(var = \"X\") %&gt;% \n  pivot_longer(-X, names_to = \"Y\")\n\n# A tibble: 49 √ó 3\n   X     Y       value\n   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1 carat carat  1     \n 2 carat depth  0.0282\n 3 carat table  0.182 \n 4 carat price  0.922 \n 5 carat x      0.975 \n 6 carat y      0.952 \n 7 carat z      0.953 \n 8 depth carat  0.0282\n 9 depth depth  1     \n10 depth table -0.296 \n# ‚Ñπ 39 more rows\n\ndiamonds %&gt;% \n   select_if(is.numeric) %&gt;%  # you can calculate cor only for numerical variables\n  cor() %&gt;%  # correlation matrix\n  data.frame() %&gt;% \n  rownames_to_column(var = \"X\") %&gt;% \n  pivot_longer(-X, names_to = \"Y\") %&gt;% \n  ggplot(aes(X, Y, fill = value)) + \n  geom_tile(color = \"black\") +\n  scale_fill_gradient2(midpoint = 0) # 0 should be white on the plot for correlations\n\n\n\nHeatmap\n\n\n\nAdvanced cases\n\neurostat::search_eurostat(\"vacancy\")\n\n# A tibble: 10 √ó 8\n   title   code  type  `last update of data` last table structure‚Ä¶¬π `data start`\n   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;                 &lt;chr&gt;                  &lt;chr&gt;       \n 1 Job va‚Ä¶ ei_l‚Ä¶ data‚Ä¶ 14.09.2023            14.09.2023             2001Q1      \n 2 Job va‚Ä¶ jvs_‚Ä¶ data‚Ä¶ 08.06.2016            08.02.2021             2008        \n 3 Job va‚Ä¶ jvs_‚Ä¶ data‚Ä¶ 17.01.2017            19.07.2021             2000        \n 4 Job va‚Ä¶ jvs_‚Ä¶ data‚Ä¶ 14.09.2023            18.08.2023             2001Q1      \n 5 Job va‚Ä¶ jvs_‚Ä¶ data‚Ä¶ 14.09.2023            18.08.2023             2001Q1      \n 6 Job va‚Ä¶ jvs_‚Ä¶ data‚Ä¶ 14.09.2023            17.02.2023             2008        \n 7 Job va‚Ä¶ jvs_‚Ä¶ data‚Ä¶ 17.01.2017            19.07.2021             2000        \n 8 Job va‚Ä¶ jvs_‚Ä¶ data‚Ä¶ 16.06.2016            08.02.2021             2001Q1      \n 9 Job va‚Ä¶ jvs_‚Ä¶ data‚Ä¶ 08.06.2016            08.02.2021             2008        \n10 Job va‚Ä¶ jvs_‚Ä¶ data‚Ä¶ 14.09.2023            18.08.2023             2001Q1      \n# ‚Ñπ abbreviated name: ¬π‚Äã`last table structure change`\n# ‚Ñπ 2 more variables: `data end` &lt;chr&gt;, values &lt;chr&gt;\n\n\nLets see what to do with time-series data. We need some data for this from Eurostat.\n\neurostat::search_eurostat(\"employment\") %&gt;% \n  filter(str_detect(title, \"quarter\"))\n\n\nunemployment_df &lt;- eurostat::get_eurostat(\"une_rt_q\")\nvacancy_df &lt;- eurostat::get_eurostat(\"ei_lmjv_q_r2\")\n\n\nunemployment_df &lt;- unemployment_df %&gt;% \n  filter(sex == \"T\" & age == \"Y15-74\") %&gt;% \n  filter(unit == \"PC_ACT\", s_adj == \"NSA\") %&gt;% \n  select(geo, time, unemployment = values)\n\n\nvacancy_df &lt;- vacancy_df %&gt;% \n  filter(s_adj == \"NSA\", nace_r2 == \"A-S\", sizeclas == \"TOTAL\") %&gt;% \n  select(geo, time, vacancy = values)\n\nLets merge the two data.frames by the time and geo column.\n\ndf &lt;- full_join(x = vacancy_df, y = unemployment_df)\n\n\ndf %&gt;% \n  filter(geo == \"DE\") %&gt;%\n  ggplot() + \n  aes(x = time, y = unemployment) +\n  geom_line()\n\n\n\nTime-series plot\n\n\n\n\ndf %&gt;% \n  filter(geo == \"DE\") %&gt;% \n  ggplot() + \n  geom_line(aes(time, unemployment)) + \n  geom_line(aes(time, vacancy))\n\n\n\nTwo variables with two geom_line command\n\n\n\nIf we need legend:\n\ndf %&gt;% \n  filter(geo == \"DE\") %&gt;% \n  pivot_longer(unemployment:vacancy) # color based on the name\n\n# A tibble: 116 √ó 4\n   geo   time       name         value\n   &lt;chr&gt; &lt;date&gt;     &lt;chr&gt;        &lt;dbl&gt;\n 1 DE    2023-04-01 unemployment   3  \n 2 DE    2023-04-01 vacancy        4.1\n 3 DE    2023-01-01 unemployment   3  \n 4 DE    2023-01-01 vacancy        4.1\n 5 DE    2022-10-01 unemployment   3  \n 6 DE    2022-10-01 vacancy        4.4\n 7 DE    2022-07-01 unemployment   3.2\n 8 DE    2022-07-01 vacancy        4.3\n 9 DE    2022-04-01 unemployment   3.1\n10 DE    2022-04-01 vacancy        4.5\n# ‚Ñπ 106 more rows\n\n\n\ndf %&gt;% \n  filter(geo == \"DE\") %&gt;% \n  pivot_longer(unemployment:vacancy) %&gt;%\n  ggplot() +\n  aes(x = time, y = value, color = name) + \n  geom_line()\n\n\n\nTwo time-series variables with pivot_longer to create legend.\n\n\n\nAlternatively, we can put the two variables on the x and y-axis and show the time as label. This is a famous figure named as the Beveridge-curve (You will learn more about in your macroeconomics course).\n\ndf %&gt;% \n  filter(geo == \"DE\") %&gt;%\n  arrange(time) %&gt;% \n  ggplot() + \n  aes(x = unemployment, y = vacancy, label = time)  +\n  geom_path() + # observation are connected in prevalence\n  geom_text()\n\n\n\nBeveridge-curve\n\n\n\nThe data input in the geom_ function can also be a funtion or lambda type funcion.\n\ndf %&gt;% \n  filter(geo == \"DE\") %&gt;%\n  arrange(time) %&gt;% \n  ggplot() + \n  aes(x = unemployment, y = vacancy, label = time)  +\n  geom_path() + # observation are connected in prevalence\n  geom_point() +\n  geom_text(data = ~ slice(., floor(seq(from = 1, to = n(), length.out = 5))))\n\n\n\nBeveridge-curve"
  },
  {
    "objectID": "content/10-content.html",
    "href": "content/10-content.html",
    "title": "Advanced ggplot2\n",
    "section": "",
    "text": "We reviewed the basic functionality of the main R visualization package. This is the ggplot2. In this chapter, we will focus on applying this in more complex situations and consider which visualization solutions are appropriate in different cases.\nlibrary(tidyverse)"
  },
  {
    "objectID": "content/10-content.html#inflation-distribution",
    "href": "content/10-content.html#inflation-distribution",
    "title": "Advanced ggplot2\n",
    "section": "Inflation distribution",
    "text": "Inflation distribution\nOriginal idea and article: The Economist.\n\nread.csv(\"https://www.ksh.hu/stadat_files/ara/hu/ara0043.csv\", sep = \";\", dec = \",\", encoding = \"UTF-8\", skip = 1, fileEncoding = \"ISO-8859-1\", na.strings = \"..\") %&gt;% \n  tibble() %&gt;% \n  rename_all(str_replace_all, pattern = \"[.]\", replacement = \" \") %&gt;% \n  slice(-1) %&gt;% \n  filter(!cumany(`Megnevez√©s` == \"\")) %&gt;% \n  filter(`K√≥dsz√°m` %in% 1:15) %&gt;% \n  select(`Megnevez√©s`, ends_with(\"S√∫ly\"), last_col() - 3) %&gt;% \n  select(1, last_col() - 1:0) %&gt;% \n  mutate(\n    across(2:last_col(), ~ as.numeric(str_replace(., \",\", \".\"))),\n    across(3, ~ . / 100 - 1)\n  ) %&gt;% \n  set_names(\"coicop\", \"w\", \"inflation\") %&gt;% \n  arrange(inflation) %&gt;% \n  mutate(\n    coicop = fct_inorder(coicop),\n    y_l = cumsum(w) - w,\n    y_h = cumsum(w),\n    y_m = (y_l + y_h) / 2\n  ) %&gt;% \n  rowwise() %&gt;% \n  mutate(\n    x_l = min(inflation, 0),\n    x_h = max(inflation, 0)\n  ) %&gt;% \n  ungroup() %&gt;% \n  ggplot() +\n  geom_rect(aes(xmin = x_l, xmax = x_h, ymin = y_l, ymax = y_h, fill = inflation), color = \"black\", show.legend = FALSE) +\n  ggrepel::geom_text_repel(data = ~ sample_n(., 4), aes(y = y_m, x = x_h, label = str_wrap(coicop, 20)), size = 5, nudge_x = .2, arrow = arrow(length = unit(0.02, \"npc\")),\n                           box.padding = 1, label.r = .6) +\n  geom_vline(xintercept = 0) + \n  labs(\n    x = \"Infl√°ci√≥\",\n    y = \"Coicop besorol√°s\",\n    title = \"Infl√°ci√≥ alakul√°sa term√©kcsoportok szerint\",\n    subtitle = \"2023. szeptember\",\n    caption = \"Forr√°s: KSH\"\n  )"
  },
  {
    "objectID": "content/11-content.html",
    "href": "content/11-content.html",
    "title": "Causality",
    "section": "",
    "text": "Statistics and data analysis are widely applied in every field of science, as there is a need to objectively perceive things and either support or reject theories. Therefore, it is often formulated that statistics is the grammar of science. However, each field of science has its specific characteristics. In economics and other disciplines, the focus is usually on determining whether a demonstrated relationship can be considered causal or not.\nCausality is important in economics because it allows us to understand the relationships between different economic variables and determine the true impact of various factors on economic outcomes. By establishing causal relationships, economists can make informed policy recommendations, predict the consequences of different actions, and evaluate the effectiveness of interventions."
  },
  {
    "objectID": "content/11-content.html#why",
    "href": "content/11-content.html#why",
    "title": "Causality",
    "section": "Why?",
    "text": "Why?\n\nPolicy-making: Causal relationships help policymakers identify the effective interventions to achieve desired economic outcomes. By understanding the causal impact of different policies, governments can make informed decisions about taxation, regulation, and public spending to promote economic growth, reduce inequality, or address market failures.\nCounterfactual Analysis: Causality allows economists to estimate what would have happened in the absence of a particular event or policy intervention. This counterfactual analysis helps in evaluating the effectiveness of policies and estimating their true impact on economic outcomes.\nUnderstanding Market Dynamics: Causal relationships help economists understand how changes in one economic variable affect others. For example, understanding the causal relationship between interest rates and investment can provide insights into how changes in monetary policy impact economic activity.\nEvaluating Economic Theories: Causality is crucial for testing and validating economic theories. By examining causal relationships, economists can assess the validity of economic models and theories, refine them, and develop a deeper understanding of how the economy functions.\nPredictive Power: Causal relationships allow economists to make predictions about future economic outcomes. By understanding the causal mechanisms at play, economists can forecast the impact of changes in economic variables and provide valuable insights for businesses, policymakers, and individuals."
  },
  {
    "objectID": "content/11-content.html#some-example-methods",
    "href": "content/11-content.html#some-example-methods",
    "title": "Causality",
    "section": "Some example methods",
    "text": "Some example methods\n\n\nGranger Causality: Many questions may arise where an acceptable theory is that the cause precedes the consequence. One such case is a bar fight, where the activity that triggered the fight was likely the cause leading to the fight. Consequently, models based on Granger causality have become widespread, such as vector autoregressive models in the case of time series.\n\n\n\n\n\n\n\nCaution\n\n\n\nIt should be noted that this assertion does not necessarily hold true. Let us imagine the scenario where the sultan wakes up every day and commands the sun to rise, and indeed the sun rises. It is worth contemplating whether it was truly the word of the sultan that caused the sun to rise.\n\n\n\n\nRandomized Controlled Trials (RCTs): RCTs involve randomly assigning individuals or groups to different treatment conditions and measuring the outcomes. By randomly assigning treatments, RCTs help establish a causal relationship between the treatment and the outcome, as any differences observed can be attributed to the treatment itself.\n\n\n\n\n\n\n\nCaution\n\n\n\nRCTs are often considered the gold standard for establishing causality but most of the time may not be feasible or ethical in economic contexts.\n\n\n\n\nInstrumental Variables (IV): IV analysis is used when there is endogeneity or potential confounding between the treatment variable and the outcome variable. An instrumental variable is a variable that is correlated with the treatment variable but not directly with the outcome. By using an instrumental variable, researchers can estimate the causal effect of the treatment variable on the outcome while addressing endogeneity.\n\n\n\n\n\n\n\nWarning\n\n\n\nThe application of instrumental variables is often the most convincing solution to validate a controversial economic theory (for example, whether institutions cause well-being or whether well-being leads to high-quality institutions). However, finding a good instrument is very challenging, and providing substantial evidence requires extensive and in-depth knowledge in the specific subject area.\n\n\n\nDifference-in-Differences (DiD) Analysis: DiD analysis compares the changes in outcomes between a treatment group and a control group before and after a specific intervention or treatment. By comparing the differences in changes over time, DiD analysis helps identify the causal effect of the treatment by accounting for time-varying factors that affect both groups. One frequent application area is the event study design, which you will encounter later on.\nPropensity Score Matching (PSM): PSM is used to address selection bias in observational studies. It involves estimating the propensity score, which is the probability of receiving the treatment given a set of observed characteristics. By matching individuals with similar propensity scores, researchers can create a control group that closely resembles the treatment group, allowing for a more accurate estimation of the causal effect.\nRegression Discontinuity Design (RDD): RDD is used when there is a clear cutoff or threshold that determines whether an individual receives a treatment or not. By comparing individuals just above and below the cutoff, RDD helps estimate the causal effect of the treatment by exploiting the discontinuity in treatment assignment (Lee and Lemieux 2010). This is a widely adopted solution in educational policy matters."
  },
  {
    "objectID": "content/11-content.html#the-basics",
    "href": "content/11-content.html#the-basics",
    "title": "Causality",
    "section": "The basics",
    "text": "The basics\nWhen discussing causality, it is neccessary that we possess a comprehensive understanding of some fundamental definitions.\n\nTreatment: In the context of causal analysis, treatment refers to an intervention or action that is applied to a group or individual with the intention of bringing about a change in their outcomes or behavior. It can be a medical treatment, a policy intervention, an educational program, or any other deliberate action aimed at influencing a specific outcome of interest.\nControl Group: A control group is a group of individuals or units that do not receive the treatment or intervention being studied. The purpose of the control group is to provide a baseline or comparison against which the effects of the treatment can be measured. By comparing the outcomes of the treated group with the outcomes of the control group, researchers can assess the causal impact of the treatment.\nTreated Group: The treated group refers to the group of individuals or units that receive the treatment or intervention being studied. This group is exposed to the treatment and its outcomes are compared to those of the control group. The treated group allows researchers to evaluate the effects of the treatment and determine its effectiveness in achieving the desired outcomes.\nOutcome: An outcome refers to the result or consequence of a treatment or intervention. It is the variable of interest that researchers measure or observe to assess the impact of the treatment. Outcomes can be various types, such as health outcomes, educational outcomes, economic outcomes, or behavioral outcomes. The comparison of outcomes between the treated and control groups helps determine the causal effect of the treatment.\nITE (Individual Treatment Effect): \\[\nITE = Y_{1i} - Y_{0i}\n\\] where \\(Y_{1i}\\) represents the outcome for individual \\(i\\) when they receive the treatment, and \\(Y_{0i}\\) represents the outcome for individual \\(i\\) when they do not receive the treatment.\n\n\n\n\n\n\n\nCaution\n\n\n\nMost of the time, it is impossible to observe the ITE values. One will be treated or not, but it is a rare case, when both values are observed.\n\n\n\nATE (Average Treatment Effect): \\[\nATE = \\frac{1}{N} \\sum_{i=1}^{N} (Y_{1i} - Y_{0i})\n\\] where \\(N\\) is the total number of individuals in the population.\n\n\n\n\n\n\n\nTip\n\n\n\n99% of the time, we care about ATE.\n\n\n\nATET (Average Treatment Effect on the Treated): \\[\nATET = \\frac{1}{N_{T}} \\sum_{i=1}^{N_{T}} (Y_{1i} - Y_{0i})\n\\] where \\(N_{T}\\) is the number of individuals who receive the treatment."
  },
  {
    "objectID": "content/11-content.html#matching",
    "href": "content/11-content.html#matching",
    "title": "Causality",
    "section": "Matching",
    "text": "Matching\nMatching is a technique used in causal analysis to create comparable groups by pairing or matching treated and control units based on their observed characteristics. The goal of matching is to reduce the potential bias caused by confounding variables, which are factors that may influence both the treatment assignment and the outcome of interest.\nMatching involves identifying individuals or units in the treated group and finding similar individuals or units in the control group based on a set of pre-treatment characteristics or covariates. These covariates can include demographic information, socioeconomic status, health conditions, or any other relevant factors that may affect both the treatment assignment and the outcome.\nMatching can be particularly useful in situations where there are limited control units available. It allows for a more rigorous comparison between the treated and control groups, helping to isolate the causal impact of the treatment and improve the validity of the analysis. Matching methods can range from simple techniques like nearest neighbor matching to more advanced methods like propensity score matching or genetic matching, depending on the complexity of the data and research question.\n\nearnings_df &lt;- read.csv(\"../data/earnings.csv\", sep = \";\") %&gt;% \n  tibble() %&gt;% \n  transmute(\n    sex = ifelse(nem == 1, \"male\", \"female\"),\n    age = eletkor,\n    school = case_when(\n      iskvegz == 1 ~ \"primary\",\n      iskvegz == 2 ~ \"secondary\",\n      iskvegz == 3 ~ \"tertiary\",\n    ),\n    position = case_when(\n      munkakor == 1 ~ \"Highest leader\",\n      munkakor == 2 ~ \"Department/institute leader\",\n      munkakor == 3 ~ \"Other (economic, legal, technical, etc.) leader\",\n      munkakor == 4 ~ \"University/college teacher\",\n      munkakor == 5 ~ \"Highly trained clerk\",\n      munkakor == 6 ~ \"Clerk/secretary\",\n      munkakor == 7 ~ \"Trained/assistant worker\"\n    ),\n    earning = kereset\n  )\n\n\nearnings_df\n\n# A tibble: 647 √ó 5\n   sex      age school    position                    earning\n   &lt;chr&gt;  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;                         &lt;dbl&gt;\n 1 male      50 tertiary  Highest leader                2411.\n 2 female    51 tertiary  Highest leader                1073.\n 3 male      53 tertiary  Highest leader                1990.\n 4 female    64 tertiary  Highest leader                1609.\n 5 male      32 secondary Department/institute leader    706.\n 6 female    33 tertiary  Department/institute leader    994.\n 7 female    34 secondary Department/institute leader    632.\n 8 male      35 secondary Department/institute leader    512.\n 9 male      35 tertiary  Department/institute leader    987.\n10 female    35 tertiary  Department/institute leader    880.\n# ‚Ñπ 637 more rows\n\n\nOLS\n\nearnings_df %&gt;% \n  lm(formula = earning ~ sex) %&gt;% \n  broom::tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    493.       13.7     36.0  1.13e-156\n2 sexmale         46.7      22.1      2.11 3.52e-  2\n\n\n\nearnings_df %&gt;% \n  lm(formula = earning ~ .) %&gt;% \n  broom::tidy()\n\n# A tibble: 11 √ó 5\n   term                                    estimate std.error statistic  p.value\n   &lt;chr&gt;                                      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                               284.      39.6       7.18  2.02e-12\n 2 sexmale                                    32.0     15.4       2.08  3.84e- 2\n 3 age                                         2.04     0.750     2.71  6.81e- 3\n 4 schoolsecondary                             8.71    26.2       0.333 7.39e- 1\n 5 schooltertiary                             44.4     30.7       1.45  1.48e- 1\n 6 positionDepartment/institute leader       497.      30.1      16.5   4.63e-51\n 7 positionHighest leader                   1315.      95.7      13.7   8.46e-38\n 8 positionHighly trained clerk               66.8     27.2       2.45  1.45e- 2\n 9 positionOther (economic, legal, techni‚Ä¶   515.      44.7      11.5   4.99e-28\n10 positionTrained/assistant worker          -88.3     51.0      -1.73  8.38e- 2\n11 positionUniversity/college teacher         12.7     25.4       0.500 6.17e- 1\n\n\nExact matching\n\nLets calculate the difference in each possible group the average earnings.\n\n\nearnings_df %&gt;% \n  group_by(sex, age, school, position) %&gt;%\n  summarise(\n    n = n(),\n    earning = mean(earning)\n  )\n\n# A tibble: 351 √ó 6\n# Groups:   sex, age, school [203]\n   sex      age school    position                     n earning\n   &lt;chr&gt;  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;                    &lt;int&gt;   &lt;dbl&gt;\n 1 female    24 primary   Clerk/secretary              1    537.\n 2 female    24 tertiary  Highly trained clerk         2    406.\n 3 female    25 primary   Clerk/secretary              1    342.\n 4 female    25 secondary Highly trained clerk         2    450.\n 5 female    26 primary   Clerk/secretary              2    373.\n 6 female    26 tertiary  Highly trained clerk         1    583.\n 7 female    27 primary   Clerk/secretary              1    367.\n 8 female    27 secondary Highly trained clerk         1    426 \n 9 female    27 secondary Trained/assistant worker     1    327.\n10 female    27 tertiary  Highly trained clerk         2    384.\n# ‚Ñπ 341 more rows\n\n\n\nCompare the difference between the treatment and the related control groups.\n\n\nearnings_df %&gt;% \n  group_by(sex, age, school, position) %&gt;%\n  summarise(\n    n = n(),\n    earning = mean(earning)\n  ) %&gt;% \n  ungroup() %&gt;% \n  pivot_wider(names_from = sex, values_from = c(n, earning)) %&gt;% \n  mutate(\n    d = earning_male - earning_female\n  ) %&gt;% \n  drop_na()\n\n# A tibble: 79 √ó 8\n     age school    position   n_female n_male earning_female earning_male      d\n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;         &lt;int&gt;  &lt;int&gt;          &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n 1    26 primary   Clerk/sec‚Ä¶        2      1           373.         587  214.  \n 2    27 primary   Clerk/sec‚Ä¶        1      1           367.         342. -24.9 \n 3    27 secondary Highly tr‚Ä¶        1      2           426          411  -15   \n 4    27 tertiary  Universit‚Ä¶        1      2           227.         279.  51.2 \n 5    28 secondary Clerk/sec‚Ä¶        1      1           372.         381.   9.10\n 6    29 secondary Clerk/sec‚Ä¶        1      1           311.         575  264.  \n 7    31 secondary Highly tr‚Ä¶        1      1           338.         408.  70.8 \n 8    32 primary   Clerk/sec‚Ä¶        1      1           250          486. 236.  \n 9    34 secondary Clerk/sec‚Ä¶        1      1           321.         376.  54.7 \n10    34 tertiary  Highly tr‚Ä¶        2      1           538.         614.  75.4 \n# ‚Ñπ 69 more rows\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is important to note that an exact match may only be found in a limited number of instances. This is primarily due to the presence of a numerical covariate with a multitude of potential values.\n\n\nCoarsened exact matching\nWe lump qualitative variables (e.g., country groups, young/old subjects) and we divide quantitative variables into intervals (e.g., age groups).\n\ncoarsened_matching_df &lt;- earnings_df %&gt;% \n  mutate(\n    age_group = cut(age, breaks = c(0, 30, 40, 50, Inf))\n  ) %&gt;% \n  group_by(sex, age_group, school, position) %&gt;% \n  summarise(\n    n = n(),\n    earning = mean(earning)\n  ) %&gt;% \n  ungroup() %&gt;% \n  pivot_wider(names_from = sex, values_from = c(n, earning)) %&gt;% \n  mutate(\n    d = earning_male - earning_female\n  )\n\ncoarsened_matching_df\n\n# A tibble: 52 √ó 8\n   age_group school   position n_female n_male earning_female earning_male     d\n   &lt;fct&gt;     &lt;chr&gt;    &lt;chr&gt;       &lt;int&gt;  &lt;int&gt;          &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 (0,30]    primary  Clerk/s‚Ä¶        7      3           401.         422.  20.6\n 2 (0,30]    primary  Highly ‚Ä¶        1      2           424.         491.  66.8\n 3 (0,30]    seconda‚Ä¶ Clerk/s‚Ä¶        2      4           341.         400.  58.3\n 4 (0,30]    seconda‚Ä¶ Highly ‚Ä¶        7      2           421.         411  -10.3\n 5 (0,30]    seconda‚Ä¶ Trained‚Ä¶        1     NA           327.          NA   NA  \n 6 (0,30]    seconda‚Ä¶ Univers‚Ä¶        1     NA           339.          NA   NA  \n 7 (0,30]    tertiary Highly ‚Ä¶       10     NA           452.          NA   NA  \n 8 (0,30]    tertiary Univers‚Ä¶        1      5           227.         320.  92.6\n 9 (30,40]   primary  Clerk/s‚Ä¶        9      5           394.         407.  12.3\n10 (30,40]   seconda‚Ä¶ Clerk/s‚Ä¶       15      5           390.         434.  43.3\n# ‚Ñπ 42 more rows\n\n\n\ncoarsened_matching_df %&gt;% \n  drop_na() %&gt;% \n  summarise(\n    s = sum(n_female, na.rm = TRUE) + sum(n_male, na.rm = TRUE),\n    ate = weighted.mean(\n      d, w = n_female + n_male\n    ),\n    atet = weighted.mean(\n      d, w = n_male\n    ),\n  )\n\n# A tibble: 1 √ó 3\n      s   ate  atet\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   621  28.0  29.6\n\n\nP-score matching\n\nEstimating the Propensity Score: A statistical model, such as logistic regression, is used to estimate the propensity score for each individual or unit in the sample. The model predicts the probability of receiving the treatment based on a set of pre-treatment covariates.\n\n\naux_logit &lt;- earnings_df %&gt;% \n  mutate(\n    sex = sex == \"male\"\n  ) %&gt;% \n  glm(formula = sex ~ age + school + position, family = binomial(link = \"logit\")) %&gt;% \n  broom::augment(type.predict = \"response\")\n\n\nMatching Based on Propensity Score: Treated and control units with similar propensity scores are matched or paired. The goal is to create pairs of treated and control units that have similar probabilities of receiving the treatment, thereby reducing the potential bias caused by confounding variables.\n\n\nearnings_df %&gt;% \n  mutate(\n    propensity_score = aux_logit$.fitted,\n    earning_control = map2_dbl(sex, propensity_score, ~ {\n      aux_logit %&gt;% \n        bind_cols(select(earnings_df, earning_control = earning)) %&gt;%\n        filter(sex != (.x == \"male\")) %&gt;% \n        slice_min(abs(.fitted - .y))%&gt;% \n        summarise(earning_control = mean(earning_control, na.rm = TRUE)) %&gt;% \n        pull(earning_control)\n    })\n  ) %&gt;% \n  summarise(\n    ATE = mean((earning - earning_control) * ifelse(sex == \"male\", 1, -1))\n  )\n\n# A tibble: 1 √ó 1\n    ATE\n  &lt;dbl&gt;\n1  22.7\n\n\nLimitations\n\n\n\n\n\n\nCaution\n\n\n\nIt is important to note that p-score matching relies on the assumption of unconfoundedness, which assumes that all relevant confounding variables are included in the propensity score model. Additionally, the quality of the matching depends on the accuracy of the propensity score estimation and the availability of a sufficient number of control units with similar propensity scores."
  },
  {
    "objectID": "content/11-content.html#iv",
    "href": "content/11-content.html#iv",
    "title": "Causality",
    "section": "IV",
    "text": "IV\nIn econometrics, exogenous variation refers to changes or variations in an explanatory variable that are not correlated with the error term in a regression model. In simpler terms, it is a change in the explanatory variable that is not driven by any factors that might influence the outcome variable, except through the explanatory variable itself. This concept is crucial for causal inference since it ensures that the observed relationship between the explanatory variable and the outcome variable is not confounded by omitted variables.\nAn exogenous variation can serve as an instrumental variable (IV), which helps identify causal relationships when the main explanatory variable of interest is endogenously determined and correlated with the error term.\nFinding such a case is particularly challenging, and proving that the instrument truly captures this variation primarily requires not statistical expertise.\nOne of the most famous paper is The Colonial Origins of Comparative Development by Acemoglu, Johnson, and Robinson (2001).\nThe idea is to examine whether institutions or development cause the other by analyzing the mortality rate of the settlers. This determines what kind of institutions they brought with them, which has an impact on today‚Äôs institutions. What conditions need to be met in order to accept that institutions really cause development?\n\nIn the case of lower mortality, institutional development should be higher.\nThe institutional development resulting solely from the difference in mortality should lead to a higher GDP.\nThe difference in mortality should not influence GDP in any other way.\n\n\nlibrary(dagitty)\nlibrary(ggdag)\ndagitty('dag {\n\"Modern GDP\" [outcome,pos=\"-0.081,-0.630\"]\n\"Modern insitituions\" [adjusted,pos=\"-0.260,0.173\"]\n\"Mortality rate\" [exposure,pos=\"-1.132,-0.244\"]\n\"Early institutions\" [adjusted,pos=\"-1.530,0.153\"]\n\"Modern GDP\" &lt;-&gt; \"Modern insitituions\"\n\"Mortality rate\" &lt;-&gt; \"Modern GDP\"\n\"Mortality rate\" -&gt; \"Early institutions\"\n\"Early institutions\" -&gt; \"Modern insitituions\"\n}\n') %&gt;% \n  tidy_dagitty() %&gt;% \n  ggdag_status(text_col = \"black\", edge_type = \"link_arc\") +\n  theme_dag() +\n  labs(color = \"\")"
  },
  {
    "objectID": "content/12-content.html",
    "href": "content/12-content.html",
    "title": "Web scraping",
    "section": "",
    "text": "Nowadays, we manage a very huge part of our life online. This has an important side-effect: we can collect enormous data from the web for our researches. You can access data about shops, blogs, social media etc. The target of this chapter is to give a brief introduction how you can collect this data effectively. We will need a new package for this purpose: rvest\nlibrary(tidyverse)\nlibrary(rvest)\nWe will scrape the data from hasznaltauto, which is the online second hand car market of Hungary. Lets navigate to the page in our browser and lets click on search.\nwww.hasznaltauto.hu/\nNow we have to copy and paste the new url from the browser to Rstudio. This will be the first link we want to visit while scraping. Lets assign this url as url in R.\nurl &lt;- \"https://www.hasznaltauto.hu/talalatilista/PCOG2VGRR3RDAD [...]\" # long url\nClick on the search button.\nThe next step is load the website into your R session. This can be done by the read_html function from the rvest package.\npage &lt;- read_html(url)\n\npage\n\n{html_document}\n&lt;html lang=\"hu-HU\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body&gt;\\n    &lt;script type=\"text/javascript\"&gt;window.dataLayer = window.data ..."
  },
  {
    "objectID": "content/12-content.html#naviagtion-on-the-page",
    "href": "content/12-content.html#naviagtion-on-the-page",
    "title": "Web scraping",
    "section": "Naviagtion on the page",
    "text": "Naviagtion on the page\nNow we can see the webpage as html codes in RStudio. This is the same what you get if you open developer view in your browser.\n\n\n\n\nDeveloper view in browser.\n\n\n\nIn the developer view, you can find the information that is relevant to you and select it using the html_nodes function. Alternatively, however, there is a simpler method. Add Selector Gadget to your browser. This add-on helps you find the ID of an item by clicking on it. This way, you can navigate without having web development skills. We can easily install this add-on in chrome and edge, just search for its name and the first hit should be this.\n\n\n\n\nAdd Selector Gadget to your browser.\n\n\n\nYou can activate the add-on from the menu of your browser.\nFirst, find the IDs for the car ads. To do this, first select the name of a specific car and then mark everything you do not want to include. The target is to make every car ads title to yellow or green, but nothing else should be green.\n\n\n\n\nUsing Selector Gadget to find IDs for the car ads.\n\n\n\nIf you have the ID you are looking for, put it in the html_nodes function. The code above selects the ad titles from the page.\n\nmy_node &lt;- page %&gt;% \n  html_nodes(\".cim-kontener a\")\n\nmy_node\n\n{xml_nodeset (130)}\n [1] &lt;a class=\"btn hagomb hagomb-nagy parkoloBtn parkolo-nolabel\" data-hirkod ...\n [2] &lt;a class=\"\" href=\"https://www.hasznaltauto.hu/szemelyauto/suzuki/vitara/ ...\n [3] &lt;a class=\"\" href=\"https://www.hasznaltauto.hu/szemelyauto/suzuki/vitara/ ...\n [4] &lt;a class=\"btn hagomb hagomb-nagy parkoloBtn parkolo-nolabel\" data-hirkod ...\n [5] &lt;a class=\"\" href=\"https://www.hasznaltauto.hu/szemelyauto/opel/astra_l/o ...\n [6] &lt;a class=\"\" href=\"https://www.hasznaltauto.hu/szemelyauto/opel/astra_l/o ...\n [7] &lt;a class=\"btn hagomb hagomb-nagy parkoloBtn parkolo-nolabel\" data-hirkod ...\n [8] &lt;a class=\"\" href=\"https://www.hasznaltauto.hu/szemelyauto/nissan/x-trail ...\n [9] &lt;a class=\"\" href=\"https://www.hasznaltauto.hu/szemelyauto/nissan/x-trail ...\n[10] &lt;a class=\"btn hagomb hagomb-nagy parkoloBtn parkolo-nolabel\" data-hirkod ...\n[11] &lt;a class=\"\" href=\"https://www.hasznaltauto.hu/szemelyauto/nissan/qashqai ...\n[12] &lt;a class=\"\" href=\"https://www.hasznaltauto.hu/szemelyauto/nissan/qashqai ...\n[13] &lt;a class=\"btn hagomb hagomb-nagy parkoloBtn parkolo-nolabel\" data-hirkod ...\n[14] &lt;a class=\"\" href=\"https://www.hasznaltauto.hu/szemelyauto/suzuki/s-cross ...\n[15] &lt;a class=\"\" href=\"https://www.hasznaltauto.hu/szemelyauto/suzuki/s-cross ...\n[16] &lt;a class=\"btn hagomb hagomb-nagy parkoloBtn parkolo-nolabel\" data-hirkod ...\n[17] &lt;a class=\"\" href=\"https://www.hasznaltauto.hu/szemelyauto/peugeot/5008/p ...\n[18] &lt;a class=\"\" href=\"https://www.hasznaltauto.hu/szemelyauto/peugeot/5008/p ...\n[19] &lt;a class=\"btn hagomb hagomb-nagy parkoloBtn parkolo-nolabel\" data-hirkod ...\n[20] &lt;a class=\"\" href=\"https://www.hasznaltauto.hu/szemelyauto/peugeot/3008/p ...\n...\n\n\nBut this is still an html_code.If we want to keep the text of the element, we use the html_text function, but if we are interested in the url it points to, we use thehtml_attr function. In the latter case, it is always necessary to specify the href element, ie which web page you are referring to.\n\nname_of_the_car &lt;- my_node %&gt;% \n  html_text()\n\nname_of_the_car\n\n  [1] \"P\"                                                                                                                                       \n  [2] \"SUZUKI VITARA 1.4 Hybrid GL+ ak√°r: 2.1 milli√≥ √°r-el≈ëny+0% THM!\"                                                                          \n  [3] \"SUZUKI VITARA 1.4 Hybrid GL+ ak√°r: 2.1 milli√≥ √°r-el≈ëny+0% THM!\"                                                                          \n  [4] \"P\"                                                                                                                                       \n  [5] \"OPEL ASTRA L 1.2 T Business Edition\"                                                                                                     \n  [6] \"OPEL ASTRA L 1.2 T Business Edition\"                                                                                                     \n  [7] \"P\"                                                                                                                                       \n  [8] \"NISSAN X-TRAIL 1.5 VC-T Mild Hybrid Xtronic Acenta FIX 0%THM !\"                                                                          \n  [9] \"NISSAN X-TRAIL 1.5 VC-T Mild Hybrid Xtronic Acenta FIX 0%THM !\"                                                                          \n [10] \"P\"                                                                                                                                       \n [11] \"NISSAN QASHQAI 1.3 DIG-T Mild Hybrid Tekna X-Tronic FIX 0%THM ! K√©szletr≈ël!\"                                                             \n [12] \"NISSAN QASHQAI 1.3 DIG-T Mild Hybrid Tekna X-Tronic FIX 0%THM ! K√©szletr≈ël!\"                                                             \n [13] \"P\"                                                                                                                                       \n [14] \"SUZUKI S-CROSS 1.4 Hybrid GL+ ak√°r: 2.1 milli√≥s √°rel≈ëny + 0% THM!\"                                                                       \n [15] \"SUZUKI S-CROSS 1.4 Hybrid GL+ ak√°r: 2.1 milli√≥s √°rel≈ëny + 0% THM!\"                                                                       \n [16] \"P\"                                                                                                                                       \n [17] \"PEUGEOT 5008 1.2 PureTech Active Pack (7 szem√©lyes ) AZONNAL ELVIHET≈ê! √âV V√âG√âIG AKCI√ìS 3.3% THM-MEL!\"                                   \n [18] \"PEUGEOT 5008 1.2 PureTech Active Pack (7 szem√©lyes ) AZONNAL ELVIHET≈ê! √âV V√âG√âIG AKCI√ìS 3.3% THM-MEL!\"                                   \n [19] \"P\"                                                                                                                                       \n [20] \"PEUGEOT 3008 1.2 PureTech Active Pack\"                                                                                                   \n [21] \"PEUGEOT 3008 1.2 PureTech Active Pack\"                                                                                                   \n [22] \"P\"                                                                                                                                       \n [23] \"MAZDA 6 Sportkombi G194 20th Anniversary - K√âSZLETEN\"                                                                                    \n [24] \"MAZDA 6 Sportkombi G194 20th Anniversary - K√âSZLETEN\"                                                                                    \n [25] \"P\"                                                                                                                                       \n [26] \"OPEL GRANDLAND 1.5 CDTI Business Edition (Automata) hamarosan meg√©rkezik\"                                                                \n [27] \"OPEL GRANDLAND 1.5 CDTI Business Edition (Automata) hamarosan meg√©rkezik\"                                                                \n [28] \"P\"                                                                                                                                       \n [29] \"KIA SORENTO 1.6 T-GDI PHEV Platinum 4WD (Automata) PLUG IN HYBRID! √ñSSZKER√âK! T√ñBB SZ√çNBEN AZONNAL!\"                                     \n [30] \"KIA SORENTO 1.6 T-GDI PHEV Platinum 4WD (Automata) PLUG IN HYBRID! √ñSSZKER√âK! T√ñBB SZ√çNBEN AZONNAL!\"                                     \n [31] \"P\"                                                                                                                                       \n [32] \"PEUGEOT 308 SW 1.6 PureTech PHEV GT EAT8 AZONNAL ELVIHET≈ê!\"                                                                              \n [33] \"PEUGEOT 308 SW 1.6 PureTech PHEV GT EAT8 AZONNAL ELVIHET≈ê!\"                                                                              \n [34] \"P\"                                                                                                                                       \n [35] \"PEUGEOT 208 1.2 PureTech Active Pack EAT8\"                                                                                               \n [36] \"PEUGEOT 208 1.2 PureTech Active Pack EAT8\"                                                                                               \n [37] \"P\"                                                                                                                                       \n [38] \"PEUGEOT RIFTER e-Rifter 50kWh Active Pack KIZ√ÅR√ìLAG GY√ÅRT√ÅSRENDEL√âSB≈êL!\"                                                                 \n [39] \"PEUGEOT RIFTER e-Rifter 50kWh Active Pack KIZ√ÅR√ìLAG GY√ÅRT√ÅSRENDEL√âSB≈êL!\"                                                                 \n [40] \"P\"                                                                                                                                       \n [41] \"SSANGYONG REXTON 2.2 e-XDI Premium 4WD (Automata) (7 szem√©lyes ) TESZTAUT√ì!\"                                                             \n [42] \"SSANGYONG REXTON 2.2 e-XDI Premium 4WD (Automata) (7 szem√©lyes ) TESZTAUT√ì!\"                                                             \n [43] \"P\"                                                                                                                                       \n [44] \"OPEL CORSA F 1.2 Edition Akci√≥s modell kimagasl√≥ kedvezm√©nnyel!!!\"                                                                       \n [45] \"OPEL CORSA F 1.2 Edition Akci√≥s modell kimagasl√≥ kedvezm√©nnyel!!!\"                                                                       \n [46] \"P\"                                                                                                                                       \n [47] \"KIA CEE'D Ceed 1.0 T-GDI Silver Magyar. Szervizk√∂nyv. Kamera. √Åf√°s\"                                                                      \n [48] \"KIA CEE'D Ceed 1.0 T-GDI Silver Magyar. Szervizk√∂nyv. Kamera. √Åf√°s\"                                                                      \n [49] \"P\"                                                                                                                                       \n [50] \"OPEL CROSSLAND 1.2 T Edition Kiemelt akci√≥s modell !!!\"                                                                                  \n [51] \"OPEL CROSSLAND 1.2 T Edition Kiemelt akci√≥s modell !!!\"                                                                                  \n [52] \"P\"                                                                                                                                       \n [53] \"SSANGYONG KORANDO 1.5 Turbo GDI Clever K√úL√ñNB√ñZ≈ê FELSZERELTS√âGEK SZINEK AZONNAL RAKT√ÅRR√ìL!\"                                              \n [54] \"SSANGYONG KORANDO 1.5 Turbo GDI Clever K√úL√ñNB√ñZ≈ê FELSZERELTS√âGEK SZINEK AZONNAL RAKT√ÅRR√ìL!\"                                              \n [55] \"P\"                                                                                                                                       \n [56] \"HYUNDAI TUCSON 1.6 T-GDI hybrid Executive Plus (Automata) K√©szletr≈ël azonnal! 230LE Hybrid!\"                                             \n [57] \"HYUNDAI TUCSON 1.6 T-GDI hybrid Executive Plus (Automata) K√©szletr≈ël azonnal! 230LE Hybrid!\"                                             \n [58] \"P\"                                                                                                                                       \n [59] \"MERCEDES-BENZ C 220 T CDI BlueEFFICIENCY Avantgarde AMG LINE!!!HARMANKARDON!!GY√ñNY√ñR≈∞ √ÅLLAPOT!!\"                                         \n [60] \"MERCEDES-BENZ C 220 T CDI BlueEFFICIENCY Avantgarde AMG LINE!!!HARMANKARDON!!GY√ñNY√ñR≈∞ √ÅLLAPOT!!\"                                         \n [61] \"P\"                                                                                                                                       \n [62] \"OPEL MOKKA 1.2 T Edition Kiemelt akci√≥s modell utol√©rhetetlen √°ron!!!Egyedi l√≠zingakci√≥!!!\"                                              \n [63] \"OPEL MOKKA 1.2 T Edition Kiemelt akci√≥s modell utol√©rhetetlen √°ron!!!Egyedi l√≠zingakci√≥!!!\"                                              \n [64] \"P\"                                                                                                                                       \n [65] \"PEUGEOT 408 1.2 PureTech Allure Pack EAT8 TESZTAUT√ì! 2024 FEBRU√ÅRI √ÅTV√âTELLEL!\"                                                          \n [66] \"PEUGEOT 408 1.2 PureTech Allure Pack EAT8 TESZTAUT√ì! 2024 FEBRU√ÅRI √ÅTV√âTELLEL!\"                                                          \n [67] \"P\"                                                                                                                                       \n [68] \"SSANGYONG TIVOLI 1.5 GDi-T Style √öJ FORMA. BEV√ÅLT TARTALOM RENDELJE MEG N√ÅLUNK AZ √öJ TIVOLIT!\"                                           \n [69] \"SSANGYONG TIVOLI 1.5 GDi-T Style √öJ FORMA. BEV√ÅLT TARTALOM RENDELJE MEG N√ÅLUNK AZ √öJ TIVOLIT!\"                                           \n [70] \"P\"                                                                                                                                       \n [71] \"SSANGYONG TORRES 1.5 Turbo GDI Club MEG√âRKEZETT AZ √öJ TORRES PR√ìB√ÅLJA KI √âS RENDELJE MEG N√ÅLUNK!\"                                        \n [72] \"SSANGYONG TORRES 1.5 Turbo GDI Club MEG√âRKEZETT AZ √öJ TORRES PR√ìB√ÅLJA KI √âS RENDELJE MEG N√ÅLUNK!\"                                        \n [73] \"P\"                                                                                                                                       \n [74] \"AUDI A6 ALLROAD 55 TFSI quattro S-tronic EGYEDI SZ√çN. BANG&OLUFSEN HIFI !!\"                                                              \n [75] \"AUDI A6 ALLROAD 55 TFSI quattro S-tronic EGYEDI SZ√çN. BANG&OLUFSEN HIFI !!\"                                                              \n [76] \"P\"                                                                                                                                       \n [77] \"MAZDA CX-5 2.5i e-Skyactiv Homura AWD (Automata) NAVI! M√°rkakeresked√©sb≈ël!\"                                                              \n [78] \"MAZDA CX-5 2.5i e-Skyactiv Homura AWD (Automata) NAVI! M√°rkakeresked√©sb≈ël!\"                                                              \n [79] \"P\"                                                                                                                                       \n [80] \"KIA NIRO 1.6 GDI HEV Silver DCT\"                                                                                                         \n [81] \"KIA NIRO 1.6 GDI HEV Silver DCT\"                                                                                                         \n [82] \"P\"                                                                                                                                       \n [83] \"KIA XCEED 1.5 T-GDI X-Gold K√©szletr≈ël egyedi kedvezm√©nnyel!\"                                                                             \n [84] \"KIA XCEED 1.5 T-GDI X-Gold K√©szletr≈ël egyedi kedvezm√©nnyel!\"                                                                             \n [85] \"P\"                                                                                                                                       \n [86] \"SKODA SUPERB 2.0 TDI SCR Style 4x4 DSG Magyarorsz√°gi!1.tul.!V√©gig vezetett szervizk√∂nyv!\"                                                \n [87] \"SKODA SUPERB 2.0 TDI SCR Style 4x4 DSG Magyarorsz√°gi!1.tul.!V√©gig vezetett szervizk√∂nyv!\"                                                \n [88] \"P\"                                                                                                                                       \n [89] \"SKODA OCTAVIA 2.0 CR TDI Elegance Magyarorsz√°gi!Kit≈±n≈ë m≈±szaki √°llapot!Vezetett szerv√≠zk√∂nyv!\"                                           \n [90] \"SKODA OCTAVIA 2.0 CR TDI Elegance Magyarorsz√°gi!Kit≈±n≈ë m≈±szaki √°llapot!Vezetett szerv√≠zk√∂nyv!\"                                           \n [91] \"P\"                                                                                                                                       \n [92] \"MERCEDES-BENZ C 160 LED F√©nysz√≥r√≥k / AGILITY CONTROL fut√≥m≈± / Tolat√≥kamera\"                                                              \n [93] \"P\"                                                                                                                                       \n [94] \"MERCEDES-BENZ CLA 200 Progressive 7G-DCT Magyarorsz√°gi ISP\"                                                                              \n [95] \"P\"                                                                                                                                       \n [96] \"MERCEDES-BENZ C 180 9G-TRONIC Mild hybrid drive √öjszer≈±. √ÅF√Å-s! 9.99%THM\"                                                                \n [97] \"P\"                                                                                                                                       \n [98] \"MERCEDES-BENZ GLC 250 d 4Matic 9G-TRONIC 1.tulajd. vezetett szervizk! 9.99%THM\"                                                          \n [99] \"P\"                                                                                                                                       \n[100] \"BMW X4 xDrive20d (Automata) Adapt√≠v LED.Msport fut√≥m≈±.Ak√°r 2√©v Garanci√°val!\"                                                             \n[101] \"P\"                                                                                                                                       \n[102] \"MERCEDES-BENZ A 180 d AMG Line 8G-DCT Mo.-i. Garanci√°lis. ISP 2026.05.-ig. AMG Line. Panor√°ma tet≈ë\"                                      \n[103] \"P\"                                                                                                                                       \n[104] \"BMW 730d xDrive (Automata) Laser vil√°g√≠t√°s.BowersWilkinson.HeadUPDisplay.Panor√°ma tet≈ë.ak√°r 2 √©v Garanci√°va\"                             \n[105] \"P\"                                                                                                                                       \n[106] \"MERCEDES-AMG A-OSZT√ÅLY AMG A 35 Magyar!\"                                                                                                 \n[107] \"P\"                                                                                                                                       \n[108] \"BMW 330e M Sport (Automata)\"                                                                                                             \n[109] \"P\"                                                                                                                                       \n[110] \"MERCEDES-BENZ EQA 250 Gy√∂ny√∂r≈± √°llapot. Gy√°ri garancia 9.99%THM\"                                                                         \n[111] \"P\"                                                                                                                                       \n[112] \"MERCEDES-BENZ GLA 200 d Progressive Line 8G-DCT √ÅF√Å-s / Hossz√∫t√°v√∫ b√©rleti lehet≈ës√©g / Kedvez≈ë finansz√≠roz√°si opci√≥k!\"                   \n[113] \"P\"                                                                                                                                       \n[114] \"MERCEDES-BENZ S 560 4Matic 9G-TRONIC / √Åf√°s / Garanci√°lis / S√©r√ºl√©smentes / √Åll√≥f≈±t√©s / Iwc Schaffhausen anal√≥g √≥ra\"                     \n[115] \"P\"                                                                                                                                       \n[116] \"MERCEDES-BENZ GLC 300 de 4Matic 9G-TRONIC Plug-in hybrid √ÅF√Å-S/Hossz√∫t√°v√∫ b√©rleti lehet≈ës√©g!/Kedvez≈ë EUR alap√∫ finansz√≠roz√°si lehet≈ës√©g!\"\n[117] \"P\"                                                                                                                                       \n[118] \"MERCEDES-BENZ GLC 300 de 4Matic 9G-TRONIC Plug-in hybrid √ÅF√Å-s/ Hossz√∫t√°v√∫ b√©rleti lehet≈ës√©g/ Kedvez≈ë finansz√≠roz√°si opci√≥k!\"            \n[119] \"P\"                                                                                                                                       \n[120] \"MERCEDES-BENZ GLE 350 de 4Matic 9G-TRONIC Plug-in hybrid 1.tulajd. vezetett szervizk. 9.99%THM!\"                                         \n[121] \"P\"                                                                                                                                       \n[122] \"MERCEDES-BENZ GLE 400 d 4Matic 9G-TRONIC √ÅF√Å-s/ Garanci√°lis. Integr√°lt szerviz-csomag: 2026.12.h√≥-ig!\"                                   \n[123] \"P\"                                                                                                                                       \n[124] \"MERCEDES-BENZ S 400 d L 4Matic 9G-TRONIC Nett√≥ √°r: 42 511 811 Ft\"                                                                        \n[125] \"P\"                                                                                                                                       \n[126] \"OPEL CORSA F 1.2 BEST Magyarorsz√°gon els≈ënek forgalomba helyezett! Garanci√°lis g√©pj√°rm≈±\"                                                 \n[127] \"P\"                                                                                                                                       \n[128] \"SKODA OCTAVIA 1.0 TSI Ambition MAGYARORSZ√ÅGI. 87694 KM. F√âNYEZ√âSMENTES KAROSSZ√âRIA. 3 √âV GARANCIA!\"                                      \n[129] \"P\"                                                                                                                                       \n[130] \"HYUNDAI I30 1.6 T-GDi VEZETETT SZERVIZK√ñNYV - PANOR√ÅMATET≈ê - 186LE - F≈∞THET≈ê KORM√ÅNY - 3 √âV GARANCIA!\""
  },
  {
    "objectID": "content/12-content.html#find-the-link",
    "href": "content/12-content.html#find-the-link",
    "title": "Web scraping",
    "section": "Find the link",
    "text": "Find the link\n\nurl_to_car &lt;- my_node %&gt;% \n  html_attr(\"href\")\n\nurl_to_car\n\n  [1] NA                                                                                                                                                                                                                                                      \n  [2] \"https://www.hasznaltauto.hu/szemelyauto/suzuki/vitara/suzuki_vitara_1.4_hybrid_gl_plusz_akar_2.1_millio_ar-elony_plusz_0_thm-18561661#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                        \n  [3] \"https://www.hasznaltauto.hu/szemelyauto/suzuki/vitara/suzuki_vitara_1.4_hybrid_gl_plusz_akar_2.1_millio_ar-elony_plusz_0_thm-18561661#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                        \n  [4] NA                                                                                                                                                                                                                                                      \n  [5] \"https://www.hasznaltauto.hu/szemelyauto/opel/astra_l/opel_astra_l_1.2_t_business_edition-18643458#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                                            \n  [6] \"https://www.hasznaltauto.hu/szemelyauto/opel/astra_l/opel_astra_l_1.2_t_business_edition-18643458#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                                            \n  [7] NA                                                                                                                                                                                                                                                      \n  [8] \"https://www.hasznaltauto.hu/szemelyauto/nissan/x-trail/nissan_x-trail_1.5_vc-t_mild_hybrid_xtronic_acenta_fix_0thm-18677911#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                  \n  [9] \"https://www.hasznaltauto.hu/szemelyauto/nissan/x-trail/nissan_x-trail_1.5_vc-t_mild_hybrid_xtronic_acenta_fix_0thm-18677911#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                  \n [10] NA                                                                                                                                                                                                                                                      \n [11] \"https://www.hasznaltauto.hu/szemelyauto/nissan/qashqai/nissan_qashqai_1.3_dig-t_mild_hybrid_tekna_x-tronic_fix_0thm_keszletrol-18813851#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                      \n [12] \"https://www.hasznaltauto.hu/szemelyauto/nissan/qashqai/nissan_qashqai_1.3_dig-t_mild_hybrid_tekna_x-tronic_fix_0thm_keszletrol-18813851#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                      \n [13] NA                                                                                                                                                                                                                                                      \n [14] \"https://www.hasznaltauto.hu/szemelyauto/suzuki/s-cross/suzuki_s-cross_1.4_hybrid_gl_plusz_akar_2.1_millios_arelony_plusz_0_thm-19002890#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                      \n [15] \"https://www.hasznaltauto.hu/szemelyauto/suzuki/s-cross/suzuki_s-cross_1.4_hybrid_gl_plusz_akar_2.1_millios_arelony_plusz_0_thm-19002890#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                      \n [16] NA                                                                                                                                                                                                                                                      \n [17] \"https://www.hasznaltauto.hu/szemelyauto/peugeot/5008/peugeot_5008_1.2_puretech_active_pack_7_szemelyes_azonnal_elviheto_ev_vegeig_akcios_3.3_thm-mel-19047130#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                \n [18] \"https://www.hasznaltauto.hu/szemelyauto/peugeot/5008/peugeot_5008_1.2_puretech_active_pack_7_szemelyes_azonnal_elviheto_ev_vegeig_akcios_3.3_thm-mel-19047130#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                \n [19] NA                                                                                                                                                                                                                                                      \n [20] \"https://www.hasznaltauto.hu/szemelyauto/peugeot/3008/peugeot_3008_1.2_puretech_active_pack-19070070#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                                          \n [21] \"https://www.hasznaltauto.hu/szemelyauto/peugeot/3008/peugeot_3008_1.2_puretech_active_pack-19070070#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                                          \n [22] NA                                                                                                                                                                                                                                                      \n [23] \"https://www.hasznaltauto.hu/szemelyauto/mazda/6/mazda_6_sportkombi_g194_20th_anniversary_-_keszleten-19159548#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                                \n [24] \"https://www.hasznaltauto.hu/szemelyauto/mazda/6/mazda_6_sportkombi_g194_20th_anniversary_-_keszleten-19159548#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                                \n [25] NA                                                                                                                                                                                                                                                      \n [26] \"https://www.hasznaltauto.hu/szemelyauto/opel/grandland/opel_grandland_1.5_cdti_business_edition_automata_hamarosan_megerkezik-19194133#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                       \n [27] \"https://www.hasznaltauto.hu/szemelyauto/opel/grandland/opel_grandland_1.5_cdti_business_edition_automata_hamarosan_megerkezik-19194133#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                       \n [28] NA                                                                                                                                                                                                                                                      \n [29] \"https://www.hasznaltauto.hu/szemelyauto/kia/sorento/kia_sorento_1.6_t-gdi_phev_platinum_4wd_automata_plug_in_hybrid_osszkerek_tobb_szinben_azonnal-19310379#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                  \n [30] \"https://www.hasznaltauto.hu/szemelyauto/kia/sorento/kia_sorento_1.6_t-gdi_phev_platinum_4wd_automata_plug_in_hybrid_osszkerek_tobb_szinben_azonnal-19310379#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                  \n [31] NA                                                                                                                                                                                                                                                      \n [32] \"https://www.hasznaltauto.hu/szemelyauto/peugeot/308/peugeot_308_sw_1.6_puretech_phev_gt_eat8_azonnal_elviheto-19354937#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                       \n [33] \"https://www.hasznaltauto.hu/szemelyauto/peugeot/308/peugeot_308_sw_1.6_puretech_phev_gt_eat8_azonnal_elviheto-19354937#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                       \n [34] NA                                                                                                                                                                                                                                                      \n [35] \"https://www.hasznaltauto.hu/szemelyauto/peugeot/208/peugeot_208_1.2_puretech_active_pack_eat8-19355043#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                                       \n [36] \"https://www.hasznaltauto.hu/szemelyauto/peugeot/208/peugeot_208_1.2_puretech_active_pack_eat8-19355043#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                                       \n [37] NA                                                                                                                                                                                                                                                      \n [38] \"https://www.hasznaltauto.hu/szemelyauto/peugeot/rifter/peugeot_rifter_e-rifter_50kwh_active_pack_kizarolag_gyartasrendelesbol-19355087#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                       \n [39] \"https://www.hasznaltauto.hu/szemelyauto/peugeot/rifter/peugeot_rifter_e-rifter_50kwh_active_pack_kizarolag_gyartasrendelesbol-19355087#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                       \n [40] NA                                                                                                                                                                                                                                                      \n [41] \"https://www.hasznaltauto.hu/szemelyauto/ssangyong/rexton/ssangyong_rexton_2.2_e-xdi_premium_4wd_automata_7_szemelyes_tesztauto-19368594#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                      \n [42] \"https://www.hasznaltauto.hu/szemelyauto/ssangyong/rexton/ssangyong_rexton_2.2_e-xdi_premium_4wd_automata_7_szemelyes_tesztauto-19368594#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                      \n [43] NA                                                                                                                                                                                                                                                      \n [44] \"https://www.hasznaltauto.hu/szemelyauto/opel/corsa_f/opel_corsa_f_1.2_edition_akcios_modell_kimagaslo_kedvezmennyel-19396681#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                 \n [45] \"https://www.hasznaltauto.hu/szemelyauto/opel/corsa_f/opel_corsa_f_1.2_edition_akcios_modell_kimagaslo_kedvezmennyel-19396681#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                 \n [46] NA                                                                                                                                                                                                                                                      \n [47] \"https://www.hasznaltauto.hu/szemelyauto/kia/ceed/kia_ceed_ceed_1.0_t-gdi_silver_magyar_szervizkonyv_kamera_afas-19462882#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                     \n [48] \"https://www.hasznaltauto.hu/szemelyauto/kia/ceed/kia_ceed_ceed_1.0_t-gdi_silver_magyar_szervizkonyv_kamera_afas-19462882#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                     \n [49] NA                                                                                                                                                                                                                                                      \n [50] \"https://www.hasznaltauto.hu/szemelyauto/opel/crossland/opel_crossland_1.2_t_edition_kiemelt_akcios_modell-19469653#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                           \n [51] \"https://www.hasznaltauto.hu/szemelyauto/opel/crossland/opel_crossland_1.2_t_edition_kiemelt_akcios_modell-19469653#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                           \n [52] NA                                                                                                                                                                                                                                                      \n [53] \"https://www.hasznaltauto.hu/szemelyauto/ssangyong/korando/ssangyong_korando_1.5_turbo_gdi_clever_kulonbozo_felszereltsegek_szinek_azonnal_raktarrol-19665635#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                 \n [54] \"https://www.hasznaltauto.hu/szemelyauto/ssangyong/korando/ssangyong_korando_1.5_turbo_gdi_clever_kulonbozo_felszereltsegek_szinek_azonnal_raktarrol-19665635#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                 \n [55] NA                                                                                                                                                                                                                                                      \n [56] \"https://www.hasznaltauto.hu/szemelyauto/hyundai/tucson/hyundai_tucson_1.6_t-gdi_hybrid_executive_plus_automata_keszletrol_azonnal_230le_hybrid-19698246#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                      \n [57] \"https://www.hasznaltauto.hu/szemelyauto/hyundai/tucson/hyundai_tucson_1.6_t-gdi_hybrid_executive_plus_automata_keszletrol_azonnal_230le_hybrid-19698246#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                      \n [58] NA                                                                                                                                                                                                                                                      \n [59] \"https://www.hasznaltauto.hu/szemelyauto/mercedes-benz/c_220/mercedes-benz_c_220_t_cdi_blueefficiency_avantgarde_amg_lineharmankardongyonyoru_allapot-19701156#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                \n [60] \"https://www.hasznaltauto.hu/szemelyauto/mercedes-benz/c_220/mercedes-benz_c_220_t_cdi_blueefficiency_avantgarde_amg_lineharmankardongyonyoru_allapot-19701156#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                \n [61] NA                                                                                                                                                                                                                                                      \n [62] \"https://www.hasznaltauto.hu/szemelyauto/opel/mokka/opel_mokka_1.2_t_edition_kiemelt_akcios_modell_utolerhetetlen_aronegyedi_lizingakcio-19816005#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                             \n [63] \"https://www.hasznaltauto.hu/szemelyauto/opel/mokka/opel_mokka_1.2_t_edition_kiemelt_akcios_modell_utolerhetetlen_aronegyedi_lizingakcio-19816005#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                             \n [64] NA                                                                                                                                                                                                                                                      \n [65] \"https://www.hasznaltauto.hu/szemelyauto/peugeot/408/peugeot_408_1.2_puretech_allure_pack_eat8_tesztauto_2024_februari_atvetellel-19822407#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                    \n [66] \"https://www.hasznaltauto.hu/szemelyauto/peugeot/408/peugeot_408_1.2_puretech_allure_pack_eat8_tesztauto_2024_februari_atvetellel-19822407#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                    \n [67] NA                                                                                                                                                                                                                                                      \n [68] \"https://www.hasznaltauto.hu/szemelyauto/ssangyong/tivoli/ssangyong_tivoli_1.5_gdi-t_style_uj_forma_bevalt_tartalom_rendelje_meg_nalunk_az_uj_tivolit-19839434#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                \n [69] \"https://www.hasznaltauto.hu/szemelyauto/ssangyong/tivoli/ssangyong_tivoli_1.5_gdi-t_style_uj_forma_bevalt_tartalom_rendelje_meg_nalunk_az_uj_tivolit-19839434#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                \n [70] NA                                                                                                                                                                                                                                                      \n [71] \"https://www.hasznaltauto.hu/szemelyauto/ssangyong/torres/ssangyong_torres_1.5_turbo_gdi_club_megerkezett_az_uj_torres_probalja_ki_es_rendelje_meg_nalunk-19839893#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                            \n [72] \"https://www.hasznaltauto.hu/szemelyauto/ssangyong/torres/ssangyong_torres_1.5_turbo_gdi_club_megerkezett_az_uj_torres_probalja_ki_es_rendelje_meg_nalunk-19839893#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                            \n [73] NA                                                                                                                                                                                                                                                      \n [74] \"https://www.hasznaltauto.hu/szemelyauto/audi/a6_allroad/audi_a6_allroad_55_tfsi_quattro_s-tronic_egyedi_szin_bangolufsen_hifi-19853292#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                       \n [75] \"https://www.hasznaltauto.hu/szemelyauto/audi/a6_allroad/audi_a6_allroad_55_tfsi_quattro_s-tronic_egyedi_szin_bangolufsen_hifi-19853292#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                       \n [76] NA                                                                                                                                                                                                                                                      \n [77] \"https://www.hasznaltauto.hu/szemelyauto/mazda/cx-5/mazda_cx-5_2.5i_e-skyactiv_homura_awd_automata_navi_markakereskedesbol-19887851#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                           \n [78] \"https://www.hasznaltauto.hu/szemelyauto/mazda/cx-5/mazda_cx-5_2.5i_e-skyactiv_homura_awd_automata_navi_markakereskedesbol-19887851#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                           \n [79] NA                                                                                                                                                                                                                                                      \n [80] \"https://www.hasznaltauto.hu/szemelyauto/kia/niro/kia_niro_1.6_gdi_hev_silver_dct-19893267#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                                                    \n [81] \"https://www.hasznaltauto.hu/szemelyauto/kia/niro/kia_niro_1.6_gdi_hev_silver_dct-19893267#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                                                    \n [82] NA                                                                                                                                                                                                                                                      \n [83] \"https://www.hasznaltauto.hu/szemelyauto/kia/xceed/kia_xceed_1.5_t-gdi_x-gold_keszletrol_egyedi_kedvezmennyel-19957055#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                        \n [84] \"https://www.hasznaltauto.hu/szemelyauto/kia/xceed/kia_xceed_1.5_t-gdi_x-gold_keszletrol_egyedi_kedvezmennyel-19957055#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                        \n [85] NA                                                                                                                                                                                                                                                      \n [86] \"https://www.hasznaltauto.hu/szemelyauto/skoda/superb/skoda_superb_2.0_tdi_scr_style_4x4_dsg_magyarorszagi1tulvegig_vezetett_szervizkonyv-19974031#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                            \n [87] \"https://www.hasznaltauto.hu/szemelyauto/skoda/superb/skoda_superb_2.0_tdi_scr_style_4x4_dsg_magyarorszagi1tulvegig_vezetett_szervizkonyv-19974031#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                            \n [88] NA                                                                                                                                                                                                                                                      \n [89] \"https://www.hasznaltauto.hu/szemelyauto/skoda/octavia/skoda_octavia_2.0_cr_tdi_elegance_magyarorszagikituno_muszaki_allapotvezetett_szervizkonyv-19995745#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                    \n [90] \"https://www.hasznaltauto.hu/szemelyauto/skoda/octavia/skoda_octavia_2.0_cr_tdi_elegance_magyarorszagikituno_muszaki_allapotvezetett_szervizkonyv-19995745#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                    \n [91] NA                                                                                                                                                                                                                                                      \n [92] \"https://www.hasznaltauto.hu/szemelyauto/mercedes-benz/c_160/mercedes-benz_c_160_led_fenyszorok_agility_control_futomu_tolatokamera-19486548#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                  \n [93] NA                                                                                                                                                                                                                                                      \n [94] \"https://www.hasznaltauto.hu/szemelyauto/mercedes-benz/cla_200/mercedes-benz_cla_200_progressive_7g-dct_magyarorszagi_isp-19997915#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                            \n [95] NA                                                                                                                                                                                                                                                      \n [96] \"https://www.hasznaltauto.hu/szemelyauto/mercedes-benz/c_180/mercedes-benz_c_180_9g-tronic_mild_hybrid_drive_ujszeru_afa-s_9.99thm-19677410#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                   \n [97] NA                                                                                                                                                                                                                                                      \n [98] \"https://www.hasznaltauto.hu/szemelyauto/mercedes-benz/glc_250/mercedes-benz_glc_250_d_4matic_9g-tronic_1tulajd_vezetett_szervizk_9.99thm-19720733#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                            \n [99] NA                                                                                                                                                                                                                                                      \n[100] \"https://www.hasznaltauto.hu/szemelyauto/bmw/x4/bmw_x4_xdrive20d_automata_adaptiv_ledmsport_futomuakar_2ev_garanciaval-19812619#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                               \n[101] NA                                                                                                                                                                                                                                                      \n[102] \"https://www.hasznaltauto.hu/szemelyauto/mercedes-benz/a_180/mercedes-benz_a_180_d_amg_line_8g-dct_mo-i_garancialis_isp_2026.05-ig_amg_line_panorama_teto-19473288#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                            \n[103] NA                                                                                                                                                                                                                                                      \n[104] \"https://www.hasznaltauto.hu/szemelyauto/bmw/730/bmw_730d_xdrive_automata_laser_vilagitasbowerswilkinsonheadupdisplaypanorama_tetoakar_2_ev_garanciava-19930412#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                               \n[105] NA                                                                                                                                                                                                                                                      \n[106] \"https://www.hasznaltauto.hu/szemelyauto/mercedes-amg/a-osztaly/mercedes-amg_a-osztaly_amg_a_35_magyar-20000137#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                               \n[107] NA                                                                                                                                                                                                                                                      \n[108] \"https://www.hasznaltauto.hu/szemelyauto/bmw/330/bmw_330e_m_sport_automata-19939643#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                                                                           \n[109] NA                                                                                                                                                                                                                                                      \n[110] \"https://www.hasznaltauto.hu/szemelyauto/mercedes-benz/eqa/mercedes-benz_eqa_250_gyonyoru_allapot_gyari_garancia_9.99thm-19392310#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                             \n[111] NA                                                                                                                                                                                                                                                      \n[112] \"https://www.hasznaltauto.hu/szemelyauto/mercedes-benz/gla_200/mercedes-benz_gla_200_d_progressive_line_8g-dct_afa-s_hosszutavu_berleti_lehetoseg_kedvezo_finanszirozasi_opciok-19336319#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                      \n[113] NA                                                                                                                                                                                                                                                      \n[114] \"https://www.hasznaltauto.hu/szemelyauto/mercedes-benz/s_560/mercedes-benz_s_560_4matic_9g-tronic_afas_garancialis_serulesmentes_allofutes_iwc_schaffhausen_analog_ora-19656611#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                               \n[115] NA                                                                                                                                                                                                                                                      \n[116] \"https://www.hasznaltauto.hu/szemelyauto/mercedes-benz/glc_300/mercedes-benz_glc_300_de_4matic_9g-tronic_plug-in_hybrid_afa-s_hosszutavu_berleti_lehetoseg_kedvezo_eur_alapu_finanszirozasi_lehetoseg-19330101#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"\n[117] NA                                                                                                                                                                                                                                                      \n[118] \"https://www.hasznaltauto.hu/szemelyauto/mercedes-benz/glc_300/mercedes-benz_glc_300_de_4matic_9g-tronic_plug-in_hybrid_afa-s_hosszutavu_berleti_lehetoseg_kedvezo_finanszirozasi_opciok-19486938#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"             \n[119] NA                                                                                                                                                                                                                                                      \n[120] \"https://www.hasznaltauto.hu/szemelyauto/mercedes-benz/gle_350/mercedes-benz_gle_350_de_4matic_9g-tronic_plug-in_hybrid_1tulajd_vezetett_szervizk_9.99thm-19766015#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                            \n[121] NA                                                                                                                                                                                                                                                      \n[122] \"https://www.hasznaltauto.hu/szemelyauto/mercedes-benz/gle_400/mercedes-benz_gle_400_d_4matic_9g-tronic_afa-s_garancialis_integralt_szerviz-csomag_2026.12ho-ig-19357679#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                      \n[123] NA                                                                                                                                                                                                                                                      \n[124] \"https://www.hasznaltauto.hu/szemelyauto/mercedes-benz/s_400/mercedes-benz_s_400_d_l_4matic_9g-tronic_netto_ar_42_511_811_ft-19996855#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                                         \n[125] NA                                                                                                                                                                                                                                                      \n[126] \"https://www.hasznaltauto.hu/szemelyauto/opel/corsa_f/opel_corsa_f_1.2_best_magyarorszagon_elsonek_forgalomba_helyezett_garancialis_gepjarmu-20012929#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                         \n[127] NA                                                                                                                                                                                                                                                      \n[128] \"https://www.hasznaltauto.hu/szemelyauto/skoda/octavia/skoda_octavia_1.0_tsi_ambition_magyarorszagi_87694_km_fenyezesmentes_karosszeria_3_ev_garancia-19849488#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                                \n[129] NA                                                                                                                                                                                                                                                      \n[130] \"https://www.hasznaltauto.hu/szemelyauto/hyundai/i30/hyundai_i30_1.6_t-gdi_vezetett_szervizkonyv_-_panoramateto_-_186le_-_futheto_kormany_-_3_ev_garancia-19942766#sid=651dab01-0dab-4703-bd70-25ee2ea122e5\"                                            \n\n\nWe‚Äôve now collected the names of all the ads and the url leading to them from the first page. The next step is to collect this data from all pages. However, downloading info from all sites is a lengthy process. Always try the first few pages first and only download them all if you are sure that your program is running without error.\nBut how do we download data from multiple pages at once? Let‚Äôs see if the url changes when we go to the next page.\nIf we look at the second page of the results list in the browser, we can see that the link has been expanded with a ‚Äú/page2‚Äù member compared to our previous url address. Continue with ‚Äú/ page3‚Äù on page 3, etc.\nThis way we can easily generate a vector that contains the links to the first 1, 10, 100, 1000 or even all pages. Lets put these into a data.frame.\n\nurl_ending &lt;- str_c(\"/page\", 2:10)\n\nurl_ending\n\n[1] \"/page2\"  \"/page3\"  \"/page4\"  \"/page5\"  \"/page6\"  \"/page7\"  \"/page8\" \n[8] \"/page9\"  \"/page10\"\n\nurl_ending &lt;- c(\"\", url_ending) # nothing to add at the first page\n\nurl_ending\n\n [1] \"\"        \"/page2\"  \"/page3\"  \"/page4\"  \"/page5\"  \"/page6\"  \"/page7\" \n [8] \"/page8\"  \"/page9\"  \"/page10\"\n\ncars_add_df &lt;- tibble(url = str_c(url, url_ending))\n\ncars_add_df\n\n# A tibble: 10 √ó 1\n   url                                                                          \n   &lt;chr&gt;                                                                        \n 1 https://www.hasznaltauto.hu/talalatilista/PCOG2VGRR3RDADH4S56ACFGY3UFY652CQ5‚Ä¶\n 2 https://www.hasznaltauto.hu/talalatilista/PCOG2VGRR3RDADH4S56ACFGY3UFY652CQ5‚Ä¶\n 3 https://www.hasznaltauto.hu/talalatilista/PCOG2VGRR3RDADH4S56ACFGY3UFY652CQ5‚Ä¶\n 4 https://www.hasznaltauto.hu/talalatilista/PCOG2VGRR3RDADH4S56ACFGY3UFY652CQ5‚Ä¶\n 5 https://www.hasznaltauto.hu/talalatilista/PCOG2VGRR3RDADH4S56ACFGY3UFY652CQ5‚Ä¶\n 6 https://www.hasznaltauto.hu/talalatilista/PCOG2VGRR3RDADH4S56ACFGY3UFY652CQ5‚Ä¶\n 7 https://www.hasznaltauto.hu/talalatilista/PCOG2VGRR3RDADH4S56ACFGY3UFY652CQ5‚Ä¶\n 8 https://www.hasznaltauto.hu/talalatilista/PCOG2VGRR3RDADH4S56ACFGY3UFY652CQ5‚Ä¶\n 9 https://www.hasznaltauto.hu/talalatilista/PCOG2VGRR3RDADH4S56ACFGY3UFY652CQ5‚Ä¶\n10 https://www.hasznaltauto.hu/talalatilista/PCOG2VGRR3RDADH4S56ACFGY3UFY652CQ5‚Ä¶\n\n\nTaking advantage of the {purr} package, we can load and manipulate entire pages in data frames in a transparent and human-readable (tidy) way.\nThe second example shows that if we specify only one function as the second input, without ‚Äú~‚Äù and ‚Äú.‚Äù, the first input of map will automatically be the first possible input for the specified function, and then the elements added after the function will follow in order. In the present case, we prepared random samples with 1, 2, and 3 elements, where the expected value of the samples is 10, because the first input of the function rnorm is the number of elements (n) and the second is the expected value (mean).\nWe will now use the map function inside themutate function, so a list will actually be a column of the original table. The elements of this list will be the web pages that are loaded with the read_html function.\n\ncars_add_df %&gt;% \n  head(2) %&gt;%\n  mutate(\n    page = map(url, read_html)\n  )\n\n# A tibble: 2 √ó 2\n  url                                                                 page      \n  &lt;chr&gt;                                                               &lt;list&gt;    \n1 https://www.hasznaltauto.hu/talalatilista/PCOG2VGRR3RDADH4S56ACFGY‚Ä¶ &lt;xml_dcmn&gt;\n2 https://www.hasznaltauto.hu/talalatilista/PCOG2VGRR3RDADH4S56ACFGY‚Ä¶ &lt;xml_dcmn&gt;\n\n\nNow let‚Äôs insert a nodes column that contains only the required points of the page, and aurl_to_cars column, which is the links extracted from it. Since this is already a computationally intensive step, it is worth saving the result. Save it with the same name so your environment won‚Äôt be full of unnecessary variables.\n\ncars_add_df &lt;- cars_add_df %&gt;% \n  head(2) %&gt;%\n  mutate(\n    page = map(url, read_html),\n    nodes = map(page, ~ html_nodes(., \".cim-kontener a\")),\n    ad_title = map(nodes, html_text),\n    url_to_cars = map(nodes, html_attr, \"href\")\n  )\n\n\ncars_add_df\n\n# A tibble: 2 √ó 5\n  url                                 page       nodes      ad_title url_to_cars\n  &lt;chr&gt;                               &lt;list&gt;     &lt;list&gt;     &lt;list&gt;   &lt;list&gt;     \n1 https://www.hasznaltauto.hu/talala‚Ä¶ &lt;xml_dcmn&gt; &lt;xml_ndst&gt; &lt;chr&gt;    &lt;chr [130]&gt;\n2 https://www.hasznaltauto.hu/talala‚Ä¶ &lt;xml_dcmn&gt; &lt;xml_ndst&gt; &lt;chr&gt;    &lt;chr [40]&gt; \n\n\nFrom now on, all we need is a column with links and titles, but these are nested. The first cell of the url_to_cars column contains more than a hundred links. This nesting option opens a lot of new doors in the process of data manipulation. Use the unnest function to extract these columns.\n\ncars_add_df %&gt;% \n  select(url_to_cars, ad_title) %&gt;% \n  unnest()\n\n# A tibble: 170 √ó 2\n   url_to_cars                                                          ad_title\n   &lt;chr&gt;                                                                &lt;chr&gt;   \n 1 &lt;NA&gt;                                                                 P       \n 2 https://www.hasznaltauto.hu/szemelyauto/suzuki/vitara/suzuki_vitara‚Ä¶ SUZUKI ‚Ä¶\n 3 https://www.hasznaltauto.hu/szemelyauto/suzuki/vitara/suzuki_vitara‚Ä¶ SUZUKI ‚Ä¶\n 4 &lt;NA&gt;                                                                 P       \n 5 https://www.hasznaltauto.hu/szemelyauto/opel/astra_l/opel_astra_l_1‚Ä¶ OPEL AS‚Ä¶\n 6 https://www.hasznaltauto.hu/szemelyauto/opel/astra_l/opel_astra_l_1‚Ä¶ OPEL AS‚Ä¶\n 7 &lt;NA&gt;                                                                 P       \n 8 https://www.hasznaltauto.hu/szemelyauto/nissan/x-trail/nissan_x-tra‚Ä¶ NISSAN ‚Ä¶\n 9 https://www.hasznaltauto.hu/szemelyauto/nissan/x-trail/nissan_x-tra‚Ä¶ NISSAN ‚Ä¶\n10 &lt;NA&gt;                                                                 P       \n# ‚Ñπ 160 more rows\n\n\nWe can see that the nodes selection was not entirely perfect (usually not), as there were no matching elements or duplications left.\nBut now we can still remove the items that don‚Äôt fit here. Fortunately, there is no link behind these, and if all columns are the same, unique1 will be our service.\n\ncars_add_df &lt;- cars_add_df %&gt;% \n  select(url_to_cars, ad_title) %&gt;% \n  unnest() %&gt;% \n  na.omit() %&gt;% # remove rows where url_to_cars is missing (NA)\n  unique() # delete duplications\n\ncars_add_df\n\n# A tibble: 70 √ó 2\n   url_to_cars                                                          ad_title\n   &lt;chr&gt;                                                                &lt;chr&gt;   \n 1 https://www.hasznaltauto.hu/szemelyauto/suzuki/vitara/suzuki_vitara‚Ä¶ SUZUKI ‚Ä¶\n 2 https://www.hasznaltauto.hu/szemelyauto/opel/astra_l/opel_astra_l_1‚Ä¶ OPEL AS‚Ä¶\n 3 https://www.hasznaltauto.hu/szemelyauto/nissan/x-trail/nissan_x-tra‚Ä¶ NISSAN ‚Ä¶\n 4 https://www.hasznaltauto.hu/szemelyauto/nissan/qashqai/nissan_qashq‚Ä¶ NISSAN ‚Ä¶\n 5 https://www.hasznaltauto.hu/szemelyauto/suzuki/s-cross/suzuki_s-cro‚Ä¶ SUZUKI ‚Ä¶\n 6 https://www.hasznaltauto.hu/szemelyauto/peugeot/5008/peugeot_5008_1‚Ä¶ PEUGEOT‚Ä¶\n 7 https://www.hasznaltauto.hu/szemelyauto/peugeot/3008/peugeot_3008_1‚Ä¶ PEUGEOT‚Ä¶\n 8 https://www.hasznaltauto.hu/szemelyauto/mazda/6/mazda_6_sportkombi_‚Ä¶ MAZDA 6‚Ä¶\n 9 https://www.hasznaltauto.hu/szemelyauto/opel/grandland/opel_grandla‚Ä¶ OPEL GR‚Ä¶\n10 https://www.hasznaltauto.hu/szemelyauto/kia/sorento/kia_sorento_1.6‚Ä¶ KIA SOR‚Ä¶\n# ‚Ñπ 60 more rows\n\n\nWe now have a ready-made list of available cars and their links. Let‚Äôs visit one in the browser to see what‚Äôs next."
  },
  {
    "objectID": "content/12-content.html#data-from-tables",
    "href": "content/12-content.html#data-from-tables",
    "title": "Web scraping",
    "section": "Data from tables",
    "text": "Data from tables\n\n\n\n\nA random example of car ad.\n\n\n\nYou can see that the data is tabulated on the page. This is the best option for us, as you don‚Äôt have to search for the item ID on the page one by one (as before with the ad title). You can simply apply the html_table function to a loaded page, which collects all the tables on the it.\n\ncar_url &lt;- \"https://www.hasznaltauto.hu/szemelyauto/suzuki/vitara/suzuki_vitara_1.4_hybrid_gl_plusz_akar_2.1_millio_ar-elony_plusz_0_thm-18561661#sid=662a20d7-dd8c-4ede-965c-8736e82d6024\"\n\ninfo_tables &lt;- read_html(car_url) %&gt;% \n  html_table(fill = TRUE)\n\ninfo_tables\n\n[[1]]\n# A tibble: 44 √ó 2\n   `√År, k√∂lts√©gek`                                               `√År, k√∂lts√©gek`\n   &lt;chr&gt;                                                         &lt;chr&gt;          \n 1 \"V√©tel√°r:\"                                                    \"√År n√©lk√ºl\"    \n 2 \"√Åltal√°nos adatok\"                                            \"√Åltal√°nos ada‚Ä¶\n 3 \"√Åtvehet≈ë:\"                                                   \"2023.\"        \n 4 \"√âvj√°rat:\"                                                    \"2023\"         \n 5 \"√Ållapot:\"                                                    \"Norm√°l\"       \n 6 \"Kivitel:\"                                                    \"V√°rosi terepj‚Ä¶\n 7 \"Nemzetk√∂zi j√°rm≈±el≈ë√©let\"                                     \"Nemzetk√∂zi j√°‚Ä¶\n 8 \"Inform√°l√≥dj az alv√°zsz√°mr√≥l a nemzetk√∂zi aut√≥-el≈ë√©let szolg‚Ä¶ \"Inform√°l√≥dj a‚Ä¶\n 9 \"Hazai j√°rm≈±el≈ë√©let\"                                          \"Hazai j√°rm≈±el‚Ä¶\n10 \"Inform√°l√≥dj az alv√°zsz√°mr√≥l a magyarorsz√°gi aut√≥-el≈ë√©let sz‚Ä¶ \"Inform√°l√≥dj a‚Ä¶\n# ‚Ñπ 34 more rows\n\n\nWhat is the type of info_tables? Since it collects all the tables it can find from the page, it is a list of data frames.\nI will reveal that in some cases we will see that there are multiple tables on a page (certain types of info are taken separately) and we want to avoid an irrelevant single column table causing an error (step 1).\nOur goal is to be able to gather all the data about the car into a single two-column table. To join all two column tables (step 3), they must also have the same name (step 2).\n\ninfo_tables %&gt;% \n  keep(~ ncol(.) == 2) %&gt;%  # keep tables that have 2 columns\n  map(set_names, \"x\", \"y\") %&gt;%  # set the names to x and y for each table\n  bind_rows() # join the tables to one sinle table\n\n# A tibble: 44 √ó 2\n   x                                                                       y    \n   &lt;chr&gt;                                                                   &lt;chr&gt;\n 1 \"V√©tel√°r:\"                                                              \"√År ‚Ä¶\n 2 \"√Åltal√°nos adatok\"                                                      \"√Ålt‚Ä¶\n 3 \"√Åtvehet≈ë:\"                                                             \"202‚Ä¶\n 4 \"√âvj√°rat:\"                                                              \"202‚Ä¶\n 5 \"√Ållapot:\"                                                              \"Nor‚Ä¶\n 6 \"Kivitel:\"                                                              \"V√°r‚Ä¶\n 7 \"Nemzetk√∂zi j√°rm≈±el≈ë√©let\"                                               \"Nem‚Ä¶\n 8 \"Inform√°l√≥dj az alv√°zsz√°mr√≥l a nemzetk√∂zi aut√≥-el≈ë√©let szolg√°ltat√°s ha‚Ä¶ \"Inf‚Ä¶\n 9 \"Hazai j√°rm≈±el≈ë√©let\"                                                    \"Haz‚Ä¶\n10 \"Inform√°l√≥dj az alv√°zsz√°mr√≥l a magyarorsz√°gi aut√≥-el≈ë√©let szolg√°ltat√°s‚Ä¶ \"Inf‚Ä¶\n# ‚Ñπ 34 more rows\n\n\nWe see that this works, the output now is one single table. Lets use this method on all the links. To do this, we need to write a function.\n\nget_data &lt;- function(url_to_car) {\n  url_to_car %&gt;% \n    read_html() %&gt;% \n    html_table(fill = TRUE) %&gt;% \n    keep(~ ncol(.) == 2) %&gt;% \n    map(~ set_names(., \"x\", \"y\")) %&gt;% \n    bind_rows()\n}\n\n\nget_data(car_url)\n\n# A tibble: 44 √ó 2\n   x                                                                       y    \n   &lt;chr&gt;                                                                   &lt;chr&gt;\n 1 \"V√©tel√°r:\"                                                              \"√År ‚Ä¶\n 2 \"√Åltal√°nos adatok\"                                                      \"√Ålt‚Ä¶\n 3 \"√Åtvehet≈ë:\"                                                             \"202‚Ä¶\n 4 \"√âvj√°rat:\"                                                              \"202‚Ä¶\n 5 \"√Ållapot:\"                                                              \"Nor‚Ä¶\n 6 \"Kivitel:\"                                                              \"V√°r‚Ä¶\n 7 \"Nemzetk√∂zi j√°rm≈±el≈ë√©let\"                                               \"Nem‚Ä¶\n 8 \"Inform√°l√≥dj az alv√°zsz√°mr√≥l a nemzetk√∂zi aut√≥-el≈ë√©let szolg√°ltat√°s ha‚Ä¶ \"Inf‚Ä¶\n 9 \"Hazai j√°rm≈±el≈ë√©let\"                                                    \"Haz‚Ä¶\n10 \"Inform√°l√≥dj az alv√°zsz√°mr√≥l a magyarorsz√°gi aut√≥-el≈ë√©let szolg√°ltat√°s‚Ä¶ \"Inf‚Ä¶\n# ‚Ñπ 34 more rows\n\n\nThe resulting table can all be collected in a single column of our table. Each cell will be a table that we all collected from a given link.\nThis can also take a serious amount of time if we want to retrieve data from hundreds of cars at once. You should always use only a few. Use the sample_n function to randomly select a few lines to test if everything works as expected.\n\ncars_add_df %&gt;% \n  sample_n(size = 3) %&gt;% # remove this line at the end if everything is fine\n  mutate(\n    data = map(url_to_cars, get_data)\n  )\n\n# A tibble: 3 √ó 3\n  url_to_cars                                                  ad_title data    \n  &lt;chr&gt;                                                        &lt;chr&gt;    &lt;list&gt;  \n1 https://www.hasznaltauto.hu/szemelyauto/peugeot/5008/peugeo‚Ä¶ PEUGEOT‚Ä¶ &lt;tibble&gt;\n2 https://www.hasznaltauto.hu/szemelyauto/kia/niro/kia_niro_1‚Ä¶ KIA NIR‚Ä¶ &lt;tibble&gt;\n3 https://www.hasznaltauto.hu/szemelyauto/toyota/auris/toyota‚Ä¶ TOYOTA ‚Ä¶ &lt;tibble&gt;\n\n\nCurrently, our small table is also nested in a cell. Expand it!\n\ncars_add_df %&gt;% \n  sample_n(size = 3) %&gt;% # remove this line at the end if everything is fine\n  mutate(\n    data = map(url_to_cars, get_data)\n  ) %&gt;% \n  unnest()\n\n# A tibble: 107 √ó 4\n   url_to_cars                                              ad_title x     y    \n   &lt;chr&gt;                                                    &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;\n 1 https://www.hasznaltauto.hu/szemelyauto/peugeot/3008/pe‚Ä¶ PEUGEOT‚Ä¶ \"Ala‚Ä¶ \"9¬†8‚Ä¶\n 2 https://www.hasznaltauto.hu/szemelyauto/peugeot/3008/pe‚Ä¶ PEUGEOT‚Ä¶ \"Ext‚Ä¶ \"11¬†‚Ä¶\n 3 https://www.hasznaltauto.hu/szemelyauto/peugeot/3008/pe‚Ä¶ PEUGEOT‚Ä¶ \"Akc‚Ä¶ \"9¬†8‚Ä¶\n 4 https://www.hasznaltauto.hu/szemelyauto/peugeot/3008/pe‚Ä¶ PEUGEOT‚Ä¶ \"Akc‚Ä¶ \"√ârd‚Ä¶\n 5 https://www.hasznaltauto.hu/szemelyauto/peugeot/3008/pe‚Ä¶ PEUGEOT‚Ä¶ \"V√©t‚Ä¶ \"‚Ç¨¬†2‚Ä¶\n 6 https://www.hasznaltauto.hu/szemelyauto/peugeot/3008/pe‚Ä¶ PEUGEOT‚Ä¶ \"Fin‚Ä¶ \"Fin‚Ä¶\n 7 https://www.hasznaltauto.hu/szemelyauto/peugeot/3008/pe‚Ä¶ PEUGEOT‚Ä¶ \"√Ålt‚Ä¶ \"√Ålt‚Ä¶\n 8 https://www.hasznaltauto.hu/szemelyauto/peugeot/3008/pe‚Ä¶ PEUGEOT‚Ä¶ \"√Åtv‚Ä¶ \"202‚Ä¶\n 9 https://www.hasznaltauto.hu/szemelyauto/peugeot/3008/pe‚Ä¶ PEUGEOT‚Ä¶ \"Fut‚Ä¶ \"√öj ‚Ä¶\n10 https://www.hasznaltauto.hu/szemelyauto/peugeot/3008/pe‚Ä¶ PEUGEOT‚Ä¶ \"√âvj‚Ä¶ \"202‚Ä¶\n# ‚Ñπ 97 more rows\n\n\nNow that we have a lot of rows instead of 3, the url in the first row is repeated as many times as the number of rows in the table next to it so far.\nSince the x column now has the variable name and the y column has the value of the variable, we need another super useful function we‚Äôve seen before: pivot_wider!\n\ncars_add_df %&gt;% \n  sample_n(size = 3) %&gt;% # remove this line at the end if everything is fine\n  mutate(\n    data = map(url_to_cars, get_data)\n  ) %&gt;% \n  unnest() %&gt;% \n  pivot_wider(names_from = \"x\", values_from = \"y\")\n\n# A tibble: 3 √ó 55\n  url_to_cars      ad_title `Alapt√≠pus √°ra:` `Extr√°kkal n√∂velt √°r:` `Akci√≥s √°r:`\n  &lt;chr&gt;            &lt;chr&gt;    &lt;chr&gt;            &lt;chr&gt;                  &lt;chr&gt;       \n1 https://www.has‚Ä¶ PEUGEOT‚Ä¶ 9¬†840¬†000¬†Ft     11¬†700¬†000¬†Ft          9¬†840¬†000¬†Ft\n2 https://www.has‚Ä¶ MERCEDE‚Ä¶ &lt;NA&gt;             &lt;NA&gt;                   &lt;NA&gt;        \n3 https://www.has‚Ä¶ MERCEDE‚Ä¶ &lt;NA&gt;             &lt;NA&gt;                   &lt;NA&gt;        \n# ‚Ñπ 50 more variables: `Akci√≥ felt√©telei:` &lt;chr&gt;, `V√©tel√°r EUR:` &lt;chr&gt;,\n#   `Finansz√≠roz√°s kalkul√°tor                    \\n                \\n            \\n        \\n    \\n            \\n            HIRDET√âS` &lt;chr&gt;,\n#   `√Åltal√°nos adatok` &lt;chr&gt;, `√Åtvehet≈ë:` &lt;chr&gt;, `Fut√°sid≈ë:` &lt;chr&gt;,\n#   `√âvj√°rat:` &lt;chr&gt;, `√Ållapot:` &lt;chr&gt;, `Kivitel:` &lt;chr&gt;,\n#   `Nemzetk√∂zi j√°rm≈±el≈ë√©let` &lt;chr&gt;,\n#   `Inform√°l√≥dj az alv√°zsz√°mr√≥l a nemzetk√∂zi aut√≥-el≈ë√©let szolg√°ltat√°s haszn√°lat√°hoz!    \\n    \\n    \\n        \\n            El≈ë√©let lek√©rdez√©se` &lt;chr&gt;,\n#   `Hazai j√°rm≈±el≈ë√©let` &lt;chr&gt;, ‚Ä¶\n\n\nThe last thing that causes this headache for this task is the special characters used in the Hungarian language. The janitor, on the other hand, handles this smoothly as well.\nWe can now download all the cars if we wish!\n\ncars_data_df &lt;- cars_add_df %&gt;% \n  mutate(\n    data = map(url_to_cars, get_data)\n  ) %&gt;% \n  unnest() %&gt;% \n  pivot_wider(names_from = \"x\", values_from = \"y\") %&gt;% \n  janitor::clean_names()"
  },
  {
    "objectID": "content/12-content.html#footnotes",
    "href": "content/12-content.html#footnotes",
    "title": "Web scraping",
    "section": "Footnotes",
    "text": "Footnotes\n\nYou can also use it on lists and vectors.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/13-content.html",
    "href": "content/13-content.html",
    "title": "Clustering",
    "section": "",
    "text": "Cluster analysis is a powerful technique used to group similar observations together based on their characteristics and relationships. The goal of clustering is to identify patterns and structures within a dataset, allowing us to gain insights and make meaningful interpretations. By dividing objects into sets of clusters, we can uncover hidden similarities and differences, enabling us to understand the underlying structure of the data.\nOne of the key ideas in clustering is that objects within a cluster should be similar or related to each other, while being different or unrelated to objects in other clusters. This notion of similarity and dissimilarity forms the basis for grouping objects together. The greater the similarity within a cluster and the greater the difference between clusters, the more distinct and meaningful the clustering becomes.\nHowever, determining what constitutes a cluster can be challenging. The definition of a cluster is not always well-defined and can vary depending on the nature of the data and the desired results. Different clustering techniques exist, each with its own approach to dividing objects into clusters. Some techniques focus on finding central points or prototypes, while others consider the density or connectivity of the data. Ultimately, the choice of clustering method depends on the specific goals and characteristics of the dataset.\nIn this chapter, we will explore the different types of clustering techniques and how they can be applied to real-world problems. We will also discuss the advantages and disadvantages of each method and how to evaluate the results. Let‚Äôs look at some practical examples of clustering in action:\nAn example from our current researches: Daily inflation for used vehicles in Hungary"
  },
  {
    "objectID": "content/13-content.html#hierarchical-clustering",
    "href": "content/13-content.html#hierarchical-clustering",
    "title": "Clustering",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nHierarchical clustering is a clustering technique that aims to create a hierarchical structure of clusters in a dataset. It produces a set of nested clusters organized in the form of a tree-like diagram called a dendrogram. This dendrogram represents the sequences of merges or splits of clusters.\n\nThere are two main types of hierarchical clustering: agglomerative and divisive. Agglomerative hierarchical clustering starts with each data point as an individual cluster and then iteratively merges the closest pair of clusters at each step (you will mainly meet with this approach). This process continues until all data points are merged into a single cluster or until a desired number of clusters is reached. Divisive hierarchical clustering, on the other hand, starts with one cluster containing all data points and then splits clusters at each step until each cluster contains only one data point or until a desired number of clusters is obtained.\n\n\n\n\n\n\nStep-by-step\n\n\n\n\nCompute the proximity matrix\nLet each data point be a cluster\nRepeat\nMerge the two closest clusters\nUpdate the proximity matrix\nUntil only a single cluster remains\n\n\n\nHierarchical clustering does not require specifying the number of clusters in advance, allowing for flexibility in the analysis. It can be visualized using a dendrogram, which displays the cluster-subcluster relationships and the distances between clusters. By ‚Äúcutting‚Äù the dendrogram at a certain level, any desired number of clusters can be obtained.\nOne advantage of hierarchical clustering is that it can capture both global and local structures in the data. It can reveal meaningful taxonomies or hierarchical relationships among the data points. Additionally, hierarchical clustering can be used with various proximity measures, allowing for flexibility in defining cluster similarity.\nThe choice of proximity measure in clustering depends on the type of data being analyzed. Here are some commonly used proximity measures:\nProximity\n\nEuclidean Distance (L2 Distance): The Euclidean distance is a popular proximity measure for numerical data in Euclidean space. It calculates the straight-line distance between two data points in a multidimensional space. The Euclidean distance between two points, x and y, in a d-dimensional space is given by the formula:\n\n\\[ \\sqrt{\\sum_{i=1}^{d}(x_i - y_i)^2} \\]\n\nManhattan Distance (L1 Distance): The Manhattan distance, also known as the city block distance or taxicab distance, is often used for numerical data. It measures the sum of the absolute differences between the coordinates of two points. The Manhattan distance between two points, x and y, in a d-dimensional space is given by the formula:\n\n\\[ \\sum_{i=1}^{d}\\|x_i - y_i\\| \\]\n\nJaccard Similarity: The Jaccard similarity is often used for binary or categorical data. It measures the size of the intersection divided by the size of the union of two sets. The Jaccard similarity between two sets, A and B, is given by the formula:\n\n\\[ \\frac{{|A \\cap B|}}{{|A \\cup B|}} \\]\nThis approach is particularly useful in applications such as document recommendation systems, plagiarism detection, or topic modeling, where understanding the similarity between documents is crucial for effective analysis and decision-making. The Jaccard similarity provides a straightforward and intuitive measure to quantify the similarity between sets of binary or categorical data, making it a valuable tool in various information retrieval and text mining tasks.\n\nGower distance: Gower distance is a proximity measure commonly used in clustering for datasets that contain a mix of numerical, categorical, and ordinal variables. It is particularly useful when dealing with heterogeneous data types. Gower distance takes into account the different scales and types of variables in the dataset.\n\nThe Gower distance is calculated as the weighted sum of the absolute differences between variables, normalized by the range of each variable. The formula for calculating the Gower distance between two data points, x and y, is as follows:\n\\[\n\\text{Gower distance}(x, y) = \\frac{\\sum_{i=1}^{n} w_i \\cdot d_i(x_i, y_i)}{\\sum_{i=1}^{n} w_i}\n\\]\nwhere n is the number of variables, \\(d_i(x_i, y_i)\\) is the distance or dissimilarity measure between the values of variable \\(i\\) for data points \\(x\\) and \\(y\\), and \\(w_i\\) is the weight assigned to variable \\(i\\). The weights can be used to give more importance to certain variables in the calculation of the distance.\nThe Gower distance can handle different types of variables, such as numerical, categorical, and ordinal, by using appropriate distance measures for each variable type. For numerical variables, the absolute difference is typically used. For categorical variables, the distance is 0 if the categories are the same and 1 if they are different. For ordinal variables, a suitable distance measure that considers the order or ranking of the categories is used.\nBy incorporating the characteristics of different variable types, the Gower distance provides a comprehensive measure of dissimilarity that can effectively handle mixed data types in clustering analysis.\nThese are just a few examples of proximity measures commonly used in clustering. The choice of measure depends on the nature of the data and the specific requirements of the clustering task.\nOverall, hierarchical clustering is a versatile and intuitive method for grouping data points into clusters based on their similarities, providing a hierarchical structure that can aid in understanding the relationships and structures within the data.\nMerging the closest clusters\nIn hierarchical clustering, the process of merging clusters is a key step in creating the hierarchical structure. One commonly used method for cluster merging is Ward‚Äôs method.\nWard‚Äôs method calculates the proximity between two clusters based on the increase in the squared error that occurs when the clusters are merged. The objective is to minimize the increase in the total within-cluster sum of squares (SSE) when merging clusters.\nTo perform cluster merging using Ward‚Äôs method, the following steps are typically followed:\n\nStart with each data point as an individual cluster.\nCalculate the proximity between all pairs of clusters. This can be done using a proximity measure such as Euclidean distance.\nMerge the two clusters that result in the smallest increase in the SSE when combined. This is determined by comparing the proximity values.\nUpdate the proximity matrix to reflect the merged cluster.\nRepeat steps 3 and 4 until all data points are in a single cluster or until a desired number of clusters is reached.\n\nWard‚Äôs method is known for its ability to handle clusters of different sizes and shapes. It aims to minimize the distortion caused by merging clusters and produces compact, well-separated clusters. However, it can be computationally expensive and may not be suitable for large datasets.\nIt‚Äôs important to note that there are other methods for cluster merging in hierarchical clustering, such as single-linkage, complete-linkage, and average-linkage. Each method uses a different criterion to determine the proximity between clusters and has its own strengths and weaknesses. The choice of merging method depends on the specific requirements of the clustering task and the characteristics of the data.\n\nlibrary(eurostat)\n\n# GDP per capita in PPS (Purchasing Power Standards)\ngdp_per_capita &lt;- get_eurostat(\"tec00114\") %&gt;% \n  select(geo, time, gdp_per_capita = values)\n\n# Unemployment rate\nunemployment_rate &lt;- get_eurostat(\"tesem140\") %&gt;% \n  filter(sex == \"T\") %&gt;% \n  select(geo, time, unemployment_rate = values)\n\n# Life expectancy at birth\nlife_expectancy &lt;- get_eurostat(\"sdg_03_10\") %&gt;% \n  filter(sex == \"T\") %&gt;% \n  select(geo, time, life_expectancy = values)\n\n# R&D index - Human Resources in Science and Technology (HRST)\neducation_index &lt;- get_eurostat(\"hrst_st_rcat\") %&gt;% \n  filter(str_length(geo) == 2, category == \"HRST\", unit == \"PC_ACT\") %&gt;% \n  select(geo, time, education_index = values)\n\n# Gini coefficient - measure of income inequality\ngini_coefficient &lt;- get_eurostat(\"ilc_di12\") %&gt;% \n  select(geo, time, gini = values)\n\n\n# Government debt as a percentage of GDP\npublic_debt &lt;- get_eurostat(\"gov_10dd_edpt1\") %&gt;% \n  filter(str_length(geo) == 2, unit == \"MIO_EUR\", sector == \"S13\", na_item == \"B9\") %&gt;% \n  select(geo, time, public_debt = values)\n\n# Join the datasets together on the 'geo' and 'time' columns\neu_indicators &lt;- gdp_per_capita %&gt;%\n  inner_join(unemployment_rate, by = c(\"geo\", \"time\")) %&gt;%\n  inner_join(life_expectancy, by = c(\"geo\", \"time\")) %&gt;%\n  inner_join(education_index, by = c(\"geo\", \"time\")) %&gt;% \n  inner_join(gini_coefficient, by = c(\"geo\", \"time\")) %&gt;%\n  inner_join(public_debt, by = c(\"geo\", \"time\")) %&gt;% \n  filter(time == max(time)) %&gt;%\n  select(- time)\n\neu_indicators %&gt;% \n  gt() %&gt;% \n  gt_finalise()\n\n\n\n\n\n\ngeo\n      gdp_per_capita\n      unemployment_rate\n      life_expectancy\n      education_index\n      gini\n      public_debt\n    \n\n\nAT\n125\n9.5\n81.1\n51.6\n27.8\n-15839.8\n\n\nBE\n120\n16.4\n81.8\n57.1\n24.9\n-19608.9\n\n\nBG\n59\n10.7\n74.3\n37.2\n38.4\n-2484.6\n\n\nCY\n92\n18.6\n81.7\n53.7\n29.4\n676.5\n\n\nCZ\n91\n6.8\n79.1\n41.1\n24.8\n-8871.3\n\n\nDE\n117\n6.0\n80.7\n50.3\n28.8\n-96910.0\n\n\nDK\n137\n10.6\n81.3\n53.7\n27.7\n12722.0\n\n\nEE\n87\n18.6\n78.2\n52.9\n31.9\n-348.6\n\n\nEL\n68\n31.4\n80.7\n41.9\n31.4\n-4892.0\n\n\nES\n85\n29.8\n83.2\n47.8\n32.0\n-63736.0\n\n\nFI\n109\n14.2\n81.2\n53.0\n26.6\n-2097.0\n\n\nFR\n102\n17.3\n82.3\n53.9\n29.8\n-126795.9\n\n\nHR\n73\n18.0\n77.7\n37.0\n28.5\n75.7\n\n\nHU\n77\n10.6\n76.2\n41.0\n27.4\n-10544.7\n\n\nIT\n96\n23.7\n83.0\n36.3\n32.7\n-156442.0\n\n\nLT\n89\n11.9\n76.0\n54.4\n36.2\n-439.9\n\n\nLU\n261\n17.6\n83.0\n66.9\n29.5\n-222.3\n\n\nLV\n74\n15.3\n74.8\n48.4\n34.3\n-1789.3\n\n\nMT\n102\n8.3\n82.7\n46.7\n31.1\n-982.2\n\n\nNL\n129\n7.6\n81.7\n57.5\n26.3\n-898.0\n\n\nPL\n80\n10.8\n77.4\n45.9\n26.3\n-24078.2\n\n\nPT\n77\n19.0\n81.7\n43.0\n32.0\n-779.1\n\n\nRO\n77\n22.8\n75.3\n30.2\n32.0\n-17889.5\n\n\nSE\n120\n21.7\n83.1\n58.5\n27.6\n6225.2\n\n\nSI\n92\n10.1\n81.3\n52.8\n23.1\n-1717.2\n\n\nSK\n68\n19.9\n77.2\n42.0\n21.2\n-2215.6\n\n\n\n\n\n\n\neu_indicators %&gt;% \n  column_to_rownames(\"geo\") %&gt;%\n  dist() %&gt;% \n  hclust() %&gt;% \n  plot()\n\n\n\n\nWhat is the result? For example, France is close to Italy. Why? In terms of educational index, they are very far apart. Germany is most similar to Spain, but they are very far apart in terms of GDP per capita and unemployment. However, these countries are similar in terms of public debt! If distance is a simple Euclidean distance, then greater variability in absolute value distorts the analysis. Therefore, it is worth eliminating the units of measurement. One possible solution for this is standardization, which means subtracting the mean and dividing the standard deviations of the individual variables.\n\neu_indicators %&gt;% \n  column_to_rownames(\"geo\") %&gt;%\n  scale() %&gt;% # &lt;- Standardization\n  dist() %&gt;% \n  hclust() %&gt;% \n  plot()\n\n\n\n\nNow the results are different. France is close to Germany, and CEE countries to each other."
  },
  {
    "objectID": "content/13-content.html#k-means-clustering",
    "href": "content/13-content.html#k-means-clustering",
    "title": "Clustering",
    "section": "K-means clustering",
    "text": "K-means clustering\nK-means clustering is a popular algorithm used for partitioning a dataset into K distinct clusters. It is a partitional clustering approach, where each cluster is represented by a centroid, which is the center point of the cluster. The algorithm assigns each data point to the cluster with the closest centroid based on a distance measure, typically the Euclidean distance.\n\n\n\n\n\n\nStep-by-step\n\n\n\n\nInitialization: Randomly select K data points as the initial centroids or use a different initialization method. These initial centroids represent the initial cluster centers.\nAssignment: Assign each data point to the cluster with the nearest centroid based on the chosen distance measure. This step forms K clusters.\nUpdate: Recalculate the centroids of each cluster by taking the mean of the data points assigned to that cluster. The centroids represent the new cluster centers.\nIteration: Repeat steps 2 and 3 until convergence is reached. Convergence occurs when the centroids no longer change significantly or when a maximum number of iterations is reached.\n\n\n\n\nThe algorithm aims to minimize the within-cluster sum of squares (WCSS), which represents the sum of squared distances between each data point and its assigned centroid. By minimizing the WCSS, K-means tries to create compact and well-separated clusters.\n\nlibrary(palmerpenguins)\n\nfit &lt;- palmerpenguins::penguins %&gt;% \n  select(where(is.numeric)) %&gt;%\n  na.omit() %&gt;% \n  select(- year) %&gt;% \n  scale() %&gt;% \n  kmeans(centers = 3)\n\nfit\n\nK-means clustering with 3 clusters of sizes 218, 58, 66\n\nCluster means:\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1     -0.3770667     0.6114797        -0.6558620  -0.6185144\n2      1.0823361    -0.6719851         1.4786030   1.6279890\n3      0.2943188    -1.4292037         0.8669538   0.6123148\n\nClustering vector:\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[149] 1 1 1 3 2 3 2 2 3 3 2 3 3 3 2 3 2 3 2 3 2 3 2 2 3 3 3 3 3 3 2 3 2 2 3 3 2\n[186] 2 2 3 2 3 2 3 2 3 3 2 3 3 2 3 2 3 2 3 2 3 3 3 3 3 2 3 2 3 2 3 2 3 2 3 2 3\n[223] 2 2 3 3 2 3 2 3 2 3 2 3 2 3 2 3 2 3 2 3 2 3 2 3 2 2 3 3 2 3 2 3 2 3 2 3 2\n[260] 3 2 2 2 3 2 3 2 3 2 3 3 2 3 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[297] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[334] 1 1 1 1 1 1 1 1 1\n\nWithin cluster sum of squares by cluster:\n[1] 412.53194  45.20432  27.04454\n (between_SS / total_SS =  64.5 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\npalmerpenguins::penguins %&gt;% \n  select(where(is.numeric)) %&gt;%\n  na.omit() %&gt;% \n  mutate(cluster = fit$cluster) %&gt;% \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = factor(cluster))) +\n  geom_point()\n\n\n\n\nIt‚Äôs important to note that K-means requires the number of clusters, K, to be specified in advance. Choosing an appropriate value for K can be determined through domain knowledge or by using techniques like the elbow method or silhouette analysis.\n\n# Compute and plot wss for k = 1 to k = 15\nwss &lt;- map_dbl(1:15, function(k) {\n  palmerpenguins::penguins %&gt;% \n  select(where(is.numeric)) %&gt;%\n  na.omit() %&gt;% \n  kmeans(centers = k) %&gt;% \n  pluck(\"tot.withinss\")\n})\n\nelbow_plot &lt;- data.frame(k = 1:15, wss = wss)\n\nggplot(elbow_plot, aes(x = k, y = wss)) +\n  geom_line() +\n  geom_point() +\n  ggtitle(\"Elbow Method for Determining Optimal k\") +\n  xlab(\"Number of Clusters\") +\n  ylab(\"Within-cluster Sum of Squares\")\n\n\n\n\nK-means clustering is widely used in various fields, including data mining, image segmentation, customer segmentation, and pattern recognition. It is relatively efficient and easy to implement, making it a popular choice for clustering tasks. However, it has limitations, such as sensitivity to initial centroid selection and the assumption of spherical-shaped clusters.\nSilhouette Indicator\nThe silhouette indicator is a metric used to evaluate the quality of clusters formed by clustering algorithms. This technique measures how similar each object in a cluster is to the objects in its own cluster compared to the nearest neighboring cluster. The silhouette indicator ranges from -1 to 1, where a high value indicates that the object is well-matched to its own cluster and poorly matched to neighboring clusters. Conversely, a low value suggests that the object is close to the boundary of its cluster and may be wrongly classified (Rousseeuw, 1987).\nFor each sample \\(i\\), the silhouette value \\(s(i)\\) is computed as:\n\\[\ns(i)=\\frac{b(i)‚àía(i)}{max({a(i),b(i))}}\n\\]\nWhere:\n\n\\(a(i)\\) is the average distance from the ith sample to the other samples in the same cluster.\n\\(b(i)\\) is the smallest average distance from the ith sample to the samples in the other clusters, minimized over clusters.\nInterpretation\n\ns(i)‚âà1: The sample is well clustered.\ns(i)‚âà0: The sample is close to the decision boundary between two clusters.\ns(i)‚âà‚àí1: The sample is incorrectly clustered.\n\nThe mean \\(s(i)\\) over all points of a cluster is a measure of how tightly grouped all the points in the cluster are. Thus the mean s(i) over all data of the entire dataset is a measure of how appropriately the data have been clustered."
  },
  {
    "objectID": "content/13-content.html#alternatives",
    "href": "content/13-content.html#alternatives",
    "title": "Clustering",
    "section": "Alternatives",
    "text": "Alternatives\nK-medoids\n\n\n\n\n\n\nStep-by-step\n\n\n\n\nInitialization: Randomly select k: data points (not centroids) to serve as the initial medoids.\nAssignment: Assign each data point to the nearest medoid based on a chosen distance metric, such as Euclidean, Manhattan, or more domain-specific metrics.\n\nUpdate: For each medoid m and each non-medoid data point o:\n\nSwap m and o, and compute the total cost (i.e., the sum of distances from all points to their nearest medoid).\nIf the total cost of the configuration decreases, accept the swap and update the medoid to o.\n\n\nConvergence Check: Repeat the Assignment and Update steps until the medoids no longer change or a maximum number of iterations is reached.\n\n\n\nK-proto\n\n\n\n\n\n\nStep-by-step\n\n\n\n\nInitialization: Select K initial prototypes, one for each cluster. Each prototype is a point in the feature space and has both numerical and categorical attributes.\nAssignment: Assign each data point to the nearest prototype. The distance to a prototype is calculated as a weighted sum of the Euclidean distance for numerical attributes and a simple matching dissimilarity measure for categorical attributes. \\(D(x, y) = a √ó D_{Euclidean} (2_{num}, Y_{num}) + (1 - a) √ó Categorical (2_{cat} &gt; Y_{cat})\\) where a is a weighting factor between O and 1, \\(num\\) and \\(Y_{num}\\) are the numerical attributes, and cat and \\(Y_{cat}\\) are the categorical attributes.\nUpdate: Recalculate the prototypes for each cluster as the point that minimizes the sum of distances to all points in the cluster. For numerical attributes, this is the mean, and for categorical attributes, this is the mode.\nConvergence Check: If the prototypes do not change significantly or a maximum number of iterations is reached, stop the algorithm. Otherwise, go back to the Assignment step."
  },
  {
    "objectID": "content/14-content.html",
    "href": "content/14-content.html",
    "title": "Principle component analysis",
    "section": "",
    "text": "Given are multiple variables that correlate with each other. The aim of principal component analysis is to compress correlated variables into uncorrelated principal components. Principal components are expressed as linear combinations of standardized variables. We retain a reduced number of variables compared to the original dataset (dimension reduction), while maximizing the explained variance for a given dimensionality.\nIt is quite simple (and less useful) to demonstrate the matter in two dimensions. Let us consider the simplest cars table, in which the speed of the car and the stopping time are recorded. Let us colorize it according to the order of the measurement, so that we can track where each one has ended up.\n\ncars %&gt;% \n  mutate(r = row_number()) %&gt;% \n  ggplot() + \n  aes(speed, dist, color = r) +\n  geom_point() \n\n\n\n\nIt is evident that the two variables strongly co-move, thus the first dimension would be the speed at which we are referring to (essentially the vicinity of the trend line), while the second dimension would be the deviation from it (in terms of how quickly it moved and how much it is below/above the trend).\n\n# Apply PCA using the prcomp function\npca_result &lt;- prcomp(cars, scale. = TRUE)\n\n\npca_result$x %&gt;% \n  as_tibble() %&gt;% \n  mutate(r = row_number()) %&gt;% \n  ggplot() + \n  aes(PC1, PC2, color = r) +\n  geom_point()"
  },
  {
    "objectID": "content/14-content.html#what",
    "href": "content/14-content.html#what",
    "title": "Principle component analysis",
    "section": "",
    "text": "Given are multiple variables that correlate with each other. The aim of principal component analysis is to compress correlated variables into uncorrelated principal components. Principal components are expressed as linear combinations of standardized variables. We retain a reduced number of variables compared to the original dataset (dimension reduction), while maximizing the explained variance for a given dimensionality.\nIt is quite simple (and less useful) to demonstrate the matter in two dimensions. Let us consider the simplest cars table, in which the speed of the car and the stopping time are recorded. Let us colorize it according to the order of the measurement, so that we can track where each one has ended up.\n\ncars %&gt;% \n  mutate(r = row_number()) %&gt;% \n  ggplot() + \n  aes(speed, dist, color = r) +\n  geom_point() \n\n\n\n\nIt is evident that the two variables strongly co-move, thus the first dimension would be the speed at which we are referring to (essentially the vicinity of the trend line), while the second dimension would be the deviation from it (in terms of how quickly it moved and how much it is below/above the trend).\n\n# Apply PCA using the prcomp function\npca_result &lt;- prcomp(cars, scale. = TRUE)\n\n\npca_result$x %&gt;% \n  as_tibble() %&gt;% \n  mutate(r = row_number()) %&gt;% \n  ggplot() + \n  aes(PC1, PC2, color = r) +\n  geom_point()"
  },
  {
    "objectID": "content/14-content.html#how",
    "href": "content/14-content.html#how",
    "title": "Principle component analysis",
    "section": "How?",
    "text": "How?\nEasy‚Ä¶ The principal components are the eigenvectors of the covariance matrix of the variables. The eigenvalues are the variances of the principal components. The first principal component is the eigenvector with the largest eigenvalue, the second principal component is the eigenvector with the second largest eigenvalue, and so on. The first principal component explains the largest amount of variance in the data, the second principal component explains the second most variance, and so on. The principal components are orthogonal to each other.\nOkay‚Ä¶\nEigenvalues / Eigenvectors\nIn linear algebra, for a given square matrix \\(A\\), an eigenvector \\(\\nu\\) is a non-zero vector that, when multiplied by \\(A\\), results in a scalar multiple of itself. This scalar is known as the eigenvalue \\(\\lambda\\). Mathematically, this relationship is expressed as:\n\\[A‚ãÖ\\nu=\\lambda‚ãÖ\\nu\\]\n\nA &lt;- matrix(c(2, 0, 0,\n              0, 3, 0,\n              0, 0, 4), nrow = 3, byrow = TRUE)\n\nA\n\n     [,1] [,2] [,3]\n[1,]    2    0    0\n[2,]    0    3    0\n[3,]    0    0    4\n\n# Calculate eigenvalues and eigenvectors\neigen_result &lt;- eigen(A)\n\neigen_result\n\neigen() decomposition\n$values\n[1] 4 3 2\n\n$vectors\n     [,1] [,2] [,3]\n[1,]    0    0    1\n[2,]    0    1    0\n[3,]    1    0    0\n\n# Eigenvalues\neigen_values &lt;- eigen_result$values\n\n# Eigenvectors\neigen_vectors &lt;- eigen_result$vectors\n\nA %*% eigen_vectors[, 1] == # %*% matrix multiplication\n  eigen_values[1] * eigen_vectors[, 1]\n\n     [,1]\n[1,] TRUE\n[2,] TRUE\n[3,] TRUE\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPC are not a statistical model‚Ä¶ Its just a rotation of the axis to maximize the variance and represent the data with orthogonal features."
  },
  {
    "objectID": "content/14-content.html#why",
    "href": "content/14-content.html#why",
    "title": "Principle component analysis",
    "section": "Why?",
    "text": "Why?\nUsing PCA or other dimension reduction tool might be motivated by several reasons:\n\nPattern recognition: we want to identify the underlying structure of the variables. An example to this is the analysis of the yield curve, where we want to identify the co-movement present in the data. We can also create latent variables (the PCA scores) that are not directly observable, but they can be inferred from the data (example: Digitalisation and business performance - focusing on Hungarian manufacturing firms).\nDimension reduction: we want to reduce the number of variables in the dataset, while retaining as much information as possible. This is useful when we have a large number of variables and we are not able to interpret the results, we face time or space complexity (the model estimation takes to much time or memory) issues, or multicollinearity (the variables are highly correlated with each other).\nNoise reduction: we want to reduce the noise present in the data. Consider using dimension reduction if the out-of-sample estimation is not as good as the in-sample estimation and you have many variables.\nVisualization: we want to visualize the data in a lower dimensional space. This is useful when we have a large number of variables and we want to visualize the data in a 2D or 3D space (for instance biplots).\n\n\nlibrary(rvest)\n\nyields_df &lt;- map_dfr(1:25, .progress = TRUE, ~ {\n  read_html(str_c(\"https://home.treasury.gov/resource-center/data-chart-center/interest-rates/TextView?type=daily_treasury_yield_curve&field_tdr_date_value=all&page=\", .x)) %&gt;% \n    html_table() %&gt;% \n    first() %&gt;%\n    select(\n      date = Date, matches(\"\\\\d Mo\"), matches(\"\\\\d\\\\d Mo\"), matches(\"\\\\d Yr\"), matches(\"\\\\d\\\\d Yr\"),\n      - contains(\"YR\", ignore.case = FALSE)\n    ) %&gt;% \n    mutate(\n      date = lubridate::mdy(date),\n      across(where(is.character), as.numeric)\n    )\n})\n\n\nyields_df %&gt;% \n  head() %&gt;%\n  mutate(across(-1, ~ . / 100)) %&gt;%\n  gt_finalise()\n\n\n\n\n\n\nDATE\n      1 MO\n      2 MO\n      3 MO\n      4 MO\n      6 MO\n      1 YR\n      2 YR\n      3 YR\n      5 YR\n      7 YR\n      10 yr\n      20 yr\n      30 yr\n    \n\n\n1991-03-15\nNA\nNA\n5.98%\nNA\n6.10%\n6.30%\n7.01%\n7.26%\n7.73%\n7.98%\n8.10%\nNA\n8.30%\n\n\n1991-03-18\nNA\nNA\n6.03%\nNA\n6.13%\n6.36%\n7.09%\n7.35%\n7.79%\n8.04%\n8.15%\nNA\n8.34%\n\n\n1991-03-19\nNA\nNA\n6.08%\nNA\n6.23%\n6.48%\n7.22%\n7.48%\n7.93%\n8.14%\n8.25%\nNA\n8.42%\n\n\n1991-03-20\nNA\nNA\n6.07%\nNA\n6.18%\n6.44%\n7.20%\n7.46%\n7.88%\n8.09%\n8.20%\nNA\n8.37%\n\n\n1991-03-21\nNA\nNA\n6.06%\nNA\n6.15%\n6.39%\n7.13%\n7.40%\n7.82%\n8.04%\n8.16%\nNA\n8.34%\n\n\n1991-03-22\nNA\nNA\n6.06%\nNA\n6.16%\n6.37%\n7.13%\n7.39%\n7.82%\n8.03%\n8.13%\nNA\n8.33%\n\n\n\n\n\n\n\nyields_df %&gt;% \n  select(- date) %&gt;%\n  cor() %&gt;% \n  corrplot::corrplot()\n\n\n\n\n\nyields_df %&gt;% \n  select(- date) %&gt;% \n  select(where(~ any(!is.na(.x)))) %&gt;%\n  drop_na() %&gt;%\n  princomp() %&gt;% \n  loadings() %&gt;% \n  unclass() %&gt;% \n  data.frame() %&gt;%\n  rownames_to_column(\"variable\") %&gt;%\n  select(1:5) %&gt;% \n  gt_finalise() %&gt;% \n  fmt_number(\n    -1,\n    force_sign = TRUE,\n    decimals = 2\n  ) %&gt;%\n  data_color(\n    columns = 3, target_columns = 3,\n    colors = c(\"red3\", \"steelblue\")\n  )\n\n\n\n\n\n\n\nLoadings of the first 4 principle components of the yield curve\n    \n\nVariable\n      Comp 1\n      Comp 2\n      Comp 3\n      Comp 4\n    \n\n\n\n1 Mo\n+0.32\n+0.38\n+0.26\n+0.45\n\n\n2 Mo\n+0.32\n+0.35\n+0.21\n+0.21\n\n\n3 Mo\n+0.32\n+0.31\n+0.18\n‚àí0.08\n\n\n6 Mo\n+0.32\n+0.23\n+0.04\n‚àí0.38\n\n\n1 Yr\n+0.32\n+0.10\n‚àí0.18\n‚àí0.57\n\n\n2 Yr\n+0.31\n‚àí0.05\n‚àí0.43\n‚àí0.09\n\n\n3 Yr\n+0.31\n‚àí0.12\n‚àí0.45\n+0.10\n\n\n5 Yr\n+0.29\n‚àí0.22\n‚àí0.26\n+0.27\n\n\n7 Yr\n+0.27\n‚àí0.29\n‚àí0.08\n+0.33\n\n\n10 Yr\n+0.25\n‚àí0.36\n+0.16\n+0.08\n\n\n20 Yr\n+0.21\n‚àí0.40\n+0.39\n‚àí0.05\n\n\n30 Yr\n+0.20\n‚àí0.36\n+0.44\n‚àí0.28"
  },
  {
    "objectID": "content/14-content.html#spotiyf-api",
    "href": "content/14-content.html#spotiyf-api",
    "title": "Principle component analysis",
    "section": "Spotiyf API",
    "text": "Spotiyf API\nIn the following section, we will be utilizing the Spotify API service to retrieve data on some Hungarian artists. But what is an API? An API is a service that allows an application to communicate with other applications. APIs can be private, accessible only to a specific application, or public, accessible to anyone. The Spotify API is public, thus accessible to anyone. To use APIs, it is generally necessary to have an API key provided by the service provider. We can set the API key in our code using the Sys.setenv() function. The Spotify API key can be obtained from the Spotify for Developers website. After obtaining the key, we can set it as an environmental variable using the following code.\n\nlibrary(spotifyr)\nSys.setenv(SPOTIFY_CLIENT_ID = 'xxx')\nSys.setenv(SPOTIFY_CLIENT_SECRET = 'xxx')\n\nThe functions Sys.setenv and Sys.getenv enable us to set and retrieve environmental variables. The advantage of this is that within a given project, our private keys (login credentials) can be stored in a file (.Renv).\n\nSys.setenv(mypassword = \"password\") # you won't include this in your code\nSys.getenv(\"mypassword\")\n\n[1] \"password\"\n\n\n\nlibrary(spotifyr)\n\nThe plan is to analyze the numbers of the most listened artists based on the weekly top list on YouTube, and their respective characteristics that we can obtain through the Spotify API. The definition of the variables can be accessed here: https://developer.spotify.com/documentation/web-api/reference/get-audio-features.\n\nhun_music_df &lt;- c(\"Azahriah\", \"Desh\", \"Valmar\", \"Young Fly\", \"Bruno x Spaco\", \"T. Danny\",\n                  \"KKevin\", \"ByeAlex √©s a Slepp\", \"L.L. Junior\", \"BSW\", \"ekhoe\", \"Dzs√∫dl√≥\",\n                  \"Follow The Flow\", \"Roland\", \"N√≥t√°r Mary\", \"Tomi\", \"G.w.M\", \"Majka\") %&gt;% \n  map_dfr(get_artist_audio_features, .progress = TRUE, include_groups = \"single\")\n\n\nhun_music_df &lt;- hun_music_df %&gt;% \n  group_by((row_number() -1) %/% 50) %&gt;%\n  group_split() %&gt;% \n  map(pull, track_id) %&gt;% \n  map_dfr(spotifyr::get_tracks, .progress = \"popularity\") %&gt;% \n  select(popularity) %&gt;% \n  bind_cols(hun_music_df, .)\n\n\nhun_music_df %&gt;% \n  glimpse()\n\nRows: 721\nColumns: 40\n$ artist_name                  &lt;chr&gt; \"Azahriah\", \"Azahriah\", \"Azahriah\", \"Azah‚Ä¶\n$ artist_id                    &lt;chr&gt; \"6EIriUxo7vznEgJtTDlXpq\", \"6EIriUxo7vznEg‚Ä¶\n$ album_id                     &lt;chr&gt; \"1bP0ww7Dq2vnYkzryXtet8\", \"6ZO1hIeCfQ8nea‚Ä¶\n$ album_type                   &lt;chr&gt; \"single\", \"single\", \"single\", \"single\", \"‚Ä¶\n$ album_images                 &lt;list&gt; [&lt;data.frame[3 x 3]&gt;], [&lt;data.frame[3 x ‚Ä¶\n$ album_release_date           &lt;chr&gt; \"2023-09-26\", \"2023-08-24\", \"2023-08-24\",‚Ä¶\n$ album_release_year           &lt;dbl&gt; 2023, 2023, 2023, 2023, 2023, 2023, 2023,‚Ä¶\n$ album_release_date_precision &lt;chr&gt; \"day\", \"day\", \"day\", \"day\", \"day\", \"day\",‚Ä¶\n$ danceability                 &lt;dbl&gt; 0.615, 0.678, 0.668, 0.711, 0.810, 0.676,‚Ä¶\n$ energy                       &lt;dbl&gt; 0.6900, 0.6250, 0.6770, 0.6910, 0.6740, 0‚Ä¶\n$ key                          &lt;int&gt; 0, 1, 3, 4, 11, 9, 7, 8, 1, 10, 2, 2, 9, ‚Ä¶\n$ loudness                     &lt;dbl&gt; -7.392, -8.855, -8.724, -8.622, -7.816, -‚Ä¶\n$ mode                         &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,‚Ä¶\n$ speechiness                  &lt;dbl&gt; 0.1430, 0.1200, 0.2360, 0.0459, 0.1440, 0‚Ä¶\n$ acousticness                 &lt;dbl&gt; 0.5990, 0.4960, 0.3040, 0.7150, 0.7010, 0‚Ä¶\n$ instrumentalness             &lt;dbl&gt; 7.82e-04, 1.68e-03, 3.24e-03, 1.50e-01, 3‚Ä¶\n$ liveness                     &lt;dbl&gt; 0.1810, 0.2950, 0.1190, 0.2540, 0.0950, 0‚Ä¶\n$ valence                      &lt;dbl&gt; 0.2960, 0.3610, 0.5810, 0.6560, 0.2420, 0‚Ä¶\n$ tempo                        &lt;dbl&gt; 175.892, 109.980, 163.934, 98.002, 103.09‚Ä¶\n$ track_id                     &lt;chr&gt; \"4MQDC8hquSlvTBq8gMoPPo\", \"2IetMd5ZFurl1s‚Ä¶\n$ analysis_url                 &lt;chr&gt; \"https://api.spotify.com/v1/audio-analysi‚Ä¶\n$ time_signature               &lt;int&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,‚Ä¶\n$ artists                      &lt;list&gt; [&lt;data.frame[1 x 6]&gt;], [&lt;data.frame[2 x ‚Ä¶\n$ available_markets            &lt;list&gt; &lt;\"AR\", \"AU\", \"AT\", \"BE\", \"BO\", \"BR\", \"BG‚Ä¶\n$ disc_number                  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶\n$ duration_ms                  &lt;int&gt; 220227, 273818, 158048, 171428, 125825, 3‚Ä¶\n$ explicit                     &lt;lgl&gt; FALSE, TRUE, TRUE, FALSE, TRUE, FALSE, FA‚Ä¶\n$ track_href                   &lt;chr&gt; \"https://api.spotify.com/v1/tracks/4MQDC8‚Ä¶\n$ is_local                     &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,‚Ä¶\n$ track_name                   &lt;chr&gt; \"valami amerikai\", \"cerem√≥nia\", \"vicc\", \"‚Ä¶\n$ track_preview_url            &lt;chr&gt; \"https://p.scdn.co/mp3-preview/0ef5d31ad9‚Ä¶\n$ track_number                 &lt;int&gt; 1, 1, 2, 3, 4, 5, 1, 1, 1, 1, 2, 3, 4, 5,‚Ä¶\n$ type                         &lt;chr&gt; \"track\", \"track\", \"track\", \"track\", \"trac‚Ä¶\n$ track_uri                    &lt;chr&gt; \"spotify:track:4MQDC8hquSlvTBq8gMoPPo\", \"‚Ä¶\n$ external_urls.spotify        &lt;chr&gt; \"https://open.spotify.com/track/4MQDC8hqu‚Ä¶\n$ album_name                   &lt;chr&gt; \"valami amerikai\", \"tripq\", \"tripq\", \"tri‚Ä¶\n$ key_name                     &lt;chr&gt; \"C\", \"C#\", \"D#\", \"E\", \"B\", \"A\", \"G\", \"G#\"‚Ä¶\n$ mode_name                    &lt;chr&gt; \"major\", \"minor\", \"minor\", \"minor\", \"mino‚Ä¶\n$ key_mode                     &lt;chr&gt; \"C major\", \"C# minor\", \"D# minor\", \"E min‚Ä¶\n$ popularity                   &lt;int&gt; 63, 66, 61, 64, 60, 61, 56, 50, 55, 48, 6‚Ä¶\n\n\n\nfit &lt;- hun_music_df %&gt;% \n  select(danceability:tempo, - key, - mode) %&gt;% \n  prcomp(scale. = TRUE)\n\n\nfactoextra::fviz_pca_var(fit)"
  },
  {
    "objectID": "content/14-content.html#biplot",
    "href": "content/14-content.html#biplot",
    "title": "Principle component analysis",
    "section": "Biplot",
    "text": "Biplot\nWhy not to combine the PCA with the clustering results? We can do this by creating a biplot.\n\ncluster_res &lt;- hun_music_df %&gt;% \n  select(danceability:tempo, - key, - mode) %&gt;% \n  scale() %&gt;% \n  kmeans(centers = 3, nstart = 10) %&gt;% \n  pluck(\"cluster\")\n\n\nfactoextra::fviz_pca_biplot(fit, habillage = cluster_res, geom.ind = c(\"point\")) +\n  ggrepel::geom_text_repel(\n    data = hun_music_df %&gt;% \n      bind_cols(fit$x) %&gt;% \n      group_by(artist_name) %&gt;% \n      sample_n(6),\n    aes(PC1, PC2, label = artist_name),\n  )"
  },
  {
    "objectID": "content/15-content.html",
    "href": "content/15-content.html",
    "title": "Algorithmic Modelling",
    "section": "",
    "text": "Data and Cultures in Economania\n My TEDx Talk\n Multivariate Adaptive Regression Splines by Jerome H. Friedman. This is the original paper that introduced MARS.\n Tidy Modeling with R\n\n\nIn the context of linear regression, we have examined a case where we have a clear understanding of how the value of the output variable follows from the explanatory variables: as a linear combination with some noise.\nOn the contrary, another approach in a different culture does not aim to describe the ‚Äúdata generating process‚Äù that relates x to y, but rather aims to create an algorithm that accurately estimates the target variable. This approach includes neural networks and tree-based models, which were considered innovative in the 1980s. These tools have advanced to a level by the early 21st century where they are capable of tasks such as speech recognition, image recognition, and predicting nonlinear time series. The key is to focus on giving ‚Äúgood predictions‚Äù.\nOkay‚Ä¶ How do we define ‚Äúgood predictions‚Äù?"
  },
  {
    "objectID": "content/15-content.html#decision-tree",
    "href": "content/15-content.html#decision-tree",
    "title": "Algorithmic Modelling",
    "section": "Decision Treeüå≤",
    "text": "Decision Treeüå≤\nDecision trees are a type of supervised learning algorithm that can be used for both regression and classification tasks. Their goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. Decision trees are easy to interpret and visualize, and they can easily capture nonlinear relationships. However, they are prone to overfitting.\n\ndata(attrition, package = \"modeldata\") \n\n\nlibrary(rpart)\n\nattrition_tree &lt;- attrition %&gt;% \n  mutate(Attrition = Attrition == \"Yes\") %&gt;% \n  rpart(formula = Attrition ~ .,  cp = .03)\n\n\nrattle::fancyRpartPlot(attrition_tree, palettes = 'PuRd', sub = NULL)\n\n\n\n\n\n\n\n\n\n\nCART\n\n\n\nThe aim is to minimize the average heterogeneity of the tree‚Äôs leaves, weighted by numbers. If the task is classification, then the heterogeneity of a leaf is measured by the Gini index, which is the probability that two randomly selected elements from the leaf are of different classes. The Gini index is defined as follows:\n\\[\nG = 1 - \\sum_{i=1}^{k} f_i^2\n\\] If the task is regression, then the heterogeneity of a leaf is measured by the Sum of Squared Error of the target variable. The variance is defined as follows:\n\\[\nSSE = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n\\]\n\n\nIf a predictor is categorical, then the tree will split the data into subsets based on the categories of the predictor. If a predictor is continuous, then the tree will split the data into subsets based on a deciles of the predictor.\nThe tree will continue to split the data until the stopping criteria is met. The stopping criteria can be defined by the minimum number of observations in a leaf node, the maximum depth of the tree, or the minimum improvement in the Gini index or Sum of Squared Error (complexity parameter, cp).\nLet‚Äôs decrease the complexity parameter to see how the tree changes.\n\nattrition %&gt;% \n  mutate(Attrition = Attrition == \"Yes\") %&gt;% \n  rpart(formula = Attrition ~ .,  cp = .021) %&gt;% \n  rattle::fancyRpartPlot(palettes = 'PuRd', sub = NULL)\n\n\n\n\nHow should we determine the complexity parameter? We can use cross-validation to determine the optimal complexity parameter for best predictions on a test set or if visualising the tree is the goal, we can try different values to draw a tree that highlights the splits that we find to be important, but the reader can still understand the tree.\n\n\n\n\n\n\nAdvantages of Decision Trees\n\n\n\n\nIntuitive Understanding: Decision trees provide a clear and easy-to-understand model structure, which makes them particularly useful for interpretability. They mimic human decision-making processes, making their logic transparent. I personally recommend their use for descriptive statistics. They possess a highly intuitive and articulate approach to describe the data (example).\nNo Need for Feature Scaling: These models don‚Äôt require feature normalization or standardization. They can handle varied scales of data effectively.\nHandle Both Types of Data: Capable of handling both numerical and categorical variables.\nNon-Parametric Method: As a non-parametric method, decision trees make no assumptions about the underlying distributions of the data, which is beneficial for analysis with less data preprocessing.\nCapability of Feature Selection: They inherently perform feature selection by prioritizing the most informative features at the top splits.\nEasy to Visualize: Their tree-like structure allows for easy visualization, aiding in understanding the decision-making process.\nCan Model Non-Linear Relationships: Capable of capturing non-linear relationships between features and labels.\n\n\n\n\n\n\n\n\n\nDisadvantages of Decision Trees\n\n\n\n\nOverfitting: Decision trees are prone to overfitting, especially with complex trees. They can become too tailored to the training data and fail to generalize well.\nInstability: Small variations in the data can result in a completely different tree structure. This lack of robustness can be a significant issue.\nNot Suitable for Extrapolation: Decision trees cannot extrapolate or make predictions outside the range of the training data.\nPoor Performance with Unbalanced Data: They can create biased trees if some classes dominate the dataset, leading to skewed decisions.\nDifficulty with Large Number of Classes: The performance and computation time can degrade with a large number of target classes."
  },
  {
    "objectID": "content/15-content.html#random-forest",
    "href": "content/15-content.html#random-forest",
    "title": "Algorithmic Modelling",
    "section": "Random Forest üå≤üå¥üå≥",
    "text": "Random Forest üå≤üå¥üå≥\n\n\n\n\nRandom forest is an ensemble learning method that combines multiple decision trees to create a ‚Äúforest‚Äù that can make more accurate predictions than a single decision tree. It can be used for both regression and classification tasks. Random forest is a bagging technique and uses bootstrap sampling to create multiple decision trees. It also uses random feature selection to create each decision tree. The final prediction is the average prediction of all the decision trees.\n\nlibrary(randomForest)\n\nattrition_rf &lt;- attrition %&gt;% \n  mutate(Attrition = Attrition == \"Yes\") %&gt;% \n  randomForest(formula = Attrition ~ ., ntree = 100, importance = TRUE, )\n\n\n# ROC\nlibrary(pROC)\nattrition %&gt;% \n  mutate(fitted = attrition_rf$votes[, 2]) %&gt;%\n  pROC::roc(Attrition, fitted) %&gt;% \n  ggroc() +\n  geom_abline(color = \"red\", intercept = 1)\n\n\n\n\n\nvip::vi(attrition_rf) %&gt;% \n  mutate(Variable = fct_reorder(Variable, Importance)) %&gt;% \n  ggplot() +\n  geom_col(aes(Importance, Variable)) +\n  geom_vline(xintercept = 0)\n\n\n\nVariable importance"
  },
  {
    "objectID": "content/15-content.html#mars",
    "href": "content/15-content.html#mars",
    "title": "Algorithmic Modelling",
    "section": "MARS üåé",
    "text": "MARS üåé\nMultivariate adaptive regression splines (MARS) is a non-parametric regression method that can be used for both regression and classification tasks. It is a type of regression that uses piecewise linear basis functions to model the data. It is similar to decision trees, but it uses linear combinations of basis functions instead of a tree structure. It is also similar to polynomial regression, but it uses piecewise linear functions instead of polynomials. MARS is a type of additive model, which means that it is a linear combination of basis functions. The basis functions are defined as follows:\n\\[\nh(x) = \\beta_0 + \\sum_{j=1}^{k} \\beta_j B_j(x)\n\\] where \\(B_j(x)\\) is a basis function. The basis functions are defined as follows:\n\\[\nB_j(x) = \\max(0, x - c_j)_+^p\n\\] where \\(c_j\\) is a knot and \\(p\\) is the degree of the basis function. The basis functions are piecewise linear functions with a knot at \\(c_j\\). The basis functions are also known as hinge functions. The basis functions are added to the model one at a time. The first basis function is a constant function. The second basis function is a constant function plus a hinge function. The third basis function is a constant function plus two hinge functions. The fourth basis function is a constant function plus three hinge functions. The process continues until the model reaches the maximum number of basis functions or the maximum number of basis functions that can be added without increasing the error. The model is then pruned to remove basis functions that do not improve the error.\n\nThe model starts from a null model.\nIn each step, it adds the new hinge function pair that minimizes \\(R^2\\) the most, where a hinge function is a variable pair \\(max(x - c, 0)\\) and \\(max(c - x, 0)\\), with \\(x\\) being the original variable and \\(c\\) being a threshold value.\nThis creates an overfitted model, so it eliminates the variables that prove to be redundant using Cross-validation method.\n\n\nfit &lt;- earth::earth(formula = MonthlyIncome ~ ., data = attrition, degree = 2)\n\n\nfit\n\nSelected 14 of 14 terms, and 7 of 59 predictors\nTermination condition: RSq changed by less than 0.001 at 14 terms\nImportance: JobLevel, JobRoleResearch_Director, JobRoleManager, ...\nNumber of terms at each degree of interaction: 1 6 7\nGCV 1067435    RSS 1498425944    GRSq 0.9518739    RSq 0.9539798\n\n\n\nsummary(fit)\n\nCall: earth(formula=MonthlyIncome~., data=attrition, degree=2)\n\n                                                      coefficients\n(Intercept)                                              5638.0557\nJobRoleManager                                           3295.9923\nJobRoleResearch_Director                                 3351.7660\nh(2-JobLevel)                                           -2765.9146\nh(JobLevel-2)                                            3496.8878\nh(6-TotalWorkingYears)                                   -607.0069\nh(TotalWorkingYears-6)                                     51.2191\nJobRoleLaboratory_Technician * h(TotalWorkingYears-6)    -199.8807\nJobRoleResearch_Scientist * h(TotalWorkingYears-6)       -141.6059\nJobRoleSales_Representative * h(TotalWorkingYears-6)     -201.7847\nh(2-JobLevel) * h(TotalWorkingYears-7)                    152.9745\nh(2-JobLevel) * h(7-TotalWorkingYears)                    397.6352\nh(JobLevel-2) * h(TotalWorkingYears-15)                   -28.8981\nh(JobLevel-2) * h(15-TotalWorkingYears)                  -121.2103\n\nSelected 14 of 14 terms, and 7 of 59 predictors\nTermination condition: RSq changed by less than 0.001 at 14 terms\nImportance: JobLevel, JobRoleResearch_Director, JobRoleManager, ...\nNumber of terms at each degree of interaction: 1 6 7\nGCV 1067435    RSS 1498425944    GRSq 0.9518739    RSq 0.9539798\n\n\nIt can be quite challenging to navigate through the meaning of these coefficients‚Ä¶ Let‚Äôs visualise the response over a range of predictor values. A solution can be found in the plotmo package. It keeps the value of every predictor at a constant (median or mode) and varies the value of the predictor of interest. The plot shows the predicted response as a function of the predictor of interest, with the other predictors held constant. It can also be used to visualize the relationship between the response and a pair of predictors.\n\nplotmo::plotmo(fit)\n\n plotmo grid:    Age Attrition BusinessTravel DailyRate           Department\n                  36        No  Travel_Rarely       802 Research_Development\n DistanceFromHome Education EducationField EnvironmentSatisfaction Gender\n                7  Bachelor  Life_Sciences                    High   Male\n HourlyRate JobInvolvement JobLevel         JobRole JobSatisfaction\n         66           High        2 Sales_Executive       Very_High\n MaritalStatus MonthlyRate NumCompaniesWorked OverTime PercentSalaryHike\n       Married       14236                  2       No                14\n PerformanceRating RelationshipSatisfaction StockOptionLevel TotalWorkingYears\n         Excellent                     High                1                10\n TrainingTimesLastYear WorkLifeBalance YearsAtCompany YearsInCurrentRole\n                     3          Better              5                  3\n YearsSinceLastPromotion YearsWithCurrManager\n                       1                    3\n\n\n\n\n\nAdditionally, calculating the importance of variables can aid in the interpretation of the model. This can be easily done using the vip package. However, in the case of the MARS model, the Variable Importance (VI) is calculated differently. By applying our advanced documentation reading skills, we can find the following:\n\nThe earth package uses three criteria for estimating the variable importance in a MARS model (see evimp for details):\nThe nsubsets criterion (type = ‚Äúnsubsets‚Äù) counts the number of model subsets that include each feature. Variables that are included in more subsets are considered more important. This is the criterion used by summary.earth to print variable importance. By ‚Äúsubsets‚Äù we mean the subsets of terms generated by earth()‚Äôs backward pass. There is one subset for each model size (from one to the size of the selected model) and the subset is the best set of terms for that model size. (These subsets are specified in the $prune.terms component of earth()‚Äôs return value.) Only subsets that are smaller than or equal in size to the final model are used for estimating variable importance. This is the default method used by vip.\nThe rss criterion (type = ‚Äúrss‚Äù) first calculates the decrease in the RSS for each subset relative to the previous subset during earth()‚Äôs backward pass. (For multiple response models, RSS‚Äôs are calculated over all responses.) Then for each variable it sums these decreases over all subsets that include the variable. Finally, for ease of interpretation the summed decreases are scaled so the largest summed decrease is 100. Variables which cause larger net decreases in the RSS are considered more important.\nThe gcv criterion (type = ‚Äúgcv‚Äù) is similar to the rss approach, but uses the GCV statistic instead of the RSS. Note that adding a variable can sometimes increase the GCV. (Adding the variable has a deleterious effect on the model, as measured in terms of its estimated predictive power on unseen data.) If that happens often enough, the variable can have a negative total importance, and thus appear less important than unused variables.\n\n\nvip::vi(fit, type = \"gcv\") %&gt;% \n  mutate(Variable = fct_reorder(Variable, Importance)) %&gt;% \n  ggplot() +\n  geom_col(aes(Importance, Variable)) +\n  geom_vline(xintercept = 0)\n\n\n\nVariable importance for MARS\n\n\n\nWe might see that the monthly income increases with the increasing the working years, but the relationship is not linear. The slope decreases after 6 years. This function can also be used for simple OLS models if it becomes complex (or you can develop a similar solution to any other model)\n\nlm(formula = MonthlyIncome ~ JobLevel + JobRole + TotalWorkingYears, data = attrition) %&gt;% \n  plotmo::plotmo()\n\n plotmo grid:    JobLevel         JobRole TotalWorkingYears\n                        2 Sales_Executive                10\n\n\n\n\n\nLet‚Äôs focus again on the hinge functions.\n\nfit &lt;- earth::earth(formula = MonthlyIncome ~ TotalWorkingYears, data = attrition, degree = 2)\n\nsummary(fit)\n\nCall: earth(formula=MonthlyIncome~TotalWorkingYears, data=attrition, degree=2)\n\n                        coefficients\n(Intercept)                7804.0544\nh(15-TotalWorkingYears)    -405.4382\nh(TotalWorkingYears-15)    -834.0226\nh(TotalWorkingYears-18)    3348.9434\nh(TotalWorkingYears-22)   -2482.4637\n\nSelected 5 of 6 terms, and 1 of 1 predictors\nTermination condition: RSq changed by less than 0.001 at 6 terms\nImportance: TotalWorkingYears\nNumber of terms at each degree of interaction: 1 4 (additive model)\nGCV 7487091    RSS 10841924236    GRSq 0.6624387    RSq 0.6670189\n\nplotmo::plotmo(fit)\n\n\n\n\n\n\n\n\n\n\n\nIt is worth noting here that the implemented algorithm always represents the hinge function as max(x-c, 0) or max(c-x, 0), and only displays the x-c or c-x part for the coefficients. Therefore, it follows that the value of the hinge function denoted as h(15-TotalWorkingYears) decreases as work experience increases. Hence, the corresponding negative coefficient implies that the expected salary decreases as work experience increases and is below 15 years."
  },
  {
    "objectID": "content/16-content.html",
    "href": "content/16-content.html",
    "title": "Tidy modelling",
    "section": "",
    "text": "Tidy Modeling with R\n Silge‚Äôs Youtube Channel & Blog\nIn the previous chapter, we observed that a machine learning tool, regardless of the chosen model, is built upon a very similar recipe. Firstly, we provide it with data, on which certain transformations may be performed beforehand. Then, we train a model on a given dataset, varying the hyperparameters, and evaluate its predictions on another dataset. Subsequently, we either utilize the model to make predictions, preferably the best ones, or examine which variable proved to be the most important based on certain metrics (we consider which approach was employed most frequently or which one significantly improved the predictive capability of the model)\nAs the Tidyverse contains a collection of packages that are generally useful and frequently used for reading, cleaning, and visualizing data, the tidymodels is a package collection that includes important tools for modelling, when we focus on prediction.\nThe tidymodels packages are designed to work together, and they are built on top of the Tidyverse. The packages are designed to be modular, so that you can easily mix and match them to suit your needs.\nhttps://www.tidymodels.org/packages/\nLet us proceed step by step‚Ä¶"
  },
  {
    "objectID": "content/16-content.html#splitting-resamples",
    "href": "content/16-content.html#splitting-resamples",
    "title": "Tidy modelling",
    "section": "Splitting (resamples)",
    "text": "Splitting (resamples)\n\ndata(attrition, package = \"modeldata\")\n\nglimpse(attrition)\n\nRows: 1,470\nColumns: 31\n$ Age                      &lt;int&gt; 41, 49, 37, 33, 27, 32, 59, 30, 38, 36, 35, 2‚Ä¶\n$ Attrition                &lt;fct&gt; Yes, No, Yes, No, No, No, No, No, No, No, No,‚Ä¶\n$ BusinessTravel           &lt;fct&gt; Travel_Rarely, Travel_Frequently, Travel_Rare‚Ä¶\n$ DailyRate                &lt;int&gt; 1102, 279, 1373, 1392, 591, 1005, 1324, 1358,‚Ä¶\n$ Department               &lt;fct&gt; Sales, Research_Development, Research_Develop‚Ä¶\n$ DistanceFromHome         &lt;int&gt; 1, 8, 2, 3, 2, 2, 3, 24, 23, 27, 16, 15, 26, ‚Ä¶\n$ Education                &lt;ord&gt; College, Below_College, College, Master, Belo‚Ä¶\n$ EducationField           &lt;fct&gt; Life_Sciences, Life_Sciences, Other, Life_Sci‚Ä¶\n$ EnvironmentSatisfaction  &lt;ord&gt; Medium, High, Very_High, Very_High, Low, Very‚Ä¶\n$ Gender                   &lt;fct&gt; Female, Male, Male, Female, Male, Male, Femal‚Ä¶\n$ HourlyRate               &lt;int&gt; 94, 61, 92, 56, 40, 79, 81, 67, 44, 94, 84, 4‚Ä¶\n$ JobInvolvement           &lt;ord&gt; High, Medium, Medium, High, High, High, Very_‚Ä¶\n$ JobLevel                 &lt;int&gt; 2, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 2, 1, 1, 1, ‚Ä¶\n$ JobRole                  &lt;fct&gt; Sales_Executive, Research_Scientist, Laborato‚Ä¶\n$ JobSatisfaction          &lt;ord&gt; Very_High, Medium, High, High, Medium, Very_H‚Ä¶\n$ MaritalStatus            &lt;fct&gt; Single, Married, Single, Married, Married, Si‚Ä¶\n$ MonthlyIncome            &lt;int&gt; 5993, 5130, 2090, 2909, 3468, 3068, 2670, 269‚Ä¶\n$ MonthlyRate              &lt;int&gt; 19479, 24907, 2396, 23159, 16632, 11864, 9964‚Ä¶\n$ NumCompaniesWorked       &lt;int&gt; 8, 1, 6, 1, 9, 0, 4, 1, 0, 6, 0, 0, 1, 0, 5, ‚Ä¶\n$ OverTime                 &lt;fct&gt; Yes, No, Yes, Yes, No, No, Yes, No, No, No, N‚Ä¶\n$ PercentSalaryHike        &lt;int&gt; 11, 23, 15, 11, 12, 13, 20, 22, 21, 13, 13, 1‚Ä¶\n$ PerformanceRating        &lt;ord&gt; Excellent, Outstanding, Excellent, Excellent,‚Ä¶\n$ RelationshipSatisfaction &lt;ord&gt; Low, Very_High, Medium, High, Very_High, High‚Ä¶\n$ StockOptionLevel         &lt;int&gt; 0, 1, 0, 0, 1, 0, 3, 1, 0, 2, 1, 0, 1, 1, 0, ‚Ä¶\n$ TotalWorkingYears        &lt;int&gt; 8, 10, 7, 8, 6, 8, 12, 1, 10, 17, 6, 10, 5, 3‚Ä¶\n$ TrainingTimesLastYear    &lt;int&gt; 0, 3, 3, 3, 3, 2, 3, 2, 2, 3, 5, 3, 1, 2, 4, ‚Ä¶\n$ WorkLifeBalance          &lt;ord&gt; Bad, Better, Better, Better, Better, Good, Go‚Ä¶\n$ YearsAtCompany           &lt;int&gt; 6, 10, 0, 8, 2, 7, 1, 1, 9, 7, 5, 9, 5, 2, 4,‚Ä¶\n$ YearsInCurrentRole       &lt;int&gt; 4, 7, 0, 7, 2, 7, 0, 0, 7, 7, 4, 5, 2, 2, 2, ‚Ä¶\n$ YearsSinceLastPromotion  &lt;int&gt; 0, 1, 0, 3, 2, 3, 0, 0, 1, 7, 0, 0, 4, 1, 0, ‚Ä¶\n$ YearsWithCurrManager     &lt;int&gt; 5, 7, 0, 0, 2, 6, 0, 0, 8, 7, 3, 8, 3, 2, 3, ‚Ä¶\n\n\n\nlibrary(tidymodels)\n\n\nattrition_split &lt;- initial_split(tibble(attrition), prop = 0.8)\n\nOnce we have a cleaned dataset, we can split it into two parts. The first part is used for training the model, and the second part is used for testing the model. The initial_split() function performs this split. The prop argument specifies the proportion of the data that should be used for training. The default value is 0.75, which means that 75% of the data will be used for training and 25% for testing. This is fine in most cases. The initial_split() function returns a list containing two tibbles: the training data and the testing data.\n\ntraining(attrition_split)\n\n# A tibble: 1,176 √ó 31\n     Age Attrition BusinessTravel    DailyRate Department       DistanceFromHome\n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;                 &lt;int&gt; &lt;fct&gt;                       &lt;int&gt;\n 1    42 No        Travel_Rarely          1147 Human_Resources                10\n 2    28 No        Travel_Rarely          1451 Research_Develo‚Ä¶                2\n 3    34 No        Travel_Rarely           665 Research_Develo‚Ä¶                6\n 4    34 No        Travel_Frequently      1003 Research_Develo‚Ä¶                2\n 5    40 No        Travel_Rarely           444 Sales                           2\n 6    59 No        Travel_Rarely           818 Human_Resources                 6\n 7    39 Yes       Travel_Frequently       203 Research_Develo‚Ä¶                2\n 8    45 No        Travel_Rarely          1329 Research_Develo‚Ä¶                2\n 9    36 No        Travel_Rarely           917 Research_Develo‚Ä¶                6\n10    38 No        Non-Travel             1336 Human_Resources                 2\n# ‚Ñπ 1,166 more rows\n# ‚Ñπ 25 more variables: Education &lt;ord&gt;, EducationField &lt;fct&gt;,\n#   EnvironmentSatisfaction &lt;ord&gt;, Gender &lt;fct&gt;, HourlyRate &lt;int&gt;,\n#   JobInvolvement &lt;ord&gt;, JobLevel &lt;int&gt;, JobRole &lt;fct&gt;, JobSatisfaction &lt;ord&gt;,\n#   MaritalStatus &lt;fct&gt;, MonthlyIncome &lt;int&gt;, MonthlyRate &lt;int&gt;,\n#   NumCompaniesWorked &lt;int&gt;, OverTime &lt;fct&gt;, PercentSalaryHike &lt;int&gt;,\n#   PerformanceRating &lt;ord&gt;, RelationshipSatisfaction &lt;ord&gt;, ‚Ä¶\n\n\n\ntesting(attrition_split)\n\n# A tibble: 294 √ó 31\n     Age Attrition BusinessTravel    DailyRate Department       DistanceFromHome\n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;                 &lt;int&gt; &lt;fct&gt;                       &lt;int&gt;\n 1    33 No        Travel_Frequently      1392 Research_Develo‚Ä¶                3\n 2    32 No        Travel_Frequently      1005 Research_Develo‚Ä¶                2\n 3    31 No        Travel_Rarely           670 Research_Develo‚Ä¶               26\n 4    28 Yes       Travel_Rarely           103 Research_Develo‚Ä¶               24\n 5    22 No        Non-Travel             1123 Research_Develo‚Ä¶               16\n 6    46 No        Travel_Rarely           705 Sales                           2\n 7    35 No        Travel_Rarely           890 Sales                           2\n 8    36 No        Travel_Rarely           852 Research_Develo‚Ä¶                5\n 9    28 Yes       Travel_Rarely          1434 Research_Develo‚Ä¶                5\n10    44 No        Travel_Rarely          1488 Sales                           1\n# ‚Ñπ 284 more rows\n# ‚Ñπ 25 more variables: Education &lt;ord&gt;, EducationField &lt;fct&gt;,\n#   EnvironmentSatisfaction &lt;ord&gt;, Gender &lt;fct&gt;, HourlyRate &lt;int&gt;,\n#   JobInvolvement &lt;ord&gt;, JobLevel &lt;int&gt;, JobRole &lt;fct&gt;, JobSatisfaction &lt;ord&gt;,\n#   MaritalStatus &lt;fct&gt;, MonthlyIncome &lt;int&gt;, MonthlyRate &lt;int&gt;,\n#   NumCompaniesWorked &lt;int&gt;, OverTime &lt;fct&gt;, PercentSalaryHike &lt;int&gt;,\n#   PerformanceRating &lt;ord&gt;, RelationshipSatisfaction &lt;ord&gt;, ‚Ä¶\n\n\nIn many cases, it is important that the data we use to train the model and evaluate it is similar according to certain criteria. For example, if significant differences are observed in salaries, it is important to separate the original data in such a way that the distribution of salaries is the same in the training and testing tables. This can be ensured using the strata argument.\n\nattrition_split &lt;- initial_split(tibble(attrition), prop = 0.8, strata = \"MonthlyIncome\")\n\n\nggplot() +\n  aes(MonthlyIncome) +\n  geom_histogram(data=training(attrition_split), alpha = .3, fill = \"red\") +\n  geom_histogram(data=testing(attrition_split), alpha = .3, fill = \"blue\")\n\n\n\n\nFurthermore, we have observed that each model possesses different hyperparameters. In the case of decision trees, the complexity of the tree is determined, while for other models, the number of degree of freedom terms to be applied is considered. Every other model will also have its own set of hyperparameters. We are seeking the combination of hyperparameters that yields the best performance. Typically, this is not achieved by solely evaluating the model‚Äôs performance on the test data using the training data, but by further dividing the training data. One example of this is k-fold cross-validation. In the case of k-fold cross-validation, if we divide the data into 10 parts, the model is trained on 90% of the previously created training data, and evaluated on the remaining 10% for a given set of hyperparameters. Then, we reverse the process and evaluate on another 10%. This process is repeated k (10) times. The average of the results is then calculated. This is a very common method for evaluating models. The rsample package contains a function that performs this split. The vfold_cv() function performs this split. The number of folds is specified by the v argument. The default value is 10.\n\nattrition_vsplit &lt;- vfold_cv(tibble(attrition), v = 10)\n\nattrition_vsplit\n\n#  10-fold cross-validation \n# A tibble: 10 √ó 2\n   splits             id    \n   &lt;list&gt;             &lt;chr&gt; \n 1 &lt;split [1323/147]&gt; Fold01\n 2 &lt;split [1323/147]&gt; Fold02\n 3 &lt;split [1323/147]&gt; Fold03\n 4 &lt;split [1323/147]&gt; Fold04\n 5 &lt;split [1323/147]&gt; Fold05\n 6 &lt;split [1323/147]&gt; Fold06\n 7 &lt;split [1323/147]&gt; Fold07\n 8 &lt;split [1323/147]&gt; Fold08\n 9 &lt;split [1323/147]&gt; Fold09\n10 &lt;split [1323/147]&gt; Fold10\n\n\nThe cross-validation procedure can be very complex at times, for example when we want to forecast time series and always need to separate the data from the past five years to examine how we would forecast the next year. This can be done using the rset_manual function. This procedure is called step-forward cross-validation, and you can see an example of it in the link below. We could also mention the example of predicting stock prices or any other economic variable.\n\n\nWalk-forward cross-validation\n\nIf sample size is a consideration, it may be worth choosing bootstrapping instead of simple cross-validation, as in this case the measurements of the individual training and test sets will be identical, and then randomly divide the dataset, using random sampling, to collect the samples. This enables us to run the model a very large number of times, in a very large quantity, under a given parameter, while keeping the size constant.\n\nbootstraps(training(attrition_split), times = 100)\n\n# Bootstrap sampling \n# A tibble: 100 √ó 2\n   splits             id          \n   &lt;list&gt;             &lt;chr&gt;       \n 1 &lt;split [1174/415]&gt; Bootstrap001\n 2 &lt;split [1174/444]&gt; Bootstrap002\n 3 &lt;split [1174/412]&gt; Bootstrap003\n 4 &lt;split [1174/422]&gt; Bootstrap004\n 5 &lt;split [1174/432]&gt; Bootstrap005\n 6 &lt;split [1174/429]&gt; Bootstrap006\n 7 &lt;split [1174/430]&gt; Bootstrap007\n 8 &lt;split [1174/442]&gt; Bootstrap008\n 9 &lt;split [1174/433]&gt; Bootstrap009\n10 &lt;split [1174/435]&gt; Bootstrap010\n# ‚Ñπ 90 more rows"
  },
  {
    "objectID": "content/16-content.html#preprocessing-recipes",
    "href": "content/16-content.html#preprocessing-recipes",
    "title": "Tidy modelling",
    "section": "Preprocessing (recipes)",
    "text": "Preprocessing (recipes)\nAfter splitting the data, it is necessary to perform transformations for certain models. Such a transformation could be, for example, standardization or excluding variables that have very high correlation or variables that have zero variabilty. Let‚Äôs assume, for example, that we have a training data in which only one type of value occurs in a certain variable. For instance, only males are present in the training data. Obviously, when we want to make predictions with this model, we will not be able to use this column. Therefore, it is necessary that during training, this column is excluded. We can imagine that we have hundreds of variables and we cannot ensure that we can always filter out these columns in the training data. For example, we run the model for each year.\nThe tidymodels package collection contains the recipe package that allows us to perform such transformations. The recipe package is designed to create a blueprint for a model. It is a collection of steps that will be performed on the data before the model is trained. The recipe package contains a number of functions that can be used to create a recipe. The recipe() function creates a recipe object. The step_*() functions add steps to the recipe. The prep() function prepares the recipe, and the bake() function applies the recipe to the data.\nThe recipe() function takes two arguments. The first argument is the formula that specifies the target variable and the explanatory variables. The second argument is the data frame that contains the data. The step_*() functions take the recipe object as the first argument and the name of the step as the second argument. The prep() function takes the recipe object as the first argument. The bake() function takes the recipe object as the first argument and the data frame as the second argument.\n\nattrition_vsplit %&gt;% \n  pull(splits)\n\n[[1]]\n&lt;Analysis/Assess/Total&gt;\n&lt;1323/147/1470&gt;\n\n[[2]]\n&lt;Analysis/Assess/Total&gt;\n&lt;1323/147/1470&gt;\n\n[[3]]\n&lt;Analysis/Assess/Total&gt;\n&lt;1323/147/1470&gt;\n\n[[4]]\n&lt;Analysis/Assess/Total&gt;\n&lt;1323/147/1470&gt;\n\n[[5]]\n&lt;Analysis/Assess/Total&gt;\n&lt;1323/147/1470&gt;\n\n[[6]]\n&lt;Analysis/Assess/Total&gt;\n&lt;1323/147/1470&gt;\n\n[[7]]\n&lt;Analysis/Assess/Total&gt;\n&lt;1323/147/1470&gt;\n\n[[8]]\n&lt;Analysis/Assess/Total&gt;\n&lt;1323/147/1470&gt;\n\n[[9]]\n&lt;Analysis/Assess/Total&gt;\n&lt;1323/147/1470&gt;\n\n[[10]]\n&lt;Analysis/Assess/Total&gt;\n&lt;1323/147/1470&gt;\n\n\n\nattrition_recipe &lt;- recipe(MonthlyIncome ~ ., data = attrition) %&gt;% \n  step_rm(RelationshipSatisfaction) %&gt;% # should use this?\n  step_dummy(all_nominal()) %&gt;%\n  step_corr(all_numeric()) \n\n\nattrition_recipe_preped &lt;- attrition_recipe %&gt;% \n  prep()\n\n\nattrition_vsplit %&gt;% \n  pull(splits)\n\n[[1]]\n&lt;Analysis/Assess/Total&gt;\n&lt;1323/147/1470&gt;\n\n[[2]]\n&lt;Analysis/Assess/Total&gt;\n&lt;1323/147/1470&gt;\n\n[[3]]\n&lt;Analysis/Assess/Total&gt;\n&lt;1323/147/1470&gt;\n\n[[4]]\n&lt;Analysis/Assess/Total&gt;\n&lt;1323/147/1470&gt;\n\n[[5]]\n&lt;Analysis/Assess/Total&gt;\n&lt;1323/147/1470&gt;\n\n[[6]]\n&lt;Analysis/Assess/Total&gt;\n&lt;1323/147/1470&gt;\n\n[[7]]\n&lt;Analysis/Assess/Total&gt;\n&lt;1323/147/1470&gt;\n\n[[8]]\n&lt;Analysis/Assess/Total&gt;\n&lt;1323/147/1470&gt;\n\n[[9]]\n&lt;Analysis/Assess/Total&gt;\n&lt;1323/147/1470&gt;\n\n[[10]]\n&lt;Analysis/Assess/Total&gt;\n&lt;1323/147/1470&gt;\n\n\n\nattrition_vsplit %&gt;% \n  pull(splits) %&gt;% \n  first() %&gt;% \n  analysis()\n\n# A tibble: 1,323 √ó 31\n     Age Attrition BusinessTravel    DailyRate Department       DistanceFromHome\n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;                 &lt;int&gt; &lt;fct&gt;                       &lt;int&gt;\n 1    41 Yes       Travel_Rarely          1102 Sales                           1\n 2    49 No        Travel_Frequently       279 Research_Develo‚Ä¶                8\n 3    37 Yes       Travel_Rarely          1373 Research_Develo‚Ä¶                2\n 4    33 No        Travel_Frequently      1392 Research_Develo‚Ä¶                3\n 5    27 No        Travel_Rarely           591 Research_Develo‚Ä¶                2\n 6    32 No        Travel_Frequently      1005 Research_Develo‚Ä¶                2\n 7    59 No        Travel_Rarely          1324 Research_Develo‚Ä¶                3\n 8    30 No        Travel_Rarely          1358 Research_Develo‚Ä¶               24\n 9    38 No        Travel_Frequently       216 Research_Develo‚Ä¶               23\n10    36 No        Travel_Rarely          1299 Research_Develo‚Ä¶               27\n# ‚Ñπ 1,313 more rows\n# ‚Ñπ 25 more variables: Education &lt;ord&gt;, EducationField &lt;fct&gt;,\n#   EnvironmentSatisfaction &lt;ord&gt;, Gender &lt;fct&gt;, HourlyRate &lt;int&gt;,\n#   JobInvolvement &lt;ord&gt;, JobLevel &lt;int&gt;, JobRole &lt;fct&gt;, JobSatisfaction &lt;ord&gt;,\n#   MaritalStatus &lt;fct&gt;, MonthlyIncome &lt;int&gt;, MonthlyRate &lt;int&gt;,\n#   NumCompaniesWorked &lt;int&gt;, OverTime &lt;fct&gt;, PercentSalaryHike &lt;int&gt;,\n#   PerformanceRating &lt;ord&gt;, RelationshipSatisfaction &lt;ord&gt;, ‚Ä¶\n\n\nHow would the analysis data look like after these preprocessing steps?\n\nattrition_vsplit %&gt;% \n  pull(splits) %&gt;% \n  first() %&gt;% \n  analysis() %&gt;% \n  bake(object = attrition_recipe_preped)\n\n# A tibble: 1,323 √ó 53\n     Age DailyRate DistanceFromHome HourlyRate MonthlyRate NumCompaniesWorked\n   &lt;int&gt;     &lt;int&gt;            &lt;int&gt;      &lt;int&gt;       &lt;int&gt;              &lt;int&gt;\n 1    41      1102                1         94       19479                  8\n 2    49       279                8         61       24907                  1\n 3    37      1373                2         92        2396                  6\n 4    33      1392                3         56       23159                  1\n 5    27       591                2         40       16632                  9\n 6    32      1005                2         79       11864                  0\n 7    59      1324                3         81        9964                  4\n 8    30      1358               24         67       13335                  1\n 9    38       216               23         44        8787                  0\n10    36      1299               27         94       16577                  6\n# ‚Ñπ 1,313 more rows\n# ‚Ñπ 47 more variables: PercentSalaryHike &lt;int&gt;, StockOptionLevel &lt;int&gt;,\n#   TotalWorkingYears &lt;int&gt;, TrainingTimesLastYear &lt;int&gt;, YearsAtCompany &lt;int&gt;,\n#   YearsInCurrentRole &lt;int&gt;, YearsSinceLastPromotion &lt;int&gt;,\n#   YearsWithCurrManager &lt;int&gt;, MonthlyIncome &lt;int&gt;, Attrition_Yes &lt;dbl&gt;,\n#   BusinessTravel_Travel_Frequently &lt;dbl&gt;, BusinessTravel_Travel_Rarely &lt;dbl&gt;,\n#   Department_Sales &lt;dbl&gt;, Education_1 &lt;dbl&gt;, Education_2 &lt;dbl&gt;, ‚Ä¶\n\n\nNow we have our data preprocessed. Here comes the algorithm ü•Åü•Åü•Å"
  },
  {
    "objectID": "content/16-content.html#the-model-parsnip",
    "href": "content/16-content.html#the-model-parsnip",
    "title": "Tidy modelling",
    "section": "The model (parsnip) üíÉ",
    "text": "The model (parsnip) üíÉ\nThe parsnip package is a wrapper for a number of different models. The parsnip package contains a number of functions that can be used to create a model. The original models are written by individual programmers into different packages, thus we need to specify that the implemented model should be the one from which package, and which model. For example, there are different algoritms for a linear model. It can be an OLS, a LASSO (from the glmnet package), or a bayesian regression (stan package).\nIn this example, the linear_reg() function creates a linear regression model. The set_engine() function sets the model engine. If needed then the set_mode() function sets the model mode (regression/classification).\n\nlinear_reg_lm_spec &lt;-\n  linear_reg() %&gt;%\n  set_engine('lm')\n\nlinear_reg_glmnet_spec &lt;-\n  linear_reg(penalty = tune(), mixture = tune()) %&gt;%\n  set_engine('glmnet')\n\nlinear_reg_stan_spec &lt;-\n  linear_reg() %&gt;%\n  set_engine('stan')"
  },
  {
    "objectID": "content/16-content.html#the-workflow-workflows",
    "href": "content/16-content.html#the-workflow-workflows",
    "title": "Tidy modelling",
    "section": "The workflow (workflows)",
    "text": "The workflow (workflows)\nThe workflow is a combination of the recipe and a parsnip model specification. The workflow package contains a number of functions that can be used to create a workflow. The workflow() function creates a workflow object. The add_recipe() function adds a recipe to the workflow. The add_model() function adds a model to the workflow.\n\nlinear_reg_glmnet_workflow &lt;-\n  workflow() %&gt;%\n  add_recipe(attrition_recipe) %&gt;%\n  add_model(linear_reg_glmnet_spec)\n\nlinear_reg_glmnet_workflow\n\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: linear_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n3 Recipe Steps\n\n‚Ä¢ step_rm()\n‚Ä¢ step_dummy()\n‚Ä¢ step_corr()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = tune()\n\nComputational engine: glmnet"
  },
  {
    "objectID": "content/16-content.html#tuning-tune",
    "href": "content/16-content.html#tuning-tune",
    "title": "Tidy modelling",
    "section": "Tuning (tune)",
    "text": "Tuning (tune)\nOkay, we have a workflow. Now we need to find the optimal parameters for the model. We can do this with the tune_grid() function. The tune_grid() function takes the workflow object as the first argument and the resamples object as the second argument. The tune_grid() function returns a tibble with the optimal parameters for each resample.\n\nattrition_glm_tn &lt;- tune_grid(\n  linear_reg_glmnet_workflow,\n  resamples = attrition_vsplit,\n  grid = 30\n)\n\nError in `check_installs()`:\n! Some package installs are required: \n‚Ä¢ 'glmnet'\n\nattrition_glm_tn\n\nError in eval(expr, envir, enclos): object 'attrition_glm_tn' not found\n\n\nWith the code above, we tried 30 different combination of the hyperparameters. The model is built on th analysis sets and assessed on the assessment sets.\nWe can show the best solutions:\n\nshow_best(attrition_glm_tn, n = 5)\n\nError in eval(expr, envir, enclos): object 'attrition_glm_tn' not found\n\n\nOr based on \\(R^2\\) (if you used strata, then probably the same solutions will be shown):\n\nshow_best(attrition_glm_tn, n = 5, metric = \"rsq\")\n\nError in eval(expr, envir, enclos): object 'attrition_glm_tn' not found\n\n\nNow, we can finalise the workflow, with our best performing hyperparameters:\n\nlinear_reg_glmnet_workflow_final &lt;-\n  linear_reg_glmnet_workflow %&gt;%\n  finalize_workflow(\n    select_best(attrition_glm_tn)\n  )\n\nError: object 'attrition_glm_tn' not found\n\nlinear_reg_glmnet_workflow_final\n\nError in eval(expr, envir, enclos): object 'linear_reg_glmnet_workflow_final' not found"
  },
  {
    "objectID": "content/16-content.html#training",
    "href": "content/16-content.html#training",
    "title": "Tidy modelling",
    "section": "Training",
    "text": "Training\n\nlinear_reg_glmnet_fit &lt;-\n  linear_reg_glmnet_workflow_final %&gt;%\n  fit(data = (training(attrition_split)))\n\nError in eval(expr, envir, enclos): object 'linear_reg_glmnet_workflow_final' not found\n\n\nvariable importance\n\nlinear_reg_glmnet_fit %&gt;%\n  pull_workflow_fit() %&gt;%\n  vip::vi()\n\nError in eval(expr, envir, enclos): object 'linear_reg_glmnet_fit' not found"
  },
  {
    "objectID": "content/16-content.html#predicting",
    "href": "content/16-content.html#predicting",
    "title": "Tidy modelling",
    "section": "Predicting",
    "text": "Predicting\n\nlinear_reg_glmnet_fit %&gt;%\n  predict(new_data = testing(attrition_split))\n\nError in eval(expr, envir, enclos): object 'linear_reg_glmnet_fit' not found"
  },
  {
    "objectID": "content/16-content.html#evaluating-yardstick",
    "href": "content/16-content.html#evaluating-yardstick",
    "title": "Tidy modelling",
    "section": "Evaluating (yardstick)",
    "text": "Evaluating (yardstick)\n\nlinear_reg_glmnet_fit %&gt;%\n  predict(new_data = testing(attrition_split)) %&gt;%\n  bind_cols(attrition_split %&gt;% testing()) %&gt;%\n  yardstick::rsq(truth = MonthlyIncome, estimate = .pred)\n\nError in eval(expr, envir, enclos): object 'linear_reg_glmnet_fit' not found\n\n\n\nlinear_reg_glmnet_fit %&gt;%\n  predict(new_data = testing(attrition_split)) %&gt;%\n  bind_cols(attrition_split %&gt;% testing()) %&gt;%\n  yardstick::metrics(truth = MonthlyIncome, estimate = .pred)\n\nError in eval(expr, envir, enclos): object 'linear_reg_glmnet_fit' not found"
  },
  {
    "objectID": "content/16-content.html#classification",
    "href": "content/16-content.html#classification",
    "title": "Tidy modelling",
    "section": "Classification",
    "text": "Classification\n\nrand_forest_randomForest_spec &lt;-\n  rand_forest(mtry = tune(), min_n = tune()) %&gt;%\n  set_engine('randomForest') %&gt;%\n  set_mode('classification')\n\n\nattrition_recipe &lt;- recipe(Attrition ~ ., data = attrition) %&gt;% \n  step_rm(RelationshipSatisfaction) %&gt;% \n  step_pca(all_numeric(), threshold = 0.9) %&gt;% \n  step_corr(all_numeric(), threshold = .6)\n\nrand_forest_randomForest_workflow &lt;-\n  workflow() %&gt;%\n  add_recipe(attrition_recipe) %&gt;%\n  add_model(rand_forest_randomForest_spec)\n\n\nattrition_rf_tn &lt;- tune_grid(\n  rand_forest_randomForest_workflow,\n  resamples = attrition_vsplit,\n  grid = 5\n)\n\nError in `check_installs()`:\n! Some package installs are required: \n‚Ä¢ 'randomForest', 'randomForest'\n\n\n\nshow_best(attrition_rf_tn, n = 5)\n\nError in eval(expr, envir, enclos): object 'attrition_rf_tn' not found\n\n\n\nrf_workflow_final &lt;- \n  rand_forest_randomForest_workflow %&gt;% \n  finalize_workflow(\n    select_best(attrition_rf_tn)\n  )\n\nError: object 'attrition_rf_tn' not found\n\n\n\n# roc curve\n\nrf_workflow_final %&gt;% \n  fit(data = training(attrition_split)) %&gt;% \n  predict(new_data = testing(attrition_split), type= \"prob\") %&gt;% \n  bind_cols(attrition_split %&gt;% testing()) %&gt;% \n  yardstick::roc_curve(truth = Attrition, .pred_Yes) %&gt;% \n  autoplot()\n\nError in eval(expr, envir, enclos): object 'rf_workflow_final' not found"
  },
  {
    "objectID": "content/17-content.html",
    "href": "content/17-content.html",
    "title": "Working with Data Bases",
    "section": "",
    "text": "Suggested materials\n Tidy Finance: Financial Data\nDatabases are the most common way for data storage, characterized by their optimization for expedient data retrieval and efficient management of substantial data sets.\nSQL queries are the predominant method for accessing databases. If one is seeking job opportunities in the field of data analysis, proficiency in SQL is consistently expected. However, it is worth noting that accomplishing these tasks is also possible using R (with some limitation)!\nWhy using them?\nLet‚Äôs consider the following case. We have a daily data about all the historicly traded stocks in the US. The data is stored in a file called crspdaily.dta (the commonly used format for STATA) and it is 12 GB in size. We want to load the data into R and do some analysis, but our RAM is not sufficient or we want to avoid the efficiency loss that this file would cause.\nWe can load a dta file with the haven package, but we will need to load the entire file into memory. This is not a problem if the file is small, but it can be a problem if the file is large. The trick is that we selectively load specific rows of data, avoiding to load the 13 GB into the memory.\n\nsetwd(\"~/projects/momentum\") # where the files are available\nfile.size(\"crspdaily.dta\") |&gt; \n  prettyunits::pretty_bytes()\n\n[1] \"13.35 GB\"\n\nfile.size(\"crspmonthly.dta\") |&gt; \n  prettyunits::pretty_bytes()\n\n[1] \"753.24 MB\"\n\ncrsp_daily &lt;- haven::read_dta(\"crspdaily.dta\", n_max = 10) # first 10 rows\n\ncrsp_monthly &lt;- haven::read_dta(\"crspmonthly.dta\", n_max = 10)\n\nCreating a database\nWe will use the RSQLite package to create a database. The dbConnect() function creates a connection to the database. The dbWriteTable() function writes a data frame to the database. The dbDisconnect() function closes the connection to the database. You can access it with the tbl function, but‚Ä¶\n\nlibrary(RSQLite)\n\n# Connect to a new SQLite database (this will create the file)\ndb &lt;- dbConnect(RSQLite::SQLite(), dbname = \"database.sqlite\")\ndbWriteTable(db, \"iris_test\", iris, overwrite = TRUE)\ntbl(db, \"iris_test\")\n\n# Source:   table&lt;iris_test&gt; [?? x 5]\n# Database: sqlite 3.43.2 [/Users/marci/projects/bigdata2023/content/database.sqlite]\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ‚Ñπ more rows\n\n\nDuring this time, the data is not being read. With this, we are only preparing a query, but the data itself is not loaded. Therefore, we can see that the table has ?? number of rows. The data will only be loaded when the collect function is called.\n\ntbl(db, \"iris_test\") |&gt; \n  collect()\n\n# A tibble: 150 √ó 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ‚Ñπ 140 more rows\n\n\nThis is important because our dplyr functions are translated into R SQL queries using the dbplyr package, and we only receive the result of the query. Therefore, if we know which rows and columns (or aggregates) of a large gigabyte file we need, we do not need to load the entire dataset.\nTo expand an existing data table with additional observations, we can utilize the append = TRUE argument of the dbWriteTable() function.\n\ndbWriteTable(db, \"iris_test\", iris, append = TRUE)\ntbl(db, \"iris_test\") |&gt; \n  collect()\n\n# A tibble: 300 √ó 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ‚Ñπ 290 more rows\n\n\nConvert from dta to sqlite\n\ndb &lt;- dbConnect(RSQLite::SQLite(), dbname = \"crsp.sqlite\")\n\ndta_to_db &lt;- function(db = db, path_to_dta, table_name, n_max = 1e5) {\n  n &lt;- 0\n  cli::cli_status(\"Read lines: {n}\")\n  \n  while (TRUE) {\n    # read only a part of the data\n    res_df &lt;- haven::read_dta(path_to_dta, n_max = n_max, skip = n)\n    \n    # write to db\n    if (n == 0) {\n      dbWriteTable(db, table_name, res_df, overwrite = TRUE)\n    } else {\n      dbWriteTable(db, table_name, res_df, append = TRUE)\n    }\n    n &lt;- n + n_max\n    \n    # print status\n    cli::cli_status(\"Read lines: {.s {n}}\")\n    if (nrow(res_df) &lt; n_max) break\n  }\n  cli::cli_alert_success(\"Transformed!\")\n}\n\ndta_to_db(db, \"~/projects/momentum/crspdaily.dta\", \"crspdaily\")\ndta_to_db(db, \"~/projects/momentum/crspmonthly.dta\", \"crspmonthly\")\n\n\nRSQLite::dbListTables(db)\n\n[1] \"crspdaily\"   \"crspmonthly\"\n\n\n\ntbl(db, \"crspdaily\")\n\n# Source:   table&lt;crspdaily&gt; [?? x 17]\n# Database: sqlite 3.43.2 [/Users/marci/crsp/crsp.sqlite]\n   permno  date shrcd siccd cusip   divamt facpr dlret   prc   vol     ret   bid\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1  10000  5849    NA    NA 683916‚Ä¶     NA    NA    NA NA       NA NA         NA\n 2  10000  5850    10  3990 683916‚Ä¶     NA    NA    NA -2.56  1000 NA         NA\n 3  10000  5851    10  3990 683916‚Ä¶     NA    NA    NA -2.5  12800 -0.0244    NA\n 4  10000  5852    10  3990 683916‚Ä¶     NA    NA    NA -2.5   1400  0         NA\n 5  10000  5853    10  3990 683916‚Ä¶     NA    NA    NA -2.5   8500  0         NA\n 6  10000  5856    10  3990 683916‚Ä¶     NA    NA    NA -2.62  5450  0.0500    NA\n 7  10000  5857    10  3990 683916‚Ä¶     NA    NA    NA -2.75  2075  0.0476    NA\n 8  10000  5858    10  3990 683916‚Ä¶     NA    NA    NA -2.88 22490  0.0455    NA\n 9  10000  5859    10  3990 683916‚Ä¶     NA    NA    NA -3    10900  0.0435    NA\n10  10000  5860    10  3990 683916‚Ä¶     NA    NA    NA -3     8470  0         NA\n# ‚Ñπ more rows\n# ‚Ñπ 5 more variables: ask &lt;dbl&gt;, shrout &lt;dbl&gt;, openprc &lt;dbl&gt;, numtrd &lt;dbl&gt;,\n#   vwretd &lt;dbl&gt;\n\n\n\ntbl(db, \"crspmonthly\")\n\n# Source:   table&lt;crspmonthly&gt; [?? x 21]\n# Database: sqlite 3.43.2 [/Users/marci/crsp/crsp.sqlite]\n   PERMNO  date SHRCD SICCD TICKER  SHRCLS PRIMEXCH HEXCD CUSIP    DIVAMT FACPR\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1  10000  5843    NA    NA \"\"      \"\"     \"\"           3 68391610     NA    NA\n 2  10000  5874    10  3990 \"OMFGA\" \"A\"    \"Q\"          3 68391610     NA    NA\n 3  10000  5902    10  3990 \"OMFGA\" \"A\"    \"Q\"          3 68391610     NA    NA\n 4  10000  5933    10  3990 \"OMFGA\" \"A\"    \"Q\"          3 68391610     NA    NA\n 5  10000  5963    10  3990 \"OMFGA\" \"A\"    \"Q\"          3 68391610     NA    NA\n 6  10000  5993    10  3990 \"OMFGA\" \"A\"    \"Q\"          3 68391610     NA    NA\n 7  10000  6024    10  3990 \"OMFGA\" \"A\"    \"Q\"          3 68391610     NA    NA\n 8  10000  6055    10  3990 \"OMFGA\" \"A\"    \"Q\"          3 68391610     NA    NA\n 9  10000  6084    10  3990 \"OMFGA\" \"A\"    \"Q\"          3 68391610     NA    NA\n10  10000  6116    10  3990 \"OMFGA\" \"A\"    \"Q\"          3 68391610     NA    NA\n# ‚Ñπ more rows\n# ‚Ñπ 10 more variables: DLPRC &lt;dbl&gt;, DLRET &lt;dbl&gt;, PRC &lt;dbl&gt;, VOL &lt;dbl&gt;,\n#   RET &lt;dbl&gt;, BID &lt;dbl&gt;, ASK &lt;dbl&gt;, SHROUT &lt;dbl&gt;, SPREAD &lt;dbl&gt;, vwretd &lt;dbl&gt;\n\n\n\npermno_to_ticker &lt;- tbl(db, \"crspmonthly\") |&gt; \n  # stocks at large US markets\n  filter(SHRCD %in% 10:11, HEXCD == 3, TICKER != \"\") |&gt; \n  select(PERMNO, TICKER) |&gt; \n  distinct()\n\n# ! date format is coerced to numeric in the db\ndate_filter &lt;- as.integer(as.Date(\"2018-12-31\") - as.Date(\"1970-01-01\"))\n\ncrsp_df &lt;- tbl(db, \"crspdaily\") |&gt; \n  filter(date &gt; date_filter) |&gt; # after 2018\n  inner_join(x = permno_to_ticker, by = c(\"PERMNO\" = \"permno\")) |&gt;\n  collect()\n\n\ndbDisconnect(db)\n\n\ncrsp_df\n\n# A tibble: 2,748,416 √ó 18\n   PERMNO TICKER  date shrcd siccd cusip    divamt facpr dlret   prc    vol\n    &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1  10026 JJSF   17898    11  2052 46603210     NA    NA    NA  141  112825\n 2  10026 JJSF   17899    11  2052 46603210     NA    NA    NA  143.  84331\n 3  10026 JJSF   17900    11  2052 46603210     NA    NA    NA  145.  88233\n 4  10026 JJSF   17903    11  2052 46603210     NA    NA    NA  145.  79539\n 5  10026 JJSF   17904    11  2052 46603210     NA    NA    NA  149.  70200\n 6  10026 JJSF   17905    11  2052 46603210     NA    NA    NA  148.  63591\n 7  10026 JJSF   17906    11  2052 46603210     NA    NA    NA  149.  66117\n 8  10026 JJSF   17907    11  2052 46603210     NA    NA    NA  149.  58050\n 9  10026 JJSF   17910    11  2052 46603210     NA    NA    NA  148.  92311\n10  10026 JJSF   17911    11  2052 46603210     NA    NA    NA  146   89393\n# ‚Ñπ 2,748,406 more rows\n# ‚Ñπ 7 more variables: ret &lt;dbl&gt;, bid &lt;dbl&gt;, ask &lt;dbl&gt;, shrout &lt;dbl&gt;,\n#   openprc &lt;dbl&gt;, numtrd &lt;dbl&gt;, vwretd &lt;dbl&gt;\n\n\n\nt &lt;- tempfile()\ndownload.file(\"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/Siccodes17.zip\", t)\nunzip(t)\nindustry_raw &lt;- read_file(t)\n\nindustry_classification_df &lt;- industry_raw |&gt; \n  str_remove_all(\"\\r\") |&gt; \n  str_split_1(\"\\n\") |&gt; \n  enframe(value = \"raw\", name = NULL) |&gt; \n  filter(raw != \"\") |&gt; \n  mutate(\n    # `parent`: boolean whether or not the row represents a parent industry code\n    parent = str_starts(raw, \"[ ]{0,2}\\\\d\"),\n    industry17 = ifelse(parent, str_remove(raw, \"\\\\d{1,3} \\\\w{1,}\"), NA),\n    industry17 = str_squish(industry17),\n    code = map2(parent, raw, \\(p, r) {\n      if (!p) {\n        # numbers from the first to the second code extracted from `raw`\n        # \"0100-0199\"\n        seq(\n          from = as.numeric(str_extract_all(r, \"\\\\d{4}\")[[1]][1]),\n          to = as.numeric(str_extract_all(r, \"\\\\d{4}\")[[1]][2])\n        )\n      }\n    }),\n    industry = str_remove(raw, \".*\\\\d\"),\n    industry = str_squish(industry)\n  ) |&gt; \n  select(contains(\"industry\") | contains(\"code\")) |&gt; \n  fill(industry17) |&gt; \n  filter(duplicated(industry17)) |&gt; \n  unnest(code) |&gt; \n  mutate(code = ifelse(code &lt; 1000, str_c(\"0\", code), code)) |&gt; \n  select(code, industry17)\n\n\ndownload.file(\"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_daily_CSV.zip\", t)\nunzip(t)\nkenneth_factors_df &lt;- read_csv(t, skip = 3)\nkenneth_factors_df\n\n# A tibble: 25,587 √ó 5\n   ...1     `Mkt-RF`   SMB   HML    RF\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 19260701     0.1  -0.25 -0.27 0.009\n 2 19260702     0.45 -0.33 -0.06 0.009\n 3 19260706     0.17  0.3  -0.39 0.009\n 4 19260707     0.09 -0.58  0.02 0.009\n 5 19260708     0.21 -0.38  0.19 0.009\n 6 19260709    -0.71  0.43  0.57 0.009\n 7 19260710     0.62 -0.53 -0.1  0.009\n 8 19260712     0.04 -0.03  0.64 0.009\n 9 19260713     0.48 -0.28 -0.2  0.009\n10 19260714     0.04  0.07 -0.43 0.009\n# ‚Ñπ 25,577 more rows"
  },
  {
    "objectID": "content/18-content.html",
    "href": "content/18-content.html",
    "title": "Text mining",
    "section": "",
    "text": "Text Mining with R\n Supervised Machine Learning for Text Analysis in R\n Silge‚Äôs Youtube Channel & Blog"
  },
  {
    "objectID": "content/18-content.html#why",
    "href": "content/18-content.html#why",
    "title": "Text mining",
    "section": "Why?",
    "text": "Why?\nTextual data plays an important role in various areas of economics. These unstructured data sources typically require significant cleansing and processing procedures in order to extract information from them. This is where Natural Language Processing (NLP) comes into play, helping to interpret and structure these texts, facilitating a better understanding of economic trends, market behavior, consumer sentiment, political influences, and many other factors. An example of this is that by utilizing NLP technology, economic reports, banking documents, and other textual data can be transformed into structured formats, enabling a better and deeper understanding of monetary policy impacts. In contemporary research, it is essential for a master student to familiarize themselves with textual data from various perspectives.\n\nThe texts might be the thing our research focuses on (such as communication and marketing topics).\nA substantial portion of our generated data arrives in textual form. Processing textual data is crucial for addressing numerous research questions, such as generating daily sentiment indexes.\nIn many cases, text-based models can provide excellent narratives to complement our econometric models (as discussed later)."
  },
  {
    "objectID": "content/18-content.html#what",
    "href": "content/18-content.html#what",
    "title": "Text mining",
    "section": "What?",
    "text": "What?\nText mining is the process of extracting useful information from unstructured data. It is a broad term that covers a variety of techniques for analyzing textual data. Text mining is a subset of the field of Natural Language Processing (NLP), which is concerned with building systems that analyze and understand natural language. Text mining is a way to obtain insights from unstructured data. It is a process that involves structuring the input text, deriving patterns within the structured data, and finally evaluating and interpreting the output. Text mining is a multi-disciplinary field based on information retrieval, data mining, machine learning, statistics, and computational linguistics.\nThe following presentation is an illustration of the toolkit that have been used related to the central bank communication."
  },
  {
    "objectID": "content/18-content.html#how-yes-in-a-tidy-approach",
    "href": "content/18-content.html#how-yes-in-a-tidy-approach",
    "title": "Text mining",
    "section": "How? (Yes, in a tidy approach)",
    "text": "How? (Yes, in a tidy approach)\nLets recall our concepts about tidy data principles. This involve structuring datasets so that each variable is a column, each observation is a row, and each type of observational unit forms a table. Applying these principles to text mining, especially in economic research, facilitates more efficient and effective data analysis.\nIn the context of text mining, tidy text means that each row of the dataset represents a single observation about a single token (usually a word). This format is particularly useful for economic texts, as it allows for straightforward application of various text mining techniques and visualisations[^Many text mining packages have their own specific workflow (mainly for efficiency reasons), which is the opposite of this. For example STM is a mess sometimes or you can only run certain models in python. The reticulate package might help.].\nTokens\nA token is a single entity, usually a word, that is a component of a larger text. Tokenization is the process of breaking a text into tokens. The simplest way to tokenize a text is to split it into individual words. This is a good starting point, but it is not sufficient for most text mining applications. For example, the words ‚Äúrun‚Äù, ‚Äúruns‚Äù, and ‚Äúrunning‚Äù are all forms of the same verb, and it is useful to treat them as the same token. This process is called stemming.\nAnother example is that the words ‚ÄúUnited States‚Äù are a single token, and it is useful to treat them as such. This process is called n-grams. n-grams are tokens that are made up of n words. For example, ‚ÄúUnited States‚Äù is a 2-gram, and ‚ÄúUnited States of America‚Äù is a 4-gram. I personally find them useful to filter out repeating marketing slogans from the text (What are the longest n words that frequently appear together), and to interpret models that are based on words.\nFor example in the following presentation I use 2-grams to understand that why certain words might correlate with low/high airbnb rating. The original TDK paper is also available."
  },
  {
    "objectID": "content/18-content.html#term-frequency-and-inverse-document-frequency-tf-idf",
    "href": "content/18-content.html#term-frequency-and-inverse-document-frequency-tf-idf",
    "title": "Text mining",
    "section": "Term Frequency and Inverse Document Frequency (TF-IDF)",
    "text": "Term Frequency and Inverse Document Frequency (TF-IDF)\nTF-IDF is a statistical measure used to understand the importance of words in a document set. In economics, TF-IDF can be used to identify key terms in a large corpus of economic literature, highlighting prevalent themes or topics. TF-IDF is a product of two statistics, term frequency and inverse document frequency. Term frequency is the number of times a term appears in a document. Inverse document frequency is the inverse of the number of documents in which a term appears. The TF-IDF statistic is the product of these two statistics. The higher the TF-IDF statistic, the more important the term is to the document. The following presentation is an example of this. This method will identify words that frequently appear in the given document but do not appear elsewhere. This is useful for identifying key terms in a document set (here: blog post of one author)."
  },
  {
    "objectID": "content/18-content.html#sentiment-analysis",
    "href": "content/18-content.html#sentiment-analysis",
    "title": "Text mining",
    "section": "Sentiment analysis",
    "text": "Sentiment analysis\nSentiment analysis is the process of determining the emotional tone behind a series of words, used to gain an understanding of the the attitudes, opinions and emotions expressed within an online mention. Sentiment analysis is extremely useful in social media monitoring as it allows us to gain an overview of the wider public opinion behind certain topics. In economics, sentiment analysis can be used to understand the tone of central bank communication, expectations or public sentiment.\nThe following presentation is an example of the latter."
  },
  {
    "objectID": "content/18-content.html#topic-modeling",
    "href": "content/18-content.html#topic-modeling",
    "title": "Text mining",
    "section": "Topic modeling",
    "text": "Topic modeling\nTopic modeling is a type of statistical modeling for discovering the abstract ‚Äútopics‚Äù that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: ‚Äúdog‚Äù and ‚Äúbone‚Äù will appear more often in documents about dogs, ‚Äúcat‚Äù and ‚Äúmeow‚Äù will appear in documents about cats, and ‚Äúthe‚Äù and ‚Äúis‚Äù will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document‚Äôs balance of topics is.\nThe most frequenlty seen method is the Latent Dirichlet Allocation (LDA) model. LDA is based on the assumption that documents are mixtures of topics, where a topic is defined as a distribution over words. This model allows for each document in a corpus to be described by a distribution of topics, and each topic to be characterized by a distribution of words.\n\n\n\n\n\n\nStep by step\n\n\n\n\nInitialization: LDA starts by assigning each word in each document to a random topic. This initial assignment gives both topic representations for all the documents and word distributions for all the topics.\nIterative Process: The model then iteratively updates these assignments. For each word in a document, LDA adjusts the topic assignment based on two factors:\n\n\nHow prevalent is each topic in the document?\nHow prevalent is each word in the topics?\n\n\nConvergence: This process is repeated until the assignments reach a steady state, which is the model‚Äôs output.\n\n\n\nBut how many topics?\nThe number of topics is the only tuned hyperparameter of the model. Too few topics will produce overly broad results and it is impossible to interpret them, while choosing too many topics will result in the ‚Äúover-clustering‚Äù of a corpus into many small, highly similar topics. Similar to clustering methods, there are rules to determine the optimal value of this parameter, for example, semantic cohesion and exclusivity, but we apply a different framework, because of the interest in one topic or it would result too many topics.\nHow to interpret the results?\nThe results of the model are the topic-word and document-topic distributions. The topic-word distribution is a matrix of words and topics, where each cell represents the probability of a word appearing in a given topic. The document-topic distribution is a matrix of documents and topics, where each cell represents the probability of a document belonging to a given topic. The following presentation is an example of this. This method will identify topics that frequently appear in the given document but do not appear elsewhere. This is useful for identifying key topics in a document set (here: blog post of one author).\n\nlibrary(magrittr)\nlibrary(tidyverse)\nlibrary(rvest)\n\n\nget_links &lt;- (. %&gt;% \n                read_html() %&gt;% \n                html_nodes(\"a\") %&gt;% \n                html_attr(\"href\") %&gt;% \n                na.omit() %&gt;% \n                unique() %&gt;% \n                keep(str_detect, pattern = \"https://economaniablog.hu/\") %&gt;% \n                keep(str_detect, pattern = \"hu/tag/|hu/author/|hu/bloggerek/\", negate = T) %&gt;% \n                keep(str_detect, pattern = \"hu/about/|hu/kapcsolat/|hu/tanulj_tolunk/|hu/szakmai_muhely/|hu/category/|#comments|hu/page/\", negate = T) %&gt;% \n                setdiff(\"https://economaniablog.hu/\")) %&gt;% \n  possibly(otherwise = NA_character_)\n\narticle_links &lt;- str_c(\"https://economaniablog.hu/page/\", 1:40, \"/\") %&gt;% \n  map(get_links) %&gt;% \n  reduce(c) %&gt;% \n  na.omit()\n\nget_text &lt;- function(p, node) {\n  p %&gt;% \n    html_nodes(node) %&gt;% \n    html_text()\n}\n\neconomania_raw_df &lt;- tibble(article_links) %&gt;% \n  sample_frac(.1) |&gt; \n  mutate(\n    p = map(article_links, insistently(read_html), .progress = TRUE),\n    text = map(p, get_text, node = \"p\"),\n    title = map(p, get_text, node = \"h1.entry-title\"),\n    title = map_chr(title, first),\n    author = map(p, possibly(get_text), node = \".byline a\"),\n    author = map(author, first),\n  )\n\nfind_author &lt;- function(author_condition, text) {\n  author &lt;- author_condition\n  if (author_condition == \"X\" | is.na(author_condition)) {\n    author &lt;- \"X\"\n    for (i in x_authors) {\n      if (any(str_detect(text, pattern = i))) {\n        author &lt;- i\n        break\n      }\n    }\n  }\n  author\n}\n\nx_authors &lt;- c(\"Gran√°t Marcell\",\n               \"Sulyok Andr√°s\", \n               \"Marczis D√°vid\", \n               \"Szabadkai D√°niel\",\n               \"L√≥r√°nt Bal√°zs\",\n               \"Nagy Oliv√©r\",\n               \"Malatinszky G√°bor\",\n               \"V√°rged√≥ B√°lint\",\n               \"Nagy M√°rton\",\n               \"Bartha Krist√≥f\",\n               \"Heilmann Istv√°n\",\n               \"V√°rged≈ë B√°lint\",\n               \"Kiss-Mih√°ly Norbert\",\n               \"T√≥falvi N√≥ra\",\n               \"K√°lm√°n P√©ter\",\n               \"Moldicz Csaba\",\n               \"Nagy Benj√°min\",\n               \"R√°cz Oliv√©r\",\n               \"T√≥th G√°bor\",\n               \"Mik√≥ Szabolcs\",\n               \"Csontos Tam√°s Tibor\",\n               \"Siket Bence\",\n               \"Szalai Zolt√°n\",\n               \"Horv√°th D√°niel\",\n               \"Hardi Zsuzsanna √©s Szap√°ry Gy√∂rgy\",\n               \"Pavelka Alexandra\",\n               \"Horv√°th G√°bor\",\n               \"Bal√°zs Fl√≥ra\",\n               \"Dancsik B√°lint\",\n               \"Marincs√°k K√°lm√°n √Årp√°d\",\n               \"Farkas S√°ra √©s Gutpint√©r J√∫lia\",\n               \"Palotai D√°niel\", \n               \"dr. Csillik P√©ter √©s Gutpint√©r J√∫lia\",\n               \"Oliver Knels\")\n\neconomania_df &lt;- economania_raw_df %&gt;% \n  select(-p) %&gt;% \n  distinct(article_links, .keep_all = TRUE) %&gt;% \n  mutate(\n    author_cleaned = map2_chr(author, text, find_author),\n    text = map(text, setdiff, y = \"Economania blog\"),\n    text = map(text, .f = function(t) discard(t, str_detect, \"View all\")),\n    text = map(text, .f = function(t) discard(t, str_detect, \"F≈ëoldali k√©p forr√°sa:\")),\n    text = map(text, .f = function(t) discard(t, str_detect, \"Hozz√°sz√≥l√°sok letiltva.\")),\n    text = map_chr(text, str_flatten, \" \"),\n    text = map2_chr(text, author, ~ gsub(str_c(gsub(\"√©s.*\", \"\", .y), \".*\"), \"\", .x)),\n    text = gsub(\" Hivatkoz√°sok.*\", \"\", text),\n    text = gsub(\" References.*\", \"\", text),\n    lang = textcat::textcat(text),\n    time = str_extract(article_links, \"20\\\\d\\\\d/\\\\d\\\\d/\\\\d\\\\d\"),\n    time = lubridate::ymd(time)\n  ) |&gt; \n  drop_na(text) \n\nwrite_rds(economania_df, file = \"economania_df.rds\")\n\n\nlibrary(tidytext)\n\neconomania_df %&gt;% \n  filter(lang == \"hungarian\") |&gt; \n  select(author = author_cleaned, time, text) |&gt; \n  unnest_tokens(word, text) |&gt; \n  mutate(\n    word = SnowballC::wordStem(word, language = \"hungarian\")\n  ) |&gt; \n  count(author, word) |&gt; \n  bind_tf_idf(word, author, n)\n\n# A tibble: 10,976 √ó 6\n   author          word      n       tf   idf   tf_idf\n   &lt;chr&gt;           &lt;chr&gt; &lt;int&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 Baranyai Eszter 1         3 0.00214  0.442 0.000945\n 2 Baranyai Eszter 10        1 0.000713 0.693 0.000494\n 3 Baranyai Eszter 11        1 0.000713 1.95  0.00139 \n 4 Baranyai Eszter 12        1 0.000713 1.54  0.00110 \n 5 Baranyai Eszter 14        2 0.00143  2.64  0.00376 \n 6 Baranyai Eszter 15        1 0.000713 1.54  0.00110 \n 7 Baranyai Eszter 1963      1 0.000713 2.64  0.00188 \n 8 Baranyai Eszter 1971      1 0.000713 2.64  0.00188 \n 9 Baranyai Eszter 1995      1 0.000713 2.64  0.00188 \n10 Baranyai Eszter 2         2 0.00143  0.693 0.000988\n# ‚Ñπ 10,966 more rows\n\n\n\nlibrary(topicmodels)\n\n\nfit &lt;- economania_df |&gt; \n  pull(text) |&gt; \n  dfm() |&gt; \n  topicmodels::LDA(k = 5, control = list(seed = 1234))\n\nError in dfm(pull(economania_df, text)): could not find function \"dfm\"\n\nbroom::tidy(fit, matrix = \"beta\")\n\nError in eval(expr, envir, enclos): object 'fit' not found\n\nbroom::tidy(fit, matrix = \"gamma\")\n\nError in eval(expr, envir, enclos): object 'fit' not found"
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Readings, lectures, and videos",
    "section": "",
    "text": "This website contains readings related to the lecture. I don‚Äôt expect you to read the chapters before the class, as we‚Äôll cover everything during the lessons. If you have any questions or find any mistakes, please don‚Äôt hesitate to reach out to me."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            Big Data & Data Visualisation\n        ",
    "section": "",
    "text": "Big Data & Data Visualisation\n        \n        \n            Learn how to analyse, understand, and communicate about your data!\n        \n        \n            Fall 2023 MNB Insitute John von Neumann University\n        \n    \n    \n        \n            \n            \n            \n        \n    \n\n\n\n\n\nInstructor\n\n ¬† Marcell Gran√°t\n ¬† Infopark I Building\n ¬† granat.marcell@nje.hu\n ¬† Schedule an appointment\n\n\n\nCourse details\n\n ¬† on Tuesdays\n ¬† Sept 11‚ÄìDecember 02, 2023\n ¬† 15:00-18.30\n ¬† Neumann Room (computer lab)\n\n\n\nContacting me\nE-mails or Teams are the best ways to get in contact with me. I will try to respond to all course-related e-mails and messages within 24 hours. Feel free to schedule an appointment for an online meeting (especially if I am your supervisor)."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Here‚Äôs your roadmap for the semester!\n\nContent (): This page contains the readings, slides, and recorded lectures for the topic. Read and watch these first.\nAssignment (): This page contains the instructions for each assignment. Assignments are due by 11:59 PM on the day they‚Äôre listed.\n\n\n\n\n\n\n\nSubscribe!\n\n\n\nYou can subscribe to this calendar URL in Outlook, Google Calendar, or Apple Calendar:\n\n\n\n ‚ÄÉDownload"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Marcell Gran√°t\n\n\n ¬† Infopark I Building\n\n ¬† granat.marcell@nje.hu\n\n\n ¬† Schedule an appointment\n\n\n\n\n ¬† on Tuesdays\n\n ¬† Sept 11‚ÄìDecember 02, 2023\n\n ¬† 15:00-18.30\n\n ¬† Neumann Room (computer lab)"
  },
  {
    "objectID": "syllabus.html#course-objectives",
    "href": "syllabus.html#course-objectives",
    "title": "Syllabus",
    "section": "Course objectives",
    "text": "Course objectives\n\nIn the digital era, an abundant amount of data is created every day, which contains valuable information about the economy, but their proper handling is not trivial. Data analysis and visualization have now become one of the most important skills in business, but in the world of research, it is clearly the most important.\n\nThis course introduces several statistical and visualization methods that helps to work with data, such as advanced inferential statistics and dimension reduction techniques. In addition, we will put a lot of effort into helping you to use  programming language (properly) to be able to apply these tools in practice."
  },
  {
    "objectID": "syllabus.html#topics",
    "href": "syllabus.html#topics",
    "title": "Syllabus",
    "section": "Topics",
    "text": "Topics\n\n\n\n\n\n\nCaution\n\n\n\nThis is just a plan! We are still in the early stages of this program. We don‚Äôt have much experience with how much preliminary knowledge you have, so the final agenda may change a bit as we progress.\n\n\n\n\n\n\n\n\n\nWeek\n      Topic\n    \n\n\n1\nIntroduction to R, Descriptive Statistics\n\n\n2\nData Manipulation I\n\n\n3\nData Manipulation II, Functional Programming\n\n\n4\nRegression, Classification\n\n\n5\nData Visualization I\n\n\n6\nData Visualization II, Tools for Publishing\n\n\n7\nCasuality\n\n\n8\nTidy Modelling\n\n\n9\nClustering\n\n\n10\nWeb scraping, Dimension Reduction\n\n\n11\nText Mining"
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Syllabus",
    "section": "Course materials",
    "text": "Course materials\nReadings that I highly recommend for this course are free, as this course focuses primarily on the technical aspects of data analysis, which are rapidly evolving. The majority of pertinent sources are available online.\nBooks\nHighly recommended (and also free)\n\nWickham, H., √áetinkaya-Rundel, M., & Grolemund, G. (2023).¬†R for data science. ‚Äù O‚ÄôReilly Media, Inc.‚Äù\nWitten, D., & James, G. (2013).¬†An introduction to statistical learning with applications in R. springer publication.\nKuhn, M., & Silge, J. (2022).¬†Tidy modeling with R. ‚Äù O‚ÄôReilly Media, Inc.‚Äù\nVillanueva, R. A. M., & Chen, Z. J. (2019). ggplot2: elegant graphics for data analysis.\nRecommended\n\nG√°bor B√©k√©s, G√°bor K√©zdi. 2021. Data Analysis for Business, Economics, and Policy. Cambridge University Press\nCole Nussbaumer Knaflic. 2015. Storytelling with Data: A Data Visualization Guide for Business Professionals. 1st Edition. Wiley\nSchedule an appointment\nSeriously. Feel free to schedule an appointment for an online meeting (especially if I am your supervisor). I strive to keep this calendar up-to-date, so that the vacant slots are accessible to you. Oh, and it would be greatly appreciated if you could provide a brief overview of the topic we will be discussing, so I can do any necessary preparation beforehand."
  },
  {
    "objectID": "syllabus.html#assignments",
    "href": "syllabus.html#assignments",
    "title": "Syllabus",
    "section": "Assignments",
    "text": "Assignments\nYou can find the assignments on the assignments page.\nYou have the opportunity to fill out the task multiple times, but for personal reasons, I ask you not to try it for too long (I pay for the server usage fee, not the university), or run it manually. Simply copy and paste the following code into your R console, and you will be all set (You must install both of those packages first):\n\n# install.packages(\"git2r\")\n# install.packages(\"rmarkdown\")\n# install.packages(\"learnr\")\n# install.packages(\"devtools\")\n# install.packages(\"httr\")\n# devtools::install_github(\"rundel/learnrhash\")\n# install.packages(\"tidyverse\")\n# devtools::install_github(\"rstudio/gradethis\")\n# install.packages(\"shinyalert\")\n\nunlink(\"repo\", recursive = TRUE)\ngit2r::clone(url = \"https://github.com/MarcellGranat/bigdata2023_learnr.git\", local_path = \"repo\")\nrmarkdown::run(\"repo/learnr.Rmd\")\n\nOnce you have completed all the tasks, simply click on the generate button to obtain a hash code. If you got a message that your results are saved, then its saved, and you do not have any additional task. But if you do not get it, then send this hash code to me via email (the server is on local computers and anything may happen ü§∑‚Äç‚ôÇÔ∏è).\nThis hash code contains your answers as well as other relevant information about your submission. You must complete each problem set by Monday, 23:59 of the following week.\nLate work\nYou will lose 0.5 point per day for each day a problem set is late (yes, even if it‚Äôs only 5 minutes after the deadline). The table with current points are automatically generated from the server."
  },
  {
    "objectID": "syllabus.html#grades",
    "href": "syllabus.html#grades",
    "title": "Syllabus",
    "section": "Grades",
    "text": "Grades\nYou will earn a significant portion of the points with the end-of-semester written exam, but passing the course also requires completing (minimum 24 points from the 48) the ongoing problem set assignments throughout the year.\n\n\n\n\n\n\n\nAssignment\n      Points\n      Percent\n    \n\n\nProblem sets\n50\n20%\n\n\nExam\n200\n80%\n\n\nTotal\n250\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\n      Range\n    \n\n\n5\n90‚Äì100%\n\n\n4\n80‚Äì89%\n\n\n3\n66‚Äì79%\n\n\n2\n50‚Äì65%\n\n\n1\n0‚Äì49%"
  }
]