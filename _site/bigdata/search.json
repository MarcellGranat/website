[
  {
    "objectID": "content/01-content.html",
    "href": "content/01-content.html",
    "title": "The base R",
    "section": "",
    "text": "üìà Handling large datasets: Economic data often consists of thousands or even millions of observations. Programming enables us to organize and process this data effectively. By writing code, we can automate repetitive tasks, explore the data, and perform calculations on a scale that would be impractical or time-consuming with manual methods.\n\nüßπ Data cleaning and preprocessing: Real-world data is often messy and inconsistent. Programming allows us to clean and preprocess the data, removing any errors, inconsistencies, or missing values. By writing code to handle such data cleaning tasks, we can ensure the accuracy and integrity of our analysis.\n\nüîÅ Reproducibility: Programming promotes reproducibility in statistical analysis. By documenting and sharing our code, others can replicate our analyses, verify our findings, and build upon our work. This promotes transparency and strengthens the validity of our results.\n\nüõÉ Flexibility and customization: Programming languages like  and Python provide a wide range of statistical libraries and packages specifically designed for data analysis. These libraries offer various functions and algorithms to perform statistical tests, regression models, or other techniques. The ability to customize and tailor these tools to specific research questions allows for more precise and detailed analysis.\n\n\n\n\n\nProgramming, like any other skill, requires practice and persistence ‚öíÔ∏è. As Hadley Wickham, a prominent figure in the  community, once said, ‚ÄúThe only way to write good code is to write tons of subpar code first. Feeling shame about bad code stops you from getting to good code.‚Äù üöÄ This sentiment underscores the importance of perseverance and learning from mistakes in the journey of mastering ."
  },
  {
    "objectID": "content/01-content.html#example---fertility-rates-by-country-by-year-1",
    "href": "content/01-content.html#example---fertility-rates-by-country-by-year-1",
    "title": "The base R",
    "section": "Example - Fertility rates by country / by year",
    "text": "Example - Fertility rates by country / by year\nData from OECD\n\n\nData\nCode to reproduce\n\n\n\n\nfertility_df\n\n      AUS  AUT  BEL  CAN  CZE  DNK  FIN  FRA  DEU  GRC  HUN  ISL  IRL  ITA  JPN\n1960 3.45 2.69 2.54 3.90 2.11 2.54 2.71 2.74 2.37 2.23 2.02 4.26 3.76 2.41 2.00\n1961 3.55 2.78 2.63 3.84 2.13 2.55 2.65 2.82 2.44 2.13 1.94 3.88 3.79 2.41 1.96\n1962 3.43 2.80 2.59 3.76 2.14 2.54 2.66 2.80 2.44 2.16 1.79 3.98 3.92 2.46 1.98\n1963 3.34 2.82 2.68 3.67 2.33 2.64 2.66 2.90 2.51 2.14 1.82 3.98 4.01 2.56 2.00\n1964 3.15 2.79 2.71 3.50 2.36 2.60 2.58 2.91 2.53 2.24 1.80 3.86 4.06 2.70 2.05\n1965 2.97 2.70 2.61 3.15 2.18 2.61 2.46 2.85 2.50 2.25 1.81 3.71 4.03 2.67 2.14\n1966 2.89 2.66 2.52 2.81 2.01 2.62 2.40 2.80 2.51 2.32 1.88 3.58 3.95 2.63 1.58\n1967 2.85 2.62 2.41 2.60 1.90 2.35 2.32 2.67 2.45 2.45 2.01 3.28 3.84 2.54 2.23\n1968 2.89 2.58 2.31 2.45 1.83 2.12 2.15 2.59 2.36 2.42 2.06 3.07 3.78 2.49 2.13\n1969 2.89 2.49 2.27 2.40 1.86 2.00 1.94 2.53 2.21 2.36 2.04 2.99 3.85 2.51 2.13\n1970 2.86 2.29 2.25 2.33 1.91 1.95 1.83 2.48 2.03 2.40 1.97 2.81 3.87 2.42 2.13\n1971 2.95 2.20 2.21 2.19 1.98 2.04 1.70 2.50 1.97 2.32 1.92 2.92 3.98 2.41 2.16\n1972 2.74 2.08 2.09 2.02 2.07 2.03 1.59 2.42 1.74 2.32 1.93 3.09 3.88 2.36 2.14\n1973 2.49 1.94 1.95 1.93 2.29 1.92 1.50 2.31 1.56 2.27 1.95 2.95 3.74 2.34 2.14\n1974 2.32 1.91 1.83 1.82 2.43 1.90 1.62 2.11 1.53 2.38 2.30 2.66 3.62 2.33 2.05\n1975 2.15 1.83 1.74 1.80 2.40 1.92 1.69 1.93 1.48 2.33 2.38 2.65 3.40 2.21 1.91\n1976 2.06 1.69 1.73 1.76 2.36 1.75 1.72 1.83 1.51 2.35 2.26 2.52 3.31 2.11 1.85\n1977 2.01 1.63 1.71 1.75 2.32 1.66 1.69 1.86 1.51 2.27 2.17 2.31 3.27 1.97 1.80\n1978 1.95 1.60 1.69 1.70 2.32 1.67 1.65 1.82 1.50 2.28 2.08 2.35 3.24 1.87 1.79\n1979 1.91 1.60 1.69 1.70 2.29 1.60 1.64 1.86 1.50 2.26 2.02 2.49 3.23 1.76 1.77\n1980 1.89 1.65 1.68 1.68 2.10 1.55 1.63 1.95 1.56 2.23 1.92 2.48 3.23 1.68 1.75\n1981 1.94 1.67 1.66 1.65 2.02 1.44 1.65 1.95 1.53 2.10 1.88 2.33 3.07 1.60 1.74\n1982 1.93 1.66 1.61 1.64 2.01 1.43 1.72 1.91 1.51 2.03 1.78 2.26 2.96 1.60 1.77\n1983 1.92 1.56 1.57 1.63 1.96 1.38 1.74 1.78 1.43 1.94 1.73 2.24 2.76 1.54 1.80\n1984 1.84 1.52 1.54 1.63 1.97 1.40 1.70 1.80 1.39 1.82 1.73 2.08 2.59 1.48 1.81\n1985 1.92 1.47 1.51 1.61 1.96 1.45 1.64 1.81 1.37 1.68 1.83 1.93 2.50 1.45 1.76\n1986 1.87 1.45 1.54 1.59 1.94 1.48 1.60 1.83 1.41 1.60 1.83 1.93 2.44 1.37 1.72\n1987 1.85 1.43 1.54 1.58 1.91 1.50 1.59 1.80 1.43 1.50 1.81 2.07 2.31 1.35 1.69\n1988 1.83 1.45 1.57 1.60 1.94 1.56 1.70 1.81 1.46 1.50 1.79 2.27 2.17 1.38 1.66\n1989 1.84 1.45 1.58 1.66 1.87 1.62 1.71 1.79 1.42 1.40 1.78 2.20 2.08 1.35 1.57\n1990 1.90 1.46 1.62 1.71 1.89 1.67 1.79 1.78 1.45 1.39 1.84 2.31 2.12 1.36 1.54\n1991 1.85 1.51 1.66 1.72 1.86 1.68 1.80 1.77 1.33 1.37 1.86 2.19 2.09 1.33 1.53\n1992 1.89 1.51 1.65 1.71 1.72 1.76 1.85 1.73 1.29 1.36 1.77 2.21 1.99 1.32 1.50\n1993 1.86 1.50 1.61 1.69 1.67 1.75 1.81 1.66 1.28 1.32 1.69 2.22 1.91 1.26 1.46\n1994 1.84 1.47 1.56 1.69 1.44 1.81 1.85 1.66 1.24 1.33 1.64 2.14 1.85 1.22 1.50\n1995 1.82 1.42 1.56 1.67 1.28 1.81 1.81 1.71 1.25 1.28 1.57 2.08 1.85 1.19 1.42\n1996 1.80 1.45 1.59 1.63 1.19 1.75 1.76 1.73 1.32 1.26 1.46 2.12 1.89 1.22 1.43\n1997 1.78 1.39 1.60 1.57 1.17 1.76 1.75 1.73 1.37 1.27 1.38 2.04 1.94 1.23 1.39\n1998 1.76 1.37 1.60 1.56 1.16 1.73 1.71 1.76 1.36 1.24 1.33 2.05 1.95 1.21 1.38\n1999 1.76 1.34 1.62 1.55 1.13 1.74 1.73 1.79 1.36 1.23 1.29 1.99 1.91 1.23 1.34\n2000 1.76 1.36 1.67 1.51 1.14 1.77 1.73 1.87 1.38 1.25 1.33 2.08 1.90 1.26 1.36\n2001 1.73 1.33 1.67 1.54 1.15 1.75 1.73 1.88 1.35 1.25 1.31 1.95 1.96 1.25 1.33\n2002 1.77 1.39 1.65 1.52 1.17 1.72 1.72 1.86 1.34 1.28 1.31 1.93 1.98 1.27 1.32\n2003 1.77 1.38 1.67 1.55 1.18 1.76 1.76 1.87 1.34 1.29 1.28 1.99 1.98 1.29 1.29\n2004 1.78 1.42 1.72 1.56 1.23 1.79 1.80 1.90 1.36 1.31 1.28 2.03 1.95 1.34 1.29\n2005 1.85 1.41 1.76 1.58 1.28 1.80 1.80 1.92 1.34 1.34 1.32 2.05 1.88 1.33 1.26\n2006 1.88 1.41 1.80 1.63 1.33 1.85 1.84 1.98 1.33 1.40 1.35 2.07 1.94 1.37 1.32\n2007 1.99 1.39 1.82 1.67 1.44 1.84 1.83 1.95 1.37 1.41 1.32 2.09 2.01 1.39 1.34\n2008 2.02 1.42 1.85 1.70 1.50 1.89 1.85 1.99 1.38 1.50 1.35 2.14 2.06 1.44 1.37\n2009 1.97 1.40 1.84 1.69 1.49 1.84 1.86 1.99 1.36 1.50 1.33 2.22 2.06 1.44 1.37\n2010 1.95 1.44 1.86 1.65 1.49 1.87 1.87 2.02 1.39 1.48 1.26 2.20 2.05 1.44 1.39\n2011 1.92 1.43 1.81 1.63 1.43 1.75 1.83 2.00 1.39 1.40 1.24 2.02 2.03 1.42 1.39\n2012 1.93 1.44 1.80 1.63 1.45 1.73 1.80 1.99 1.41 1.34 1.34 2.04 1.98 1.42 1.41\n2013 1.88 1.44 1.76 1.61 1.46 1.67 1.75 1.97 1.42 1.29 1.34 1.93 1.93 1.39 1.43\n2014 1.79 1.46 1.74 1.61 1.53 1.69 1.71 1.97 1.47 1.30 1.41 1.93 1.89 1.38 1.42\n2015 1.79 1.49 1.70 1.60 1.57 1.71 1.65 1.93 1.50 1.33 1.44 1.81 1.85 1.36 1.45\n2016 1.79 1.53 1.68 1.59 1.63 1.79 1.57 1.89 1.59 1.38 1.49 1.75 1.82 1.36 1.44\n2017 1.74 1.52 1.65 1.55 1.69 1.75 1.49 1.86 1.57 1.35 1.49 1.71 1.78 1.34 1.43\n2018 1.74 1.48 1.62 1.51 1.71 1.73 1.41 1.84 1.57 1.35 1.49 1.71 1.75 1.31 1.42\n2019 1.67 1.46 1.60 1.47 1.71 1.70 1.35 1.83 1.54 1.34 1.49 1.75 1.70 1.27 1.36\n2020 1.59 1.44 1.55 1.41 1.71 1.67 1.37 1.79 1.53 1.39 1.56 1.72 1.63 1.24 1.33\n2021 1.70 1.48 1.60 1.43 1.83 1.72 1.46 1.80 1.58 1.43 1.59 1.82 1.72 1.25 1.30\n2022   NA   NA   NA   NA   NA 1.55   NA   NA   NA   NA 1.52   NA   NA   NA   NA\n      KOR  LUX  MEX  NLD  NZL  NOR  POL  PRT  SVK  ESP  SWE  CHE  TUR  GBR  USA\n1960 6.00 2.28 6.77 3.12 4.24 2.91 2.98 3.10 3.07 2.86 2.20 2.44 6.40 2.72 3.65\n1961 5.80 2.33 6.76 3.22 4.31 2.94 2.83 3.16 2.96 2.76 2.23 2.53 6.33 2.80 3.62\n1962 5.60 2.35 6.76 3.18 4.19 2.91 2.72 3.21 2.83 2.80 2.26 2.60 6.26 2.88 3.46\n1963 5.40 2.33 6.75 3.19 4.05 2.93 2.70 3.11 2.93 2.88 2.34 2.67 6.19 2.92 3.32\n1964 5.20 2.38 6.75 3.17 3.80 2.98 2.57 3.21 2.91 3.01 2.48 2.68 6.01 2.97 3.19\n1965 5.00 2.42 6.76 3.04 3.54 2.94 2.52 3.14 2.80 2.94 2.42 2.61 5.84 2.89 2.91\n1966 4.80 2.37 6.77 2.90 3.41 2.90 2.34 3.12 2.67 2.99 2.36 2.52 5.66 2.79 2.72\n1967 4.66 2.25 6.79 2.81 3.35 2.81 2.33 3.08 2.49 3.03 2.27 2.41 5.49 2.69 2.56\n1968 4.52 2.13 6.81 2.72 3.34 2.75 2.24 3.00 2.40 2.96 2.07 2.30 5.31 2.60 2.46\n1969 4.53 2.02 6.83 2.75 3.28 2.69 2.20 2.95 2.43 2.93 1.93 2.19 5.18 2.51 2.46\n1970 4.53 1.98 6.83 2.57 3.17 2.50 2.20 2.83 2.40 2.90 1.94 2.10 5.00 2.43 2.48\n1971 4.54 1.96 6.79 2.36 3.18 2.49 2.25 2.78 2.43 2.88 1.96 2.04 5.00 2.40 2.27\n1972 4.12 1.75 6.70 2.15 3.00 2.38 2.24 2.69 2.49 2.86 1.91 1.91 5.00 2.20 2.01\n1973 4.07 1.58 6.56 1.90 2.76 2.23 2.26 2.65 2.56 2.84 1.86 1.81 5.59 2.04 1.88\n1974 3.77 1.58 6.37 1.77 2.58 2.13 2.26 2.60 2.60 2.89 1.87 1.73 5.46 1.92 1.84\n1975 3.43 1.55 6.13 1.66 2.37 1.98 2.27 2.58 2.53 2.80 1.77 1.61 5.32 1.81 1.77\n1976 3.00 1.48 5.86 1.63 2.27 1.86 2.30 2.58 2.52 2.80 1.68 1.55 5.19 1.74 1.74\n1977 2.99 1.49 5.59 1.58 2.21 1.75 2.23 2.48 2.47 2.67 1.64 1.53 4.90 1.69 1.79\n1978 2.64 1.47 5.32 1.58 2.07 1.77 2.21 2.28 2.45 2.55 1.60 1.51 5.05 1.75 1.76\n1979 2.90 1.47 5.06 1.56 2.12 1.75 2.28 2.17 2.44 2.37 1.66 1.52 4.84 1.86 1.81\n1980 2.82 1.50 4.84 1.60 2.03 1.72 2.28 2.18 2.31 2.22 1.68 1.55 4.63 1.90 1.84\n1981 2.57 1.55 4.64 1.56 2.01 1.70 2.24 2.13 2.28 2.04 1.63 1.55 4.41 1.82 1.81\n1982 2.39 1.49 4.46 1.50 1.95 1.71 2.34 2.07 2.27 1.94 1.62 1.56 4.20 1.78 1.83\n1983 2.06 1.44 4.30 1.47 1.92 1.66 2.42 1.95 2.27 1.80 1.61 1.52 4.11 1.77 1.80\n1984 1.74 1.42 4.15 1.49 1.93 1.66 2.37 1.90 2.25 1.73 1.65 1.53 3.93 1.77 1.81\n1985 1.66 1.38 4.02 1.51 1.93 1.68 2.33 1.72 2.25 1.64 1.73 1.52 3.76 1.79 1.84\n1986 1.58 1.44 3.90 1.55 1.96 1.71 2.22 1.66 2.20 1.56 1.79 1.53 3.58 1.78 1.84\n1987 1.53 1.39 3.79 1.56 2.03 1.75 2.15 1.62 2.14 1.50 1.84 1.52 3.40 1.81 1.87\n1988 1.55 1.51 3.68 1.55 2.10 1.84 2.13 1.62 2.15 1.45 1.96 1.57 3.29 1.82 1.93\n1989 1.56 1.52 3.57 1.55 2.12 1.89 2.07 1.58 2.08 1.40 2.02 1.56 3.39 1.79 2.01\n1990 1.57 1.62 3.47 1.62 2.18 1.93 1.99 1.56 2.09 1.36 2.14 1.59 3.07 1.83 2.08\n1991 1.71 1.60 3.37 1.61 2.09 1.92 1.98 1.56 2.05 1.33 2.12 1.58 3.00 1.82 2.06\n1992 1.76 1.67 3.27 1.59 2.06 1.89 1.85 1.53 1.99 1.32 2.09 1.58 2.93 1.79 2.05\n1993 1.65 1.69 3.18 1.57 2.04 1.86 1.77 1.51 1.93 1.27 2.00 1.51 2.87 1.76 2.02\n1994 1.66 1.72 3.10 1.57 1.98 1.87 1.72 1.44 1.67 1.20 1.89 1.49 2.81 1.74 2.00\n1995 1.63 1.67 3.02 1.53 1.98 1.87 1.55 1.41 1.52 1.17 1.74 1.48 2.75 1.71 1.98\n1996 1.57 1.76 2.95 1.53 1.96 1.89 1.53 1.44 1.47 1.16 1.61 1.50 2.69 1.73 1.98\n1997 1.54 1.71 2.88 1.56 1.96 1.86 1.47 1.47 1.43 1.18 1.53 1.48 2.63 1.72 1.97\n1998 1.46 1.67 2.82 1.63 1.89 1.81 1.41 1.48 1.37 1.16 1.51 1.47 2.56 1.71 2.00\n1999 1.43 1.71 2.77 1.65 1.97 1.85 1.37 1.51 1.33 1.19 1.50 1.48 2.48 1.68 2.01\n2000 1.48 1.78 2.72 1.72 1.98 1.85 1.37 1.56 1.29 1.23 1.55 1.50 2.27 1.64 2.06\n2001 1.31 1.66 2.67 1.71 1.97 1.78 1.32 1.46 1.20 1.24 1.57 1.38 2.37 1.63 2.03\n2002 1.18 1.63 2.62 1.73 1.89 1.75 1.25 1.47 1.19 1.25 1.65 1.39 2.17 1.63 2.01\n2003 1.19 1.62 2.58 1.75 1.93 1.80 1.22 1.44 1.20 1.30 1.72 1.39 2.09 1.70 2.04\n2004 1.16 1.66 2.54 1.73 1.98 1.83 1.23 1.41 1.24 1.31 1.75 1.42 2.11 1.75 2.05\n2005 1.09 1.62 2.50 1.71 1.97 1.84 1.24 1.42 1.25 1.33 1.77 1.42 2.12 1.76 2.06\n2006 1.13 1.64 2.46 1.72 2.01 1.90 1.27 1.38 1.24 1.36 1.85 1.44 2.12 1.82 2.11\n2007 1.26 1.61 2.42 1.72 2.18 1.90 1.31 1.35 1.25 1.38 1.88 1.46 2.16 1.86 2.12\n2008 1.19 1.60 2.39 1.77 2.19 1.96 1.39 1.40 1.32 1.45 1.91 1.48 2.15 1.91 2.07\n2009 1.15 1.59 2.36 1.79 2.13 1.98 1.40 1.35 1.41 1.38 1.94 1.50 2.10 1.89 2.00\n2010 1.23 1.63 2.34 1.80 2.17 1.95 1.38 1.39 1.40 1.37 1.98 1.54 2.08 1.92 1.93\n2011 1.24 1.51 2.32 1.76 2.09 1.88 1.30 1.35 1.45 1.34 1.90 1.52 2.05 1.91 1.89\n2012 1.30 1.57 2.29 1.72 2.10 1.85 1.30 1.29 1.34 1.32 1.91 1.53 2.11 1.92 1.88\n2013 1.19 1.55 2.27 1.68 2.01 1.78 1.26 1.21 1.34 1.27 1.89 1.52 2.11 1.83 1.86\n2014 1.21 1.50 2.21 1.71 1.92 1.76 1.29 1.23 1.37 1.32 1.88 1.54 2.18 1.81 1.86\n2015 1.24 1.47 2.14 1.66 1.99 1.73 1.29 1.31 1.40 1.33 1.85 1.54 2.15 1.80 1.84\n2016 1.17 1.41 2.09 1.66 1.87 1.71 1.36 1.36 1.48 1.34 1.85 1.54 2.11 1.79 1.82\n2017 1.05 1.39 2.04 1.62 1.81 1.62 1.45 1.38 1.52 1.31 1.78 1.52 2.07 1.74 1.77\n2018 0.98 1.38 2.00 1.59 1.71 1.56 1.44 1.42 1.54 1.26 1.75 1.52 1.99 1.68 1.73\n2019 0.92 1.34 1.92 1.57 1.72 1.53 1.42 1.43 1.57 1.23 1.70 1.48 1.88 1.63 1.71\n2020 0.84 1.36 1.91 1.54 1.61 1.48 1.39 1.41 1.59 1.19 1.66 1.46 1.76 1.56 1.64\n2021 0.81 1.38 1.82 1.62 1.64 1.55 1.33 1.35 1.63 1.19 1.67 1.51 1.70 1.53 1.66\n2022   NA   NA   NA   NA   NA 1.41   NA   NA   NA   NA   NA   NA   NA   NA   NA\n      BRA  CHL  CHN  EST  IND  IDN  ISR  RUS  SVN  ZAF  COL  LVA  LTU  ARG  BGR\n1960 6.06 4.70 4.45 1.98 5.91 5.55 3.95 2.52 2.18 6.16 6.74 1.94 2.40 3.11 2.31\n1961 6.03 4.66 3.86 1.98 5.90 5.57 3.80 2.45 2.26 6.14 6.71 1.94 2.40 3.10 2.29\n1962 5.98 4.60 6.09 1.95 5.89 5.59 3.77 2.36 2.27 6.11 6.66 1.91 2.40 3.09 2.24\n1963 5.91 4.54 7.51 1.89 5.88 5.60 3.81 2.27 2.28 6.08 6.58 1.85 2.40 3.08 2.21\n1964 5.82 4.46 6.67 1.94 5.86 5.61 3.93 2.18 2.32 6.03 6.48 1.79 2.40 3.07 2.19\n1965 5.70 4.36 6.61 1.88 5.83 5.62 3.99 2.13 2.45 5.97 6.33 1.74 2.40 3.06 2.09\n1966 5.57 4.26 6.31 1.87 5.79 5.60 3.89 2.10 2.48 5.91 6.16 1.76 2.40 3.05 2.03\n1967 5.42 4.14 5.81 1.90 5.75 5.58 3.64 2.04 2.38 5.85 5.96 1.80 2.40 3.05 2.02\n1968 5.27 4.03 6.51 2.03 5.70 5.54 3.82 1.99 2.28 5.78 5.74 1.83 2.40 3.05 2.27\n1969 5.12 3.90 6.18 2.13 5.65 5.51 3.83 1.97 2.17 5.72 5.51 1.88 2.40 3.06 2.27\n1970 4.97 3.78 6.09 2.17 5.59 5.45 3.97 1.99 2.21 5.63 5.28 2.02 2.40 3.08 2.17\n1971 4.84 3.65 5.52 2.19 5.52 5.36 3.94 2.03 2.16 5.57 5.06 2.04 2.41 3.11 2.10\n1972 4.71 3.53 5.11 2.13 5.44 5.29 3.71 2.04 2.14 5.49 4.86 2.03 2.34 3.15 2.03\n1973 4.60 3.41 4.73 2.06 5.36 5.22 3.68 2.01 2.18 5.41 4.68 1.96 2.22 3.20 2.15\n1974 4.50 3.29 4.17 2.07 5.28 5.09 3.71 2.00 2.10 5.30 4.53 2.00 2.21 3.25 2.29\n1975 4.42 3.18 3.57 2.04 5.19 5.04 3.68 1.98 2.16 5.19 4.40 1.97 2.18 3.30 2.23\n1976 4.34 3.08 3.24 2.07 5.11 4.92 3.70 1.97 2.17 5.07 4.29 1.93 2.18 3.34 2.24\n1977 4.27 2.98 2.84 2.06 5.03 4.81 3.47 1.95 2.16 4.94 4.18 1.89 2.14 3.36 2.21\n1978 4.20 2.89 2.72 2.02 4.96 4.72 3.28 1.92 2.19 4.85 4.07 1.87 2.08 3.36 2.15\n1979 4.12 2.81 2.75 2.00 4.89 4.61 3.21 1.90 2.22 4.82 3.97 1.87 2.05 3.34 2.16\n1980 4.04 2.74 2.74 2.02 4.83 4.49 3.14 1.89 2.11 4.78 3.86 1.90 1.99 3.30 2.05\n1981 3.94 2.69 2.79 2.07 4.77 4.36 3.06 1.91 1.96 4.71 3.74 1.90 1.98 3.25 2.00\n1982 3.84 2.65 2.97 2.08 4.70 4.25 3.12 2.04 1.93 4.70 3.63 1.98 1.97 3.20 2.01\n1983 3.72 2.62 2.56 2.16 4.64 4.10 3.21 2.11 1.82 4.63 3.53 2.13 2.10 3.16 2.01\n1984 3.60 2.60 2.61 2.17 4.56 3.94 3.13 2.06 1.75 4.57 3.43 2.15 2.07 3.12 2.01\n1985 3.47 2.59 2.63 2.12 4.48 3.71 3.12 2.05 1.72 4.50 3.34 2.09 2.08 3.10 1.97\n1986 3.34 2.59 2.72 2.17 4.40 3.53 3.09 2.15 1.65 4.41 3.27 2.21 2.12 3.08 2.02\n1987 3.23 2.59 2.76 2.26 4.31 3.42 3.05 2.22 1.64 4.35 3.21 2.21 2.11 3.06 1.96\n1988 3.11 2.59 2.54 2.26 4.22 3.33 3.06 2.12 1.63 4.18 3.16 2.16 2.02 3.04 1.97\n1989 3.01 2.59 2.52 2.22 4.13 3.22 3.03 2.01 1.52 3.98 3.12 2.05 1.98 3.02 1.90\n1990 2.91 2.58 2.51 2.05 4.05 3.10 3.02 1.89 1.46 3.72 3.08 2.01 2.03 3.00 1.82\n1991 2.82 2.56 1.93 1.80 3.96 3.06 2.91 1.73 1.42 3.62 3.05 1.86 2.01 2.97 1.66\n1992 2.72 2.53 1.78 1.71 3.88 2.94 2.93 1.55 1.34 3.48 3.01 1.73 1.97 2.93 1.55\n1993 2.67 2.48 1.69 1.49 3.80 2.88 2.92 1.39 1.33 3.37 2.97 1.51 1.74 2.88 1.46\n1994 2.62 2.43 1.63 1.42 3.72 2.84 2.90 1.40 1.32 3.26 2.92 1.39 1.57 2.83 1.37\n1995 2.58 2.37 1.59 1.38 3.65 2.80 2.88 1.34 1.29 3.17 2.86 1.26 1.55 2.77 1.23\n1996 2.52 2.31 1.55 1.37 3.58 2.77 2.94 1.27 1.28 2.99 2.80 1.16 1.49 2.72 1.23\n1997 2.47 2.24 1.53 1.32 3.51 2.74 2.93 1.22 1.25 2.73 2.74 1.11 1.47 2.67 1.09\n1998 2.41 2.17 1.52 1.28 3.45 2.66 2.98 1.23 1.23 2.63 2.68 1.10 1.46 2.62 1.11\n1999 2.33 2.11 1.53 1.30 3.38 2.58 2.94 1.16 1.21 2.56 2.63 1.18 1.46 2.58 1.23\n2000 2.26 2.06 1.63 1.36 3.35 2.54 2.95 1.20 1.26 2.41 2.57 1.25 1.39 2.54 1.26\n2001 2.18 2.01 1.56 1.32 3.30 2.50 2.89 1.22 1.21 2.37 2.52 1.22 1.29 2.51 1.21\n2002 2.10 1.97 1.57 1.36 3.22 2.46 2.89 1.29 1.21 2.32 2.46 1.26 1.23 2.49 1.21\n2003 2.02 1.94 1.57 1.36 3.12 2.43 2.95 1.32 1.20 2.36 2.40 1.32 1.26 2.46 1.23\n2004 2.00 1.92 1.61 1.47 3.05 2.42 2.90 1.34 1.25 2.44 2.33 1.29 1.27 2.44 1.29\n2005 1.97 1.91 1.62 1.52 2.96 2.43 2.84 1.29 1.26 2.51 2.26 1.39 1.29 2.42 1.32\n2006 1.93 1.90 1.64 1.58 2.86 2.45 2.88 1.31 1.31 2.55 2.20 1.46 1.33 2.40 1.38\n2007 1.88 1.90 1.67 1.69 2.78 2.49 2.90 1.42 1.38 2.55 2.14 1.54 1.36 2.38 1.49\n2008 1.84 1.90 1.70 1.72 2.72 2.48 2.96 1.50 1.53 2.68 2.08 1.58 1.45 2.37 1.56\n2009 1.83 1.89 1.71 1.70 2.67 2.46 2.96 1.54 1.53 2.50 2.03 1.46 1.50 2.36 1.66\n2010 1.81 1.88 1.69 1.72 2.60 2.45 3.03 1.57 1.57 2.44 1.99 1.36 1.50 2.35 1.57\n2011 1.80 1.82 1.67 1.61 2.54 2.50 3.00 1.58 1.56 2.44 1.96 1.33 1.55 2.34 1.51\n2012 1.77 1.80 1.80 1.56 2.47 2.49 3.05 1.69 1.58 2.45 1.93 1.44 1.60 2.33 1.50\n2013 1.75 1.79 1.71 1.52 2.41 2.43 3.03 1.71 1.55 2.43 1.91 1.52 1.59 2.32 1.48\n2014 1.77 1.77 1.77 1.54 2.31 2.39 3.08 1.75 1.58 2.42 1.88 1.65 1.63 2.31 1.53\n2015 1.78 1.74 1.67 1.58 2.29 2.35 3.09 1.78 1.57 2.36 1.86 1.70 1.70 2.30 1.53\n2016 1.71 1.68 1.77 1.60 2.27 2.31 3.11 1.76 1.58 2.26 1.84 1.74 1.69 2.24 1.54\n2017 1.74 1.60 1.81 1.59 2.20 2.26 3.11 1.62 1.62 2.33 1.82 1.69 1.63 2.17 1.56\n2018 1.75 1.56 1.55 1.67 2.18 2.23 3.09 1.58 1.60 2.42 1.79 1.60 1.63 2.04 1.56\n2019 1.70 1.55 1.50 1.66 2.11 2.22 3.01 1.50 1.61 2.48 1.77 1.61 1.61 1.99 1.58\n2020 1.65 1.54 1.28 1.58 2.05 2.19 2.90 1.51 1.59 2.40 1.74 1.55 1.48 1.91 1.56\n2021 1.64 1.54 1.16 1.61 2.03 2.17 3.00 1.49 1.64 2.37 1.72 1.57 1.36 1.89 1.58\n2022   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA   NA\n      HRV  CYP  MLT  ROU  SAU  PER  CRI   EU\n1960 2.20   NA   NA   NA 7.63 6.94 6.71 2.62\n1961 2.19   NA   NA   NA 7.63 6.92 6.65 2.62\n1962 2.17   NA   NA   NA 7.64 6.90 6.54 2.61\n1963 2.12   NA   NA   NA 7.65 6.86 6.39 2.65\n1964 2.12   NA   NA   NA 7.67 6.81 6.19 2.67\n1965 2.21   NA   NA   NA 7.66 6.75 5.96 2.62\n1966 2.21   NA   NA   NA 7.66 6.68 5.70 2.58\n1967 2.07   NA   NA   NA 7.66 6.60 5.42 2.53\n1968 1.99   NA   NA   NA 7.63 6.51 5.03 2.45\n1969 1.91   NA   NA   NA 7.60 6.42 4.84 2.41\n1970 1.83   NA   NA   NA 7.58 6.32 4.59 2.37\n1971 1.95   NA   NA   NA 7.56 6.21 4.36 2.36\n1972 1.97   NA   NA   NA 7.54 6.09 4.16 2.29\n1973 1.98   NA   NA   NA 7.48 5.97 3.99 2.23\n1974 1.95   NA   NA   NA 7.43 5.84 3.89 2.23\n1975 1.92   NA   NA 2.59 7.37 5.71 3.80 2.18\n1976 1.90   NA   NA 2.54 7.33 5.58 3.75 2.14\n1977 1.91   NA   NA 2.57 7.30 5.44 3.70 2.09\n1978 1.92   NA   NA 2.52 7.26 5.31 3.66 2.05\n1979 1.94   NA   NA 2.49 7.23 5.17 3.65 2.03\n1980 1.92   NA 1.99 2.43 7.19 5.04 3.59 2.00\n1981 1.91   NA 1.87 2.36 7.13 4.92 3.56 1.95\n1982 1.90 2.48 2.04 2.17 7.05 4.80 3.54 1.93\n1983 1.88 2.50 1.97 2.06 6.95 4.68 3.53 1.89\n1984 1.87 2.52 1.97 2.26 6.84 4.57 3.52 1.87\n1985 1.81 2.43 1.99 2.31 6.70 4.46 3.51 1.84\n1986 1.76 2.46 1.94 2.39 6.55 4.35 3.46 1.84\n1987 1.74 2.38 1.97 2.38 6.36 4.25 3.40 1.82\n1988 1.74 2.49 2.10 2.30 6.17 4.14 3.34 1.83\n1989 1.67 2.36 2.11 2.22 6.00 4.03 3.27 1.79\n1990 1.67 2.41 2.05 1.83 5.83 3.91 3.21 1.78\n1991 1.55 2.32 2.10 1.59 5.66 3.79 3.12 1.73\n1992 1.39 2.48 2.12 1.51 5.49 3.67 3.04 1.70\n1993 1.43 2.24 2.01 1.43 5.32 3.55 2.96 1.62\n1994 1.43 2.17 1.89 1.40 5.14 3.43 2.89 1.56\n1995 1.50 2.03 1.82 1.33 4.95 3.32 2.80 1.51\n1996 1.64 1.95 2.01 1.30 4.77 3.20 2.71 1.50\n1997 1.69 1.86 1.95 1.32 4.59 3.10 2.64 1.48\n1998 1.45 1.76 1.81 1.32 4.42 3.00 2.53 1.45\n1999 1.38 1.67 1.72 1.30 4.25 2.92 2.48 1.45\n2000 1.39 1.64 1.69 1.31 4.12 2.85 2.41 1.47\n2001 1.46 1.57 1.50 1.27 3.91 2.74 2.33 1.44\n2002 1.42 1.49 1.45 1.27 3.71 2.69 2.20 1.43\n2003 1.41 1.51 1.48 1.30 3.50 2.66 2.14 1.45\n2004 1.43 1.52 1.40 1.33 3.34 2.67 2.08 1.47\n2005 1.50 1.48 1.38 1.40 3.24 2.69 2.04 1.48\n2006 1.47 1.52 1.36 1.42 3.21 2.69 2.01 1.51\n2007 1.48 1.44 1.35 1.45 3.18 2.67 2.01 1.53\n2008 1.55 1.48 1.43 1.60 3.06 2.63 2.02 1.59\n2009 1.58 1.47 1.42 1.66 2.95 2.61 1.98 1.59\n2010 1.55 1.44 1.36 1.59 2.85 2.57 1.93 1.58\n2011 1.48 1.35 1.45 1.47 2.81 2.54 1.90 1.54\n2012 1.51 1.39 1.42 1.52 2.78 2.49 1.88 1.54\n2013 1.46 1.30 1.36 1.41 2.74 2.43 1.84 1.51\n2014 1.46 1.31 1.38 1.52 2.69 2.38 1.82 1.54\n2015 1.40 1.32 1.37 1.58 2.64 2.34 1.79 1.54\n2016 1.42 1.37 1.37 1.64 2.59 2.31 1.75 1.56\n2017 1.42 1.32 1.26 1.71 2.58 2.28 1.74 1.55\n2018 1.47 1.32 1.23 1.76 2.55 2.26 1.71 1.54\n2019 1.47 1.33 1.14 1.77 2.50 2.24 1.63 1.52\n2020 1.48 1.36 1.13 1.80 2.47 2.22 1.56 1.50\n2021 1.58 1.39 1.13 1.81 2.43 2.19 1.53 1.53\n2022   NA   NA   NA   NA   NA   NA   NA   NA\n\n\n\n\nLater, we will discuss the meanings of each step and the fact that there are much simpler solutions available.\n\nfertility_df &lt;- readr::read_csv(\"https://stats.oecd.org/sdmx-json/data/DP_LIVE/.FERTILITY.../OECD?contentType=csv&detail=code&separator=comma&csv-lang=en\")\nfertility_df &lt;- fertility_df[, c(\"LOCATION\", \"TIME\", \"Value\")]  \n\nfertility_df &lt;- tidyr::pivot_wider(fertility_df, names_from = \"LOCATION\", \n                   values_from = \"Value\")\n\nfertility_df &lt;- tibble::column_to_rownames(fertility_df, \"TIME\")\nfertility_df &lt;- fertility_df[, - 51] # OECD average\n\nfertility_df &lt;- data.frame(fertility_df)\n\n\n\n\nAt the end of these steps, we have a table, each column of which shows the time series of the fertility rate of a country.\nApply - MARGIN = 1\n\napply(fertility_df, MARGIN = 1, above_replacement_prop)\n\n      1960       1961       1962       1963       1964       1965       1966 \n0.92000000 0.92000000 0.92000000 0.92000000 0.92000000 0.92000000 0.88000000 \n      1967       1968       1969       1970       1971       1972       1973 \n0.86000000 0.86000000 0.82000000 0.80000000 0.78000000 0.68000000 0.66000000 \n      1974       1975       1976       1977       1978       1979       1980 \n0.66000000 0.62745098 0.60784314 0.56862745 0.50980392 0.52941176 0.48076923 \n      1981       1982       1983       1984       1985       1986       1987 \n0.42307692 0.39622642 0.43396226 0.39622642 0.37735849 0.43396226 0.43396226 \n      1988       1989       1990       1991       1992       1993       1994 \n0.47169811 0.37735849 0.35849057 0.32075472 0.30188679 0.28301887 0.28301887 \n      1995       1996       1997       1998       1999       2000       2001 \n0.24528302 0.26415094 0.24528302 0.24528302 0.24528302 0.22641509 0.22641509 \n      2002       2003       2004       2005       2006       2007       2008 \n0.22641509 0.18867925 0.18867925 0.18867925 0.20754717 0.22641509 0.20754717 \n      2009       2010       2011       2012       2013       2014       2015 \n0.20754717 0.18867925 0.15094340 0.18867925 0.16981132 0.16981132 0.16981132 \n      2016       2017       2018       2019       2020       2021       2022 \n0.15094340 0.13207547 0.11320755 0.11320755 0.09433962 0.09433962 0.00000000 \n\n\nIf MARGIN == 1, the apply calculates the proportion of observation above 2.1 in each row. The returned value is a named vector, which means that you can refer to its values by index, or by name.\nApply - MARGIN = 1\n\napply(fertility_df, MARGIN = 1, above_replacement_prop)[1]\n\n1960 \n0.92 \n\napply(fertility_df, MARGIN = 1, above_replacement_prop)[\"2020\"]\n\n      2020 \n0.09433962 \n\n\nApply - MARGIN = 2\n\napply(fertility_df, MARGIN = 2, above_replacement_prop)\n\n       AUS        AUT        BEL        CAN        CZE        DNK        FIN \n0.25806452 0.19354839 0.19354839 0.19354839 0.22580645 0.14285714 0.14516129 \n       FRA        DEU        GRC        HUN        ISL        IRL        ITA \n0.24193548 0.16129032 0.35483871 0.06349206 0.56451613 0.48387097 0.27419355 \n       JPN        KOR        LUX        MEX        NLD        NZL        NOR \n0.12903226 0.37096774 0.14516129 0.90322581 0.20967742 0.43548387 0.23809524 \n       POL        PRT        SVK        ESP        SWE        CHE        TUR \n0.46774194 0.35483871 0.46774194 0.33870968 0.16129032 0.17741935 0.87096774 \n       GBR        USA        BRA        CHL        CHN        EST        IND \n0.20967742 0.22580645 0.69354839 0.64516129 0.50000000 0.17741935 0.96774194 \n       IDN        ISR        RUS        SVN        ZAF        COL        LVA \n1.00000000 1.00000000 0.17741935 0.33870968 1.00000000 0.77419355 0.08064516 \n       LTU        ARG        BGR        HRV        CYP        MLT        ROU \n0.33870968 0.93548387 0.25806452 0.11290323 0.32500000 0.09523810 0.29787234 \n       SAU        PER        CRI         EU \n1.00000000 1.00000000 0.70967742 0.27419355 \n\n\nIf MARGIN == 2, the apply calculates the proportion of observation above 2.1 in each column."
  },
  {
    "objectID": "content/02-content.html",
    "href": "content/02-content.html",
    "title": "Statistics in R",
    "section": "",
    "text": "Statistical analysis serves as an information-compressing mechanism. The primary objectives can be broadly categorized into:\n\nThese tasks aim to forecast future outcomes based on historical data and current conditions.\n\nThese tasks focus on summarizing the main aspects of the data to provide an informative overview.\n\nThis involves identifying patterns, relationships, or anomalies in the data without a prior hypothesis.\n\nThis involves testing predefined hypotheses to confirm or refute them."
  },
  {
    "objectID": "content/02-content.html#objectives-of-statistical-analysis-in-economics",
    "href": "content/02-content.html#objectives-of-statistical-analysis-in-economics",
    "title": "Statistics in R",
    "section": "",
    "text": "Statistical analysis serves as an information-compressing mechanism. The primary objectives can be broadly categorized into:\n\nThese tasks aim to forecast future outcomes based on historical data and current conditions.\n\nThese tasks focus on summarizing the main aspects of the data to provide an informative overview.\n\nThis involves identifying patterns, relationships, or anomalies in the data without a prior hypothesis.\n\nThis involves testing predefined hypotheses to confirm or refute them."
  },
  {
    "objectID": "content/02-content.html#types-of-data",
    "href": "content/02-content.html#types-of-data",
    "title": "Statistics in R",
    "section": "Types of Data",
    "text": "Types of Data\nClassification Based on Structure\nUnstructured Data\nData that does not have a predefined format or organization.\nStructured Data\nData that is organized in a specific manner, often in tabular form.\nTypes of Structured Data\n\nCross-sectional\nTime-series\nLongitudinal\nSpatial\nNetwork\n\nThe type of data dictates the statistical tools and techniques that can be employed for analysis.\n\nAttribute Types\nDifferent types of attributes require different statistical techniques for effective analysis.\n\n\n\n\n\n\n\nAttribute type\n      Description\n      Examples\n      Operations\n    \n\n\nNominal\nNominal values provide only enough information to distinguish one object from another. (=, ‚â†)\nZip codes, employee ID numbers, eye color, gender\nMode, entropy, contingency correlation, chi2 test\n\n\nOrdinal\nThe values of an ordinal attribute provide enough information to order objects. (&lt;, &gt;)\nHardness of minerals, {good, better, best}, grades, street numbers\nQuantiles, rank correlation\n\n\nInterval\nDifferences between values are meaningful, i.e., a unit of measurement exists. (+, -)\nCalendar dates, temperature in Celsius or Fahrenheit\nmean, standard deviation, Pearson's correlation\n\n\nRatio\nFor ratio variables, both differences and ratios are meaningful. (*, /)\nTemperature in Kelvin, monetary quantities, counts, age, mass, length, electrical current\nGeometric mean, harmonic mean, percent variation"
  },
  {
    "objectID": "content/02-content.html#quality-of-data",
    "href": "content/02-content.html#quality-of-data",
    "title": "Statistics in R",
    "section": "Quality of Data",
    "text": "Quality of Data\n\n\n\n\nCommon Issues\n\nMissing data\nOutliers\nDuplication\nInconsistent data\n\n\n\nSome examples\n\n\nLet us examine a sample data from the modeldata R package.\n\n# load the data\ndata(attrition, package = \"modeldata\") \n\n\n\n\n\n\n\n\n\ntibble(attrition)\n\n# A tibble: 1,470 √ó 31\n     Age Attrition BusinessTravel    DailyRate Department       DistanceFromHome\n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;                 &lt;int&gt; &lt;fct&gt;                       &lt;int&gt;\n 1    41 Yes       Travel_Rarely          1102 Sales                           1\n 2    49 No        Travel_Frequently       279 Research_Develo‚Ä¶                8\n 3    37 Yes       Travel_Rarely          1373 Research_Develo‚Ä¶                2\n 4    33 No        Travel_Frequently      1392 Research_Develo‚Ä¶                3\n 5    27 No        Travel_Rarely           591 Research_Develo‚Ä¶                2\n 6    32 No        Travel_Frequently      1005 Research_Develo‚Ä¶                2\n 7    59 No        Travel_Rarely          1324 Research_Develo‚Ä¶                3\n 8    30 No        Travel_Rarely          1358 Research_Develo‚Ä¶               24\n 9    38 No        Travel_Frequently       216 Research_Develo‚Ä¶               23\n10    36 No        Travel_Rarely          1299 Research_Develo‚Ä¶               27\n# ‚Ñπ 1,460 more rows\n# ‚Ñπ 25 more variables: Education &lt;ord&gt;, EducationField &lt;fct&gt;,\n#   EnvironmentSatisfaction &lt;ord&gt;, Gender &lt;fct&gt;, HourlyRate &lt;int&gt;,\n#   JobInvolvement &lt;ord&gt;, JobLevel &lt;int&gt;, JobRole &lt;fct&gt;, JobSatisfaction &lt;ord&gt;,\n#   MaritalStatus &lt;fct&gt;, MonthlyIncome &lt;int&gt;, MonthlyRate &lt;int&gt;,\n#   NumCompaniesWorked &lt;int&gt;, OverTime &lt;fct&gt;, PercentSalaryHike &lt;int&gt;,\n#   PerformanceRating &lt;ord&gt;, RelationshipSatisfaction &lt;ord&gt;, ‚Ä¶\n\n\nAfter loading data that we would like to work with, it is worthwhile to examine and create descriptive statistics. The summary function exists in almost every language, but in R, I suggest using a dedicated package for this purpose: skmir (although there are countless others available).\n\nattrition |&gt; \n  skimr::skim()\n\n\nData summary\n\n\nName\nattrition\n\n\nNumber of rows\n1470\n\n\nNumber of columns\n31\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n15\n\n\nnumeric\n16\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nAttrition\n0\n1\nFALSE\n2\nNo: 1233, Yes: 237\n\n\nBusinessTravel\n0\n1\nFALSE\n3\nTra: 1043, Tra: 277, Non: 150\n\n\nDepartment\n0\n1\nFALSE\n3\nRes: 961, Sal: 446, Hum: 63\n\n\nEducation\n0\n1\nTRUE\n5\nBac: 572, Mas: 398, Col: 282, Bel: 170\n\n\nEducationField\n0\n1\nFALSE\n6\nLif: 606, Med: 464, Mar: 159, Tec: 132\n\n\nEnvironmentSatisfaction\n0\n1\nTRUE\n4\nHig: 453, Ver: 446, Med: 287, Low: 284\n\n\nGender\n0\n1\nFALSE\n2\nMal: 882, Fem: 588\n\n\nJobInvolvement\n0\n1\nTRUE\n4\nHig: 868, Med: 375, Ver: 144, Low: 83\n\n\nJobRole\n0\n1\nFALSE\n9\nSal: 326, Res: 292, Lab: 259, Man: 145\n\n\nJobSatisfaction\n0\n1\nTRUE\n4\nVer: 459, Hig: 442, Low: 289, Med: 280\n\n\nMaritalStatus\n0\n1\nFALSE\n3\nMar: 673, Sin: 470, Div: 327\n\n\nOverTime\n0\n1\nFALSE\n2\nNo: 1054, Yes: 416\n\n\nPerformanceRating\n0\n1\nTRUE\n2\nExc: 1244, Out: 226, Low: 0, Goo: 0\n\n\nRelationshipSatisfaction\n0\n1\nTRUE\n4\nHig: 459, Ver: 432, Med: 303, Low: 276\n\n\nWorkLifeBalance\n0\n1\nTRUE\n4\nBet: 893, Goo: 344, Bes: 153, Bad: 80\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nAge\n0\n1\n36.92\n9.14\n18\n30\n36.0\n43.00\n60\n‚ñÇ‚ñá‚ñá‚ñÉ‚ñÇ\n\n\nDailyRate\n0\n1\n802.49\n403.51\n102\n465\n802.0\n1157.00\n1499\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\nDistanceFromHome\n0\n1\n9.19\n8.11\n1\n2\n7.0\n14.00\n29\n‚ñá‚ñÖ‚ñÇ‚ñÇ‚ñÇ\n\n\nHourlyRate\n0\n1\n65.89\n20.33\n30\n48\n66.0\n83.75\n100\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\nJobLevel\n0\n1\n2.06\n1.11\n1\n1\n2.0\n3.00\n5\n‚ñá‚ñá‚ñÉ‚ñÇ‚ñÅ\n\n\nMonthlyIncome\n0\n1\n6502.93\n4707.96\n1009\n2911\n4919.0\n8379.00\n19999\n‚ñá‚ñÖ‚ñÇ‚ñÅ‚ñÇ\n\n\nMonthlyRate\n0\n1\n14313.10\n7117.79\n2094\n8047\n14235.5\n20461.50\n26999\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\nNumCompaniesWorked\n0\n1\n2.69\n2.50\n0\n1\n2.0\n4.00\n9\n‚ñá‚ñÉ‚ñÇ‚ñÇ‚ñÅ\n\n\nPercentSalaryHike\n0\n1\n15.21\n3.66\n11\n12\n14.0\n18.00\n25\n‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñÅ\n\n\nStockOptionLevel\n0\n1\n0.79\n0.85\n0\n0\n1.0\n1.00\n3\n‚ñá‚ñá‚ñÅ‚ñÇ‚ñÅ\n\n\nTotalWorkingYears\n0\n1\n11.28\n7.78\n0\n6\n10.0\n15.00\n40\n‚ñá‚ñá‚ñÇ‚ñÅ‚ñÅ\n\n\nTrainingTimesLastYear\n0\n1\n2.80\n1.29\n0\n2\n3.0\n3.00\n6\n‚ñÇ‚ñá‚ñá‚ñÇ‚ñÉ\n\n\nYearsAtCompany\n0\n1\n7.01\n6.13\n0\n3\n5.0\n9.00\n40\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nYearsInCurrentRole\n0\n1\n4.23\n3.62\n0\n2\n3.0\n7.00\n18\n‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÅ\n\n\nYearsSinceLastPromotion\n0\n1\n2.19\n3.22\n0\n0\n1.0\n3.00\n15\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nYearsWithCurrManager\n0\n1\n4.12\n3.57\n0\n2\n3.0\n7.00\n17\n‚ñá‚ñÇ‚ñÖ‚ñÅ‚ñÅ"
  },
  {
    "objectID": "content/02-content.html#measures-of-central-tendency",
    "href": "content/02-content.html#measures-of-central-tendency",
    "title": "Statistics in R",
    "section": "Measures of Central Tendency ü¶î",
    "text": "Measures of Central Tendency ü¶î\nArithmetic Mean\nThe arithmetic mean is calculated as:\n\\[\\bar{Y}=\\frac{Y_1+Y_2+\\ldots+Y_N}{N}=\\frac{\\sum_{i=1}^N Y_i}{N}\\]\nGeometric Mean\nThe geometric mean is calculated as:\n\\[\\bar{Y}_g=\\sqrt[N]{\\prod_{i=1}^N Y_i}\\]\nExample\nThe average can also be interpreted as meaning that if all elements were replaced by this value, the sum of the elements would remain the same.\nLet us see the case of indexes:\nThe GDP growth in one year was 5 percent and 5 percent in the following and 10 percent in the 3rd.\nWhat was the annual growth in the 3 years?\n\\[1.05 \\times 1.05 \\times 1.10 = \\sqrt[3]{1.21}=1.0656\\]\nHowever, the arithmetic mean equals 20/3 = 6.66.\nHarmonic Mean\nThe harmonic mean is calculated as:\n\\[\\bar{Y}_h=\\frac{N}{\\sum_{i=1}^N \\frac{1}{Y_i}}\\]\nExample\nConsider two car owners who seek to reduce their costs:\n\nAdam switches from a gas-guzzler of 12 mpg to a slightly less voracious guzzler that runs at 14 mpg.\nThe environmentally virtuous Beth switches from a Bon ss es from 30 mpg car to one that runs at 40 mpg.\n\nSuppose both drivers travel equal distances over a year. Who will save more gas by switching?\nSquared Mean\nThe squared mean is calculated as:\n\\[\\bar{Y_q}=\\sqrt{\\frac{\\sum_{i=1}^N Y_i^2}{N}}\\]"
  },
  {
    "objectID": "content/02-content.html#quantiles",
    "href": "content/02-content.html#quantiles",
    "title": "Statistics in R",
    "section": "Quantiles",
    "text": "Quantiles\nQuantiles are values that divide a dataset into equal groups. The types of quantiles include:\n\n\n\n\n\n\n\n# Groups\n      NAME\n    \n\n\n2\nMedian\n\n\n3\nTertiles\n\n\n4\nQuartiles\n\n\n5\nQuintiles\n\n\n10\nDeciles\n\n\n20\nVentiles\n\n\n100\nPercentiles\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThere will often be occasions when you need to reproduce a complete study or methodology for research or any other purpose. Generally, it‚Äôs not possible, and even the calculation of quantiles prevents precise reproduction, as everyone considers it completely obvious how to calculate the median or a quartile (and the choosen method is never documented). However, depending on the chosen methodology, we will obtain a very minimal but still different result.\n\nx &lt;- c(231, 45342, 2313, 213, 564, 874543, 92713, 3941, 31297, 2654, 4324, 6542) # random numbers\n\n\nquantile(x, probs = c(.25, .5, .75))\n\n     25%      50%      75% \n 1875.75  4132.50 34808.25 \n\n\n\nquantile(x, probs = c(.25, .5, .75), type = 2)\n\n    25%     50%     75% \n 1438.5  4132.5 38319.5"
  },
  {
    "objectID": "content/02-content.html#measures-of-variability",
    "href": "content/02-content.html#measures-of-variability",
    "title": "Statistics in R",
    "section": "Measures of Variability",
    "text": "Measures of Variability\nStandard Deviation\nStandard deviation is the average difference between the observations and the mean??\nIt was already mentioned that the sum of the deviations from the average is zero. If we divide zero by the number of observations it is still zero‚Ä¶\nHere comes the squared mean!\n\\[\\sigma=\\sqrt{\\frac{\\sum_{i=1}^N\\left(Y_i-\\bar{Y}\\right)^2}{N}}\\]\nVariance\nVariance is simply the square of standard deviation.\n\\[\\sigma^2=\\frac{\\sum_{i=1}^N\\left(Y_i-\\bar{Y}\\right)^2}{N}\\]\n\n\n\n\n\n\nNote\n\n\n\nIn many cases, we often use variance instead of standard deviation and not just for fun. If we recall what we learned about the ANOVA, variances are additive, while standard deviations are not."
  },
  {
    "objectID": "content/02-content.html#range",
    "href": "content/02-content.html#range",
    "title": "Statistics in R",
    "section": "Range",
    "text": "Range\nThe spread of your data from the lowest to the highest value.\nInterquartile range (IQR)\n\\[\\text{IQR}=Q_3-Q1\\]\nInterdecile range (IDR)\n\\[\\text{IDR}=D_9-D1\\]\nBoxplot\n\nboxplot(attrition$MonthlyIncome)\n\n\n\n\n\n\nThe boxplot is highly popular for describing a variable, as it displays numerous variables and effectively illustrates the differences based on a categorical feature (but we will discuss this further when we get there)."
  },
  {
    "objectID": "content/03-content.html",
    "href": "content/03-content.html",
    "title": "Dplyr & Tidyr basics",
    "section": "",
    "text": "Tidyverse contains the most important packages that you‚Äôre likely to use in everyday data analyses:\n\n\n\nggplot2, for data visualisation. (Later on, as we direct our attention towards the process of visualization)\ndplyr, for data manipulation. (now)\ntidyr, for tidy data. (today)\nreadr, for data import.\npurrr, for functional programming.\ntibble, for tibbles (modernized data frames).\nstringr, for strings.\nforcats, for factors.\n\n\n\n\n\nTidyverse works similarly to any other package, but when activated, 8 packages are activated at the same time. The following message should appear then on the console:\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.2 ‚îÄ‚îÄ\n‚úî ggplot2 3.4.3     ‚úî purrr   1.0.2\n‚úî tibble  3.2.1     ‚úî dplyr   1.1.2\n‚úî tidyr   1.3.0     ‚úî stringr 1.5.0\n‚úî readr   2.1.4     ‚úî forcats 1.0.0\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "content/03-content.html#the-tidyverse",
    "href": "content/03-content.html#the-tidyverse",
    "title": "Dplyr & Tidyr basics",
    "section": "",
    "text": "Tidyverse contains the most important packages that you‚Äôre likely to use in everyday data analyses:\n\n\n\nggplot2, for data visualisation. (Later on, as we direct our attention towards the process of visualization)\ndplyr, for data manipulation. (now)\ntidyr, for tidy data. (today)\nreadr, for data import.\npurrr, for functional programming.\ntibble, for tibbles (modernized data frames).\nstringr, for strings.\nforcats, for factors.\n\n\n\n\n\nTidyverse works similarly to any other package, but when activated, 8 packages are activated at the same time. The following message should appear then on the console:\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.2 ‚îÄ‚îÄ\n‚úî ggplot2 3.4.3     ‚úî purrr   1.0.2\n‚úî tibble  3.2.1     ‚úî dplyr   1.1.2\n‚úî tidyr   1.3.0     ‚úî stringr 1.5.0\n‚úî readr   2.1.4     ‚úî forcats 1.0.0\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "content/03-content.html#dplyr-functions",
    "href": "content/03-content.html#dplyr-functions",
    "title": "Dplyr & Tidyr basics",
    "section": "Dplyr functions",
    "text": "Dplyr functions\nselect() Extract the specified columns as a table.\n\nselect(.data = fertility_df, LOCATION, TIME, Value)\n\n# A tibble: 3,294 √ó 3\n   LOCATION  TIME Value\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 AUS       1960  3.45\n 2 AUS       1961  3.55\n 3 AUS       1962  3.43\n 4 AUS       1963  3.34\n 5 AUS       1964  3.15\n 6 AUS       1965  2.97\n 7 AUS       1966  2.89\n 8 AUS       1967  2.85\n 9 AUS       1968  2.89\n10 AUS       1969  2.89\n# ‚Ñπ 3,284 more rows\n\n\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, Value) # using col names\n\n# A tibble: 3,294 √ó 3\n   LOCATION  TIME Value\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 AUS       1960  3.45\n 2 AUS       1961  3.55\n 3 AUS       1962  3.43\n 4 AUS       1963  3.34\n 5 AUS       1964  3.15\n 6 AUS       1965  2.97\n 7 AUS       1966  2.89\n 8 AUS       1967  2.85\n 9 AUS       1968  2.89\n10 AUS       1969  2.89\n# ‚Ñπ 3,284 more rows\n\n\nIdentical outcome\n\nfertility_df %&gt;% \n  select(1, 6:7) # by column numbers\n\n# A tibble: 3,294 √ó 3\n   LOCATION  TIME Value\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 AUS       1960  3.45\n 2 AUS       1961  3.55\n 3 AUS       1962  3.43\n 4 AUS       1963  3.34\n 5 AUS       1964  3.15\n 6 AUS       1965  2.97\n 7 AUS       1966  2.89\n 8 AUS       1967  2.85\n 9 AUS       1968  2.89\n10 AUS       1969  2.89\n# ‚Ñπ 3,284 more rows\n\n\n\nfertility_df %&gt;% \n  select(- (2:5), -`Flag Codes`) # drop these\n\n# A tibble: 3,294 √ó 3\n   LOCATION  TIME Value\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 AUS       1960  3.45\n 2 AUS       1961  3.55\n 3 AUS       1962  3.43\n 4 AUS       1963  3.34\n 5 AUS       1964  3.15\n 6 AUS       1965  2.97\n 7 AUS       1966  2.89\n 8 AUS       1967  2.85\n 9 AUS       1968  2.89\n10 AUS       1969  2.89\n# ‚Ñπ 3,284 more rows\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn this case, ‚Äú\" had to be used when referencing the variable namedflag code. The reason for this is that in dplyr expressions, variables containing spaces or special characters always need to be enclosed in \"‚Äù To overcome the difficulties arising from special characters in the headers, the clean_names function in the janitor package provides a helpful solution, allowing all variables of a table to be renamed at once.\n\n\n\nrename() renames columns.\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, Value) %&gt;% \n  rename(geo = LOCATION, fertility = Value)\n\n# A tibble: 3,294 √ó 3\n   geo    TIME fertility\n   &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 AUS    1960      3.45\n 2 AUS    1961      3.55\n 3 AUS    1962      3.43\n 4 AUS    1963      3.34\n 5 AUS    1964      3.15\n 6 AUS    1965      2.97\n 7 AUS    1966      2.89\n 8 AUS    1967      2.85\n 9 AUS    1968      2.89\n10 AUS    1969      2.89\n# ‚Ñπ 3,284 more rows\n\n\n.content-box-green[ HINT: ]\n\n\n\n\n\n\nTip\n\n\n\nYou can do it in 1 single step.\n\nfertility_df %&gt;% \n  select(\n    geo = LOCATION, # select and rename\n    TIME,\n    fertility = Value\n  )\n\n# A tibble: 3,294 √ó 3\n   geo    TIME fertility\n   &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 AUS    1960      3.45\n 2 AUS    1961      3.55\n 3 AUS    1962      3.43\n 4 AUS    1963      3.34\n 5 AUS    1964      3.15\n 6 AUS    1965      2.97\n 7 AUS    1966      2.89\n 8 AUS    1967      2.85\n 9 AUS    1968      2.89\n10 AUS    1969      2.89\n# ‚Ñπ 3,284 more rows\n\n\n\n\nMutate\nmutate() computes new column(s).\ncountrycode::countrycode() Translates from one country naming scheme to another.\n\nlibrary(countrycode)\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, Value) %&gt;% \n  mutate(\n    country_name = countrycode(LOCATION, \"iso3c\", \"country.name\"),\n    continent = countrycode(LOCATION, \"iso3c\", \"continent\")\n  )\n\n# A tibble: 3,294 √ó 5\n   LOCATION  TIME Value country_name continent\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;    \n 1 AUS       1960  3.45 Australia    Oceania  \n 2 AUS       1961  3.55 Australia    Oceania  \n 3 AUS       1962  3.43 Australia    Oceania  \n 4 AUS       1963  3.34 Australia    Oceania  \n 5 AUS       1964  3.15 Australia    Oceania  \n 6 AUS       1965  2.97 Australia    Oceania  \n 7 AUS       1966  2.89 Australia    Oceania  \n 8 AUS       1967  2.85 Australia    Oceania  \n 9 AUS       1968  2.89 Australia    Oceania  \n10 AUS       1969  2.89 Australia    Oceania  \n# ‚Ñπ 3,284 more rows\n\n\n\nlibrary(countrycode)\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, Value) %&gt;% \n  mutate(\n    country_name = countrycode(LOCATION, \"iso3c\", \"country.name\"),\n    continent = countrycode(LOCATION, \"iso3c\", \"continent\"),\n    LOCATION = countrycode(LOCATION, \"iso3c\", \"iso2c\") # modify the original column\n  )\n\n# A tibble: 3,294 √ó 5\n   LOCATION  TIME Value country_name continent\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;    \n 1 AU        1960  3.45 Australia    Oceania  \n 2 AU        1961  3.55 Australia    Oceania  \n 3 AU        1962  3.43 Australia    Oceania  \n 4 AU        1963  3.34 Australia    Oceania  \n 5 AU        1964  3.15 Australia    Oceania  \n 6 AU        1965  2.97 Australia    Oceania  \n 7 AU        1966  2.89 Australia    Oceania  \n 8 AU        1967  2.85 Australia    Oceania  \n 9 AU        1968  2.89 Australia    Oceania  \n10 AU        1969  2.89 Australia    Oceania  \n# ‚Ñπ 3,284 more rows\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you specify a single value instead of a vector, then the values will be repeated by the number of rows.\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, Value) %&gt;% \n  mutate(avg_fertility = mean(Value))\n\n# A tibble: 3,294 √ó 4\n   LOCATION  TIME Value avg_fertility\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n 1 AUS       1960  3.45          2.34\n 2 AUS       1961  3.55          2.34\n 3 AUS       1962  3.43          2.34\n 4 AUS       1963  3.34          2.34\n 5 AUS       1964  3.15          2.34\n 6 AUS       1965  2.97          2.34\n 7 AUS       1966  2.89          2.34\n 8 AUS       1967  2.85          2.34\n 9 AUS       1968  2.89          2.34\n10 AUS       1969  2.89          2.34\n# ‚Ñπ 3,284 more rows\n\n\n\n\nTips for mutate\nifelse\nifelse() returns a vector filled with elements selected from either yes or no depending on the condition.\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, Value) %&gt;% \n  mutate(\n    avg_fertility = mean(Value),\n    above_avg = Value &gt; avg_fertility,\n    high = ifelse(above_avg, \"yes, above the avg\", \"no\")\n  )\n\n# A tibble: 3,294 √ó 6\n   LOCATION  TIME Value avg_fertility above_avg high              \n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt; &lt;lgl&gt;     &lt;chr&gt;             \n 1 AUS       1960  3.45          2.34 TRUE      yes, above the avg\n 2 AUS       1961  3.55          2.34 TRUE      yes, above the avg\n 3 AUS       1962  3.43          2.34 TRUE      yes, above the avg\n 4 AUS       1963  3.34          2.34 TRUE      yes, above the avg\n 5 AUS       1964  3.15          2.34 TRUE      yes, above the avg\n 6 AUS       1965  2.97          2.34 TRUE      yes, above the avg\n 7 AUS       1966  2.89          2.34 TRUE      yes, above the avg\n 8 AUS       1967  2.85          2.34 TRUE      yes, above the avg\n 9 AUS       1968  2.89          2.34 TRUE      yes, above the avg\n10 AUS       1969  2.89          2.34 TRUE      yes, above the avg\n# ‚Ñπ 3,284 more rows\n\n\nifelse() returns a vector filled with elements selected from either yes or no depending on the condition.\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, Value) %&gt;% \n  mutate(\n    avg_fertility = mean(Value),\n    above_avg = Value &gt; avg_fertility,\n    high = ifelse(above_avg, \"yes, above the avg\", \"no\")\n  )\n\n# A tibble: 3,294 √ó 6\n   LOCATION  TIME Value avg_fertility above_avg high              \n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt; &lt;lgl&gt;     &lt;chr&gt;             \n 1 AUS       1960  3.45          2.34 TRUE      yes, above the avg\n 2 AUS       1961  3.55          2.34 TRUE      yes, above the avg\n 3 AUS       1962  3.43          2.34 TRUE      yes, above the avg\n 4 AUS       1963  3.34          2.34 TRUE      yes, above the avg\n 5 AUS       1964  3.15          2.34 TRUE      yes, above the avg\n 6 AUS       1965  2.97          2.34 TRUE      yes, above the avg\n 7 AUS       1966  2.89          2.34 TRUE      yes, above the avg\n 8 AUS       1967  2.85          2.34 TRUE      yes, above the avg\n 9 AUS       1968  2.89          2.34 TRUE      yes, above the avg\n10 AUS       1969  2.89          2.34 TRUE      yes, above the avg\n# ‚Ñπ 3,284 more rows\n\n\nWith a single step:\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, Value) %&gt;% \n  mutate(\n    high = ifelse(Value &gt; mean(Value), \"yes, above the avg\", \"no\")\n  )\n\n# A tibble: 3,294 √ó 4\n   LOCATION  TIME Value high              \n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;             \n 1 AUS       1960  3.45 yes, above the avg\n 2 AUS       1961  3.55 yes, above the avg\n 3 AUS       1962  3.43 yes, above the avg\n 4 AUS       1963  3.34 yes, above the avg\n 5 AUS       1964  3.15 yes, above the avg\n 6 AUS       1965  2.97 yes, above the avg\n 7 AUS       1966  2.89 yes, above the avg\n 8 AUS       1967  2.85 yes, above the avg\n 9 AUS       1968  2.89 yes, above the avg\n10 AUS       1969  2.89 yes, above the avg\n# ‚Ñπ 3,284 more rows\n\n\ncase_when\ncase_when() Vectorise multiple if_else() statements (return the first value associated with TRUE condition)\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, Value) %&gt;% \n  mutate(\n     fertility_level = case_when(\n      Value &gt;= 5 ~ \"very high\",\n      Value &gt;= 3 ~ \"high\", \n      Value &gt;= 2.1 ~ \"above replacement r\",\n      Value &gt;= 1.3 ~ \"below replacemnt r\",\n      TRUE ~ \"very low\"\n    )\n  )\n\n# A tibble: 3,294 √ó 4\n   LOCATION  TIME Value fertility_level    \n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              \n 1 AUS       1960  3.45 high               \n 2 AUS       1961  3.55 high               \n 3 AUS       1962  3.43 high               \n 4 AUS       1963  3.34 high               \n 5 AUS       1964  3.15 high               \n 6 AUS       1965  2.97 above replacement r\n 7 AUS       1966  2.89 above replacement r\n 8 AUS       1967  2.85 above replacement r\n 9 AUS       1968  2.89 above replacement r\n10 AUS       1969  2.89 above replacement r\n# ‚Ñπ 3,284 more rows\n\n\nFrom the latest release:\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, Value) %&gt;% \n  mutate(\n     fertility_level = case_when(\n      Value &gt;= 5 ~ \"very high\",\n      Value &gt;= 3 ~ \"high\", \n      Value &gt;= 2.1 ~ \"above replacement r\",\n      Value &gt;= 1.3 ~ \"below replacemnt r\",\n      .default =  \"very low\"\n    )\n  )\n\n# A tibble: 3,294 √ó 4\n   LOCATION  TIME Value fertility_level    \n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              \n 1 AUS       1960  3.45 high               \n 2 AUS       1961  3.55 high               \n 3 AUS       1962  3.43 high               \n 4 AUS       1963  3.34 high               \n 5 AUS       1964  3.15 high               \n 6 AUS       1965  2.97 above replacement r\n 7 AUS       1966  2.89 above replacement r\n 8 AUS       1967  2.85 above replacement r\n 9 AUS       1968  2.89 above replacement r\n10 AUS       1969  2.89 above replacement r\n# ‚Ñπ 3,284 more rows\n\n\ntransmute() Compute new column(s), drop others. (= select + mutate)\n\nfertility_df %&gt;% \n  transmute(LOCATION, TIME, \n              fertility_level = case_when(\n                Value &gt;= 5 ~ \"very high\",\n                Value &gt;= 3 ~ \"high\", \n                Value &gt;= 2.1 ~ \"above replacement r\",\n                Value &gt;= 1.3 ~ \"below replacemnt r\",\n                TRUE ~ \"very low\" \n              )\n  )\n\nfilter() Extract rows that meet logical criteria.\n\nfertility_df %&gt;% \n  filter(LOCATION != \"EU\" & LOCATION != \"OAVG\")\n\n# A tibble: 3,170 √ó 8\n   LOCATION INDICATOR SUBJECT MEASURE   FREQUENCY  TIME Value `Flag Codes`\n   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;       \n 1 AUS      FERTILITY TOT     CHD_WOMAN A          1960  3.45 NA          \n 2 AUS      FERTILITY TOT     CHD_WOMAN A          1961  3.55 NA          \n 3 AUS      FERTILITY TOT     CHD_WOMAN A          1962  3.43 NA          \n 4 AUS      FERTILITY TOT     CHD_WOMAN A          1963  3.34 NA          \n 5 AUS      FERTILITY TOT     CHD_WOMAN A          1964  3.15 NA          \n 6 AUS      FERTILITY TOT     CHD_WOMAN A          1965  2.97 NA          \n 7 AUS      FERTILITY TOT     CHD_WOMAN A          1966  2.89 NA          \n 8 AUS      FERTILITY TOT     CHD_WOMAN A          1967  2.85 NA          \n 9 AUS      FERTILITY TOT     CHD_WOMAN A          1968  2.89 NA          \n10 AUS      FERTILITY TOT     CHD_WOMAN A          1969  2.89 NA          \n# ‚Ñπ 3,160 more rows\n\n\nslice() Select rows by position\n\nfertility_df %&gt;% \n  slice(2:5)\n\n# A tibble: 4 √ó 8\n  LOCATION INDICATOR SUBJECT MEASURE   FREQUENCY  TIME Value `Flag Codes`\n  &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;       \n1 AUS      FERTILITY TOT     CHD_WOMAN A          1961  3.55 NA          \n2 AUS      FERTILITY TOT     CHD_WOMAN A          1962  3.43 NA          \n3 AUS      FERTILITY TOT     CHD_WOMAN A          1963  3.34 NA          \n4 AUS      FERTILITY TOT     CHD_WOMAN A          1964  3.15 NA          \n\n\nsample_n Select n random rows.\n\nfertility_df %&gt;% \n  sample_n(6)\n\n# A tibble: 6 √ó 8\n  LOCATION INDICATOR SUBJECT MEASURE   FREQUENCY  TIME Value `Flag Codes`\n  &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;       \n1 PRT      FERTILITY TOT     CHD_WOMAN A          1999  1.51 NA          \n2 ROU      FERTILITY TOT     CHD_WOMAN A          2003  1.3  NA          \n3 ESP      FERTILITY TOT     CHD_WOMAN A          2020  1.19 NA          \n4 USA      FERTILITY TOT     CHD_WOMAN A          1977  1.79 NA          \n5 ISR      FERTILITY TOT     CHD_WOMAN A          1988  3.06 NA          \n6 GRC      FERTILITY TOT     CHD_WOMAN A          1971  2.32 NA          \n\n\n\n\n\n\n\n\nTip\n\n\n\nIn order to ensure that R consistently generates the same ‚Äúrandom‚Äù sample, you can utilize the set.seed function. This function has the ability to influence all sources of randomness within your ongoing session. Use it for your research project, if reproducibility is important.\n\n\narrange() Order rows by values of a column or columns (low to high), use with desc() to order from high to low.\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, \n         Value) %&gt;% \n  arrange(Value)\n\n# A tibble: 3,294 √ó 3\n   LOCATION  TIME Value\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 KOR       2021  0.81\n 2 KOR       2020  0.84\n 3 KOR       2019  0.92\n 4 KOR       2018  0.98\n 5 KOR       2017  1.05\n 6 KOR       2005  1.09\n 7 BGR       1997  1.09\n 8 LVA       1998  1.1 \n 9 LVA       1997  1.11\n10 BGR       1998  1.11\n# ‚Ñπ 3,284 more rows\n\n\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, \n         Value) %&gt;% \n  arrange(desc(Value))\n\n# A tibble: 3,294 √ó 3\n   LOCATION  TIME Value\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 SAU       1964  7.67\n 2 SAU       1965  7.66\n 3 SAU       1966  7.66\n 4 SAU       1967  7.66\n 5 SAU       1963  7.65\n 6 SAU       1962  7.64\n 7 SAU       1960  7.63\n 8 SAU       1961  7.63\n 9 SAU       1968  7.63\n10 SAU       1969  7.6 \n# ‚Ñπ 3,284 more rows\n\n\nGrouping\ngroup_by() Create a ‚Äúgrouped‚Äù copy of a table.\nsummarise() Compute table of summaries.\n\nfertility_df %&gt;% \n  group_by(LOCATION)\n\n# A tibble: 3,294 √ó 8\n# Groups:   LOCATION [54]\n   LOCATION INDICATOR SUBJECT MEASURE   FREQUENCY  TIME Value `Flag Codes`\n   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;       \n 1 AUS      FERTILITY TOT     CHD_WOMAN A          1960  3.45 NA          \n 2 AUS      FERTILITY TOT     CHD_WOMAN A          1961  3.55 NA          \n 3 AUS      FERTILITY TOT     CHD_WOMAN A          1962  3.43 NA          \n 4 AUS      FERTILITY TOT     CHD_WOMAN A          1963  3.34 NA          \n 5 AUS      FERTILITY TOT     CHD_WOMAN A          1964  3.15 NA          \n 6 AUS      FERTILITY TOT     CHD_WOMAN A          1965  2.97 NA          \n 7 AUS      FERTILITY TOT     CHD_WOMAN A          1966  2.89 NA          \n 8 AUS      FERTILITY TOT     CHD_WOMAN A          1967  2.85 NA          \n 9 AUS      FERTILITY TOT     CHD_WOMAN A          1968  2.89 NA          \n10 AUS      FERTILITY TOT     CHD_WOMAN A          1969  2.89 NA          \n# ‚Ñπ 3,284 more rows\n\n\n\nfertility_df %&gt;% \n  group_by(LOCATION) %&gt;% \n  summarise(avg_fertility = mean(Value))\n\n# A tibble: 54 √ó 2\n   LOCATION avg_fertility\n   &lt;chr&gt;            &lt;dbl&gt;\n 1 ARG               2.79\n 2 AUS               2.13\n 3 AUT               1.72\n 4 BEL               1.84\n 5 BGR               1.76\n 6 BRA               3.29\n 7 CAN               1.91\n 8 CHE               1.70\n 9 CHL               2.68\n10 CHN               2.91\n# ‚Ñπ 44 more rows\n\n\nExample: number of observations\n\nfertility_df %&gt;% \n  group_by(LOCATION) %&gt;% \n  summarise(n_obs = n())\n\n# A tibble: 54 √ó 2\n   LOCATION n_obs\n   &lt;chr&gt;    &lt;int&gt;\n 1 ARG         62\n 2 AUS         62\n 3 AUT         62\n 4 BEL         62\n 5 BGR         62\n 6 BRA         62\n 7 CAN         62\n 8 CHE         62\n 9 CHL         62\n10 CHN         62\n# ‚Ñπ 44 more rows\n\n\n\n\n\n\n\n\nWarning\n\n\n\nEvery other manipulation will behave differently if the data frame is grouped (We frequently utilise this).\n\n\n\nfertility_df %&gt;% \n  select(LOCATION, TIME, Value) %&gt;% \n  group_by(LOCATION) %&gt;% \n  mutate(avg_fertility = mean(Value))\n\n# A tibble: 3,294 √ó 4\n# Groups:   LOCATION [54]\n   LOCATION  TIME Value avg_fertility\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n 1 AUS       1960  3.45          2.13\n 2 AUS       1961  3.55          2.13\n 3 AUS       1962  3.43          2.13\n 4 AUS       1963  3.34          2.13\n 5 AUS       1964  3.15          2.13\n 6 AUS       1965  2.97          2.13\n 7 AUS       1966  2.89          2.13\n 8 AUS       1967  2.85          2.13\n 9 AUS       1968  2.89          2.13\n10 AUS       1969  2.89          2.13\n# ‚Ñπ 3,284 more rows\n\n\nExercise\nIn which decade did the average fertility rate of European countries decrease the most? (in absolute magnitude)\n\n\n\n\n\n\nTip\n\n\n\nUse the first() and the last() function to calculate difference in a period.\n\n\n‚Ä¶if, ‚Ä¶at, ‚Ä¶all (deprecated)\nSome of the previously listed functions have extended versions.\nSelect all columns where the condition is TRUE\n\nfertility_df %&gt;% \n  select_if(is.numeric)\n\n# A tibble: 3,294 √ó 2\n    TIME Value\n   &lt;dbl&gt; &lt;dbl&gt;\n 1  1960  3.45\n 2  1961  3.55\n 3  1962  3.43\n 4  1963  3.34\n 5  1964  3.15\n 6  1965  2.97\n 7  1966  2.89\n 8  1967  2.85\n 9  1968  2.89\n10  1969  2.89\n# ‚Ñπ 3,284 more rows\n\n\nModify all columns where the condition is true\n\n\n\n\n\n\nImportant\n\n\n\nYou will utilize lambda-style functions in this context. This implies that after the tilde (~), you may reference the current element using the dot operator (.).\n\n\n\nfertility_df %&gt;% \n  mutate_if(is.numeric, ~ . * 10)\n\n# A tibble: 3,294 √ó 8\n   LOCATION INDICATOR SUBJECT MEASURE   FREQUENCY  TIME Value `Flag Codes`\n   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;       \n 1 AUS      FERTILITY TOT     CHD_WOMAN A         19600  34.5 NA          \n 2 AUS      FERTILITY TOT     CHD_WOMAN A         19610  35.5 NA          \n 3 AUS      FERTILITY TOT     CHD_WOMAN A         19620  34.3 NA          \n 4 AUS      FERTILITY TOT     CHD_WOMAN A         19630  33.4 NA          \n 5 AUS      FERTILITY TOT     CHD_WOMAN A         19640  31.5 NA          \n 6 AUS      FERTILITY TOT     CHD_WOMAN A         19650  29.7 NA          \n 7 AUS      FERTILITY TOT     CHD_WOMAN A         19660  28.9 NA          \n 8 AUS      FERTILITY TOT     CHD_WOMAN A         19670  28.5 NA          \n 9 AUS      FERTILITY TOT     CHD_WOMAN A         19680  28.9 NA          \n10 AUS      FERTILITY TOT     CHD_WOMAN A         19690  28.9 NA          \n# ‚Ñπ 3,284 more rows\n\n\nAcross/where\nThe concepts of ‚Äúacross‚Äù and ‚Äúwhere‚Äù represent relatively new functions that offer an alternative to the previously used if/at/all notations. These functions provide us with a more efficient and reliable approach, eliminating the need for dependence on the aforementioned notations.\n\nfertility_df |&gt; \n  select(LOCATION, where(is.numeric))\n\n# A tibble: 3,294 √ó 3\n   LOCATION  TIME Value\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 AUS       1960  3.45\n 2 AUS       1961  3.55\n 3 AUS       1962  3.43\n 4 AUS       1963  3.34\n 5 AUS       1964  3.15\n 6 AUS       1965  2.97\n 7 AUS       1966  2.89\n 8 AUS       1967  2.85\n 9 AUS       1968  2.89\n10 AUS       1969  2.89\n# ‚Ñπ 3,284 more rows\n\n\n\nfertility_df |&gt; \n  select(LOCATION, where(is.numeric)) |&gt; \n  mutate(\n    across(is.numeric, mean)\n  )\n\n# A tibble: 3,294 √ó 3\n   LOCATION  TIME Value\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 AUS      1991.  2.34\n 2 AUS      1991.  2.34\n 3 AUS      1991.  2.34\n 4 AUS      1991.  2.34\n 5 AUS      1991.  2.34\n 6 AUS      1991.  2.34\n 7 AUS      1991.  2.34\n 8 AUS      1991.  2.34\n 9 AUS      1991.  2.34\n10 AUS      1991.  2.34\n# ‚Ñπ 3,284 more rows\n\n\n\nfertility_df |&gt; \n  select(LOCATION, where(is.numeric)) |&gt; \n  mutate(\n    across(is.numeric, list(avg = mean), .names = \"{.col}_{.fn}\")\n  )\n\n# A tibble: 3,294 √ó 5\n   LOCATION  TIME Value TIME_avg Value_avg\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 AUS       1960  3.45    1991.      2.34\n 2 AUS       1961  3.55    1991.      2.34\n 3 AUS       1962  3.43    1991.      2.34\n 4 AUS       1963  3.34    1991.      2.34\n 5 AUS       1964  3.15    1991.      2.34\n 6 AUS       1965  2.97    1991.      2.34\n 7 AUS       1966  2.89    1991.      2.34\n 8 AUS       1967  2.85    1991.      2.34\n 9 AUS       1968  2.89    1991.      2.34\n10 AUS       1969  2.89    1991.      2.34\n# ‚Ñπ 3,284 more rows"
  },
  {
    "objectID": "content/03-content.html#tidyr-functions",
    "href": "content/03-content.html#tidyr-functions",
    "title": "Dplyr & Tidyr basics",
    "section": "Tidyr functions",
    "text": "Tidyr functions\nThe goal of tidyr is to help you create tidy data. Tidy data is data where:\n\nEvery column is variable.\nEvery row is an observation.\nEvery cell is a single value.\n\n\n\n\n\n\n\nCHEATSHEETS\n\n\n\n\n\n\n\nWide/long\nDatasets are in long on wide format (but never in the currently desired one) üòÜ\nThe following example is the one we saw last week to present the apply functions. In excel you probably operated with tables in wide format, but in R we always prefer the long format.\n\nfertility_wide &lt;- fertility_df %&gt;% \n  select(LOCATION, TIME, Value) %&gt;% \n  pivot_wider(names_from = LOCATION, values_from = Value)\n\nfertility_wide\n\n# A tibble: 63 √ó 55\n    TIME   AUS   AUT   BEL   CAN   CZE   DNK   FIN   FRA   DEU   GRC   HUN   ISL\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  1960  3.45  2.69  2.54  3.9   2.11  2.54  2.71  2.74  2.37  2.23  2.02  4.26\n 2  1961  3.55  2.78  2.63  3.84  2.13  2.55  2.65  2.82  2.44  2.13  1.94  3.88\n 3  1962  3.43  2.8   2.59  3.76  2.14  2.54  2.66  2.8   2.44  2.16  1.79  3.98\n 4  1963  3.34  2.82  2.68  3.67  2.33  2.64  2.66  2.9   2.51  2.14  1.82  3.98\n 5  1964  3.15  2.79  2.71  3.5   2.36  2.6   2.58  2.91  2.53  2.24  1.8   3.86\n 6  1965  2.97  2.7   2.61  3.15  2.18  2.61  2.46  2.85  2.5   2.25  1.81  3.71\n 7  1966  2.89  2.66  2.52  2.81  2.01  2.62  2.4   2.8   2.51  2.32  1.88  3.58\n 8  1967  2.85  2.62  2.41  2.6   1.9   2.35  2.32  2.67  2.45  2.45  2.01  3.28\n 9  1968  2.89  2.58  2.31  2.45  1.83  2.12  2.15  2.59  2.36  2.42  2.06  3.07\n10  1969  2.89  2.49  2.27  2.4   1.86  2     1.94  2.53  2.21  2.36  2.04  2.99\n# ‚Ñπ 53 more rows\n# ‚Ñπ 42 more variables: IRL &lt;dbl&gt;, ITA &lt;dbl&gt;, JPN &lt;dbl&gt;, KOR &lt;dbl&gt;, LUX &lt;dbl&gt;,\n#   MEX &lt;dbl&gt;, NLD &lt;dbl&gt;, NZL &lt;dbl&gt;, NOR &lt;dbl&gt;, POL &lt;dbl&gt;, PRT &lt;dbl&gt;,\n#   SVK &lt;dbl&gt;, ESP &lt;dbl&gt;, SWE &lt;dbl&gt;, CHE &lt;dbl&gt;, TUR &lt;dbl&gt;, GBR &lt;dbl&gt;,\n#   USA &lt;dbl&gt;, BRA &lt;dbl&gt;, CHL &lt;dbl&gt;, CHN &lt;dbl&gt;, EST &lt;dbl&gt;, IND &lt;dbl&gt;,\n#   IDN &lt;dbl&gt;, ISR &lt;dbl&gt;, RUS &lt;dbl&gt;, SVN &lt;dbl&gt;, ZAF &lt;dbl&gt;, COL &lt;dbl&gt;,\n#   LVA &lt;dbl&gt;, LTU &lt;dbl&gt;, ARG &lt;dbl&gt;, BGR &lt;dbl&gt;, HRV &lt;dbl&gt;, CYP &lt;dbl&gt;, ‚Ä¶\n\n\nYes, pivot_longer is to convert back the table into long format.\n\nfertility_wide %&gt;% \n  pivot_longer(\n    cols = - 1, # all except the 1st\n    # 2:last_col()\n    # AUS, AUT, ... also work\n    names_to = \"LOCATION\",\n    values_to = \"fertility\"\n  )\n\n# A tibble: 3,402 √ó 3\n    TIME LOCATION fertility\n   &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1  1960 AUS           3.45\n 2  1960 AUT           2.69\n 3  1960 BEL           2.54\n 4  1960 CAN           3.9 \n 5  1960 CZE           2.11\n 6  1960 DNK           2.54\n 7  1960 FIN           2.71\n 8  1960 FRA           2.74\n 9  1960 DEU           2.37\n10  1960 GRC           2.23\n# ‚Ñπ 3,392 more rows"
  },
  {
    "objectID": "content/03-content.html#mutating-joins",
    "href": "content/03-content.html#mutating-joins",
    "title": "Dplyr & Tidyr basics",
    "section": "Mutating joins",
    "text": "Mutating joins\nJoins are fundamental operations that merge rows from two or more tables based on the common columns (keys). These operations play a crucial role in data manipulation and analysis, especially when dealing with multiple data sources (which is almost always the case).\nThe difference among the functions is how they handle values from the shared columns that appear in only one of the two tables.\n\n# Create two example data frames\ndf1 &lt;- data.frame(ID = c(1, 2, 3), Name = c(\"Alice\", \"Bob\", \"Charlie\"))\ndf2 &lt;- data.frame(ID = c(2, 3, 4), Score = c(90, 85, 88))\n\n\ninner_join(df1, df2, by = \"ID\")\n\n  ID    Name Score\n1  2     Bob    90\n2  3 Charlie    85\n\nleft_join(df1, df2, by = \"ID\")\n\n  ID    Name Score\n1  1   Alice    NA\n2  2     Bob    90\n3  3 Charlie    85\n\nright_join(df1, df2, by = \"ID\")\n\n  ID    Name Score\n1  2     Bob    90\n2  3 Charlie    85\n3  4    &lt;NA&gt;    88\n\nfull_join(df1, df2, by = \"ID\")\n\n  ID    Name Score\n1  1   Alice    NA\n2  2     Bob    90\n3  3 Charlie    85\n4  4    &lt;NA&gt;    88\n\nsemi_join(df1, df2, by = \"ID\")\n\n  ID    Name\n1  2     Bob\n2  3 Charlie\n\nanti_join(df1, df2, by = \"ID\")\n\n  ID  Name\n1  1 Alice"
  },
  {
    "objectID": "content/04-content.html",
    "href": "content/04-content.html",
    "title": "A whole game",
    "section": "",
    "text": "You can access an Excel spreadsheet on infant mortality downloaded from Eurostat. We discovered last week that utilizing the Eurostat data is significantly facilitated by the eurostat package; however, for the purpose of practical demonstration, we now need to showcase the multitude of cleaning procedures required to establish links between series originating from diverse sources.\n\nLets download the infant mortality rates from Eurostat (in Excel format) and link it with the fertility_df\n\ninfant_mortality_df &lt;- readxl::read_excel(\"../data/demo_minfind.xls\")\n\nThe issue here is that there are multiple tables on the first sheet (not a rare thing).\n\ninfant_mortality_df\n\n# A tibble: 339 √ó 62\n   Infant mortality rate‚Ä¶¬π ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9  ...10\n   &lt;chr&gt;                   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 &lt;NA&gt;                    &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 2 Last update             4474‚Ä¶ &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 3 Extracted on            4482‚Ä¶ &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 4 Source of data          Euro‚Ä¶ &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 5 &lt;NA&gt;                    &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 6 INDIC_DE                Earl‚Ä¶ &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 7 UNIT                    Rate  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 8 &lt;NA&gt;                    &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 9 GEO/TIME                1960  1961  1962  1963  1964  1965  1966  1967  1968 \n10 European Union - 27 co‚Ä¶ :     :     :     :     :     :     :     :     :    \n# ‚Ñπ 329 more rows\n# ‚Ñπ abbreviated name: ¬π‚Äã`Infant mortality rates [demo_minfind]`\n# ‚Ñπ 52 more variables: ...11 &lt;chr&gt;, ...12 &lt;chr&gt;, ...13 &lt;chr&gt;, ...14 &lt;chr&gt;,\n#   ...15 &lt;chr&gt;, ...16 &lt;chr&gt;, ...17 &lt;chr&gt;, ...18 &lt;chr&gt;, ...19 &lt;chr&gt;,\n#   ...20 &lt;chr&gt;, ...21 &lt;chr&gt;, ...22 &lt;chr&gt;, ...23 &lt;chr&gt;, ...24 &lt;chr&gt;,\n#   ...25 &lt;chr&gt;, ...26 &lt;chr&gt;, ...27 &lt;chr&gt;, ...28 &lt;chr&gt;, ...29 &lt;chr&gt;,\n#   ...30 &lt;chr&gt;, ...31 &lt;chr&gt;, ...32 &lt;chr&gt;, ...33 &lt;chr&gt;, ...34 &lt;chr&gt;, ‚Ä¶\n\n\nAfter exploring the data, you may realize that the name of the data is always in the second column. Our table starts after where find the ‚ÄúInfant mortality rate‚Äù.\n\npull(infant_mortality_df, 2) # 2nd column as vector\n\n  [1] NA                              \"44742.521828703699\"           \n  [3] \"44822.86385927083\"             \"Eurostat\"                     \n  [5] NA                              \"Early neonatal mortality rate\"\n  [7] \"Rate\"                          NA                             \n  [9] \"1960\"                          \":\"                            \n [11] \":\"                             \":\"                            \n [13] \":\"                             \":\"                            \n [15] \"17.100000000000001\"            \"10.9\"                         \n [17] \"10.699999999999999\"            \"13.9\"                         \n [19] \"20.800000000000001\"            \"19.699999999999999\"           \n [21] \"9.5999999999999996\"            \"16.100000000000001\"           \n [23] \"12.300000000000001\"            \"15.9\"                         \n [25] \":\"                             \"14.6\"                         \n [27] \"21\"                            \"17.800000000000001\"           \n [29] \":\"                             \":\"                            \n [31] \"7.2999999999999998\"            \"16.300000000000001\"           \n [33] \"22.100000000000001\"            \":\"                            \n [35] \"11.9\"                          \"20.199999999999999\"           \n [37] \":\"                             \"15\"                           \n [39] \":\"                             \"15.6\"                         \n [41] \"10.300000000000001\"            \"12.6\"                         \n [43] \"11.800000000000001\"            \":\"                            \n [45] \":\"                             \":\"                            \n [47] \":\"                             \"7.0999999999999996\"           \n [49] \":\"                             \"9.9000000000000004\"           \n [51] \"14.4\"                          \"13.699999999999999\"           \n [53] \":\"                             \"18.600000000000001\"           \n [55] \":\"                             \":\"                            \n [57] \":\"                             \":\"                            \n [59] \":\"                             \"17.300000000000001\"           \n [61] \":\"                             \":\"                            \n [63] \":\"                             \":\"                            \n [65] \":\"                             \":\"                            \n [67] \":\"                             \":\"                            \n [69] NA                              NA                             \n [71] \"not available\"                 NA                             \n [73] \"Infant mortality rate\"         \"Rate\"                         \n [75] NA                              \"1960\"                         \n [77] \":\"                             \":\"                            \n [79] \":\"                             \":\"                            \n [81] \":\"                             \"31.399999999999999\"           \n [83] \"45.100000000000001\"            \"20\"                           \n [85] \"21.5\"                          \"33.799999999999997\"           \n [87] \"35\"                            \"31.100000000000001\"           \n [89] \"29.300000000000001\"            \"40.100000000000001\"           \n [91] \"35.399999999999999\"            \":\"                            \n [93] \"27.699999999999999\"            \"70.400000000000006\"           \n [95] \"43.899999999999999\"            \":\"                            \n [97] \"27\"                            \"38\"                           \n [99] \"31.5\"                          \"47.600000000000001\"           \n[101] \"38.299999999999997\"            \"16.5\"                         \n[103] \"37.5\"                          \"56.100000000000001\"           \n[105] \"77.5\"                          \"75.700000000000003\"           \n[107] \"35.100000000000001\"            \"28.600000000000001\"           \n[109] \"21\"                            \"16.600000000000001\"           \n[111] \":\"                             \":\"                            \n[113] \":\"                             \"18.899999999999999\"           \n[115] \"13\"                            \"21.100000000000001\"           \n[117] \"16\"                            \"21.100000000000001\"           \n[119] \"22.5\"                          \":\"                            \n[121] \"114.59999999999999\"            \"83\"                           \n[123] \":\"                             \":\"                            \n[125] \":\"                             \":\"                            \n[127] \"107\"                           \"132.5\"                        \n[129] \":\"                             \":\"                            \n[131] \":\"                             \":\"                            \n[133] \":\"                             \":\"                            \n[135] \":\"                             NA                             \n[137] NA                              \"not available\"                \n[139] NA                              \"Late foetal mortality rate\"   \n[141] \"Rate\"                          NA                             \n[143] \"1960\"                          \":\"                            \n[145] \":\"                             \":\"                            \n[147] \":\"                             \":\"                            \n[149] \"15.300000000000001\"            \"12.199999999999999\"           \n[151] \"9.8000000000000007\"            \"12.4\"                         \n[153] \"15.300000000000001\"            \"15.5\"                         \n[155] \"13\"                            \"21.899999999999999\"           \n[157] \"14.300000000000001\"            \"27.300000000000001\"           \n[159] \":\"                             \"17\"                           \n[161] \"12.4\"                          \"24.5\"                         \n[163] \":\"                             \"10.699999999999999\"           \n[165] \"8.9000000000000004\"            \"16.100000000000001\"           \n[167] \"13.199999999999999\"            \":\"                            \n[169] \"14.9\"                          \"15\"                           \n[171] \"12.5\"                          \"26.5\"                         \n[173] \"15.9\"                          \"14.199999999999999\"           \n[175] \"10.9\"                          \"15.1\"                         \n[177] \"13.699999999999999\"            \":\"                            \n[179] \":\"                             \":\"                            \n[181] \"12.4\"                          \"12.699999999999999\"           \n[183] \"10.4\"                          \"13.9\"                         \n[185] \"11.4\"                          \"20.100000000000001\"           \n[187] \":\"                             \"9.8000000000000007\"           \n[189] \":\"                             \":\"                            \n[191] \":\"                             \":\"                            \n[193] \":\"                             \"8.6999999999999993\"           \n[195] \":\"                             \":\"                            \n[197] \":\"                             \":\"                            \n[199] \":\"                             \":\"                            \n[201] \":\"                             \":\"                            \n[203] NA                              NA                             \n[205] \"not available\"                 NA                             \n[207] \"Neonatal mortality rate\"       \"Rate\"                         \n[209] NA                              \"1960\"                         \n[211] \":\"                             \":\"                            \n[213] \":\"                             \":\"                            \n[215] \":\"                             \"20.5\"                         \n[217] \"19.399999999999999\"            \"13.1\"                         \n[219] \"16.100000000000001\"            \"23.899999999999999\"           \n[221] \"23.199999999999999\"            \":\"                            \n[223] \"20.399999999999999\"            \"19.5\"                         \n[225] \"20.199999999999999\"            \":\"                            \n[227] \"17.699999999999999\"            \"35.100000000000001\"           \n[229] \"23.899999999999999\"            \":\"                            \n[231] \"10.9\"                          \"13.4\"                         \n[233] \"19.100000000000001\"            \"27\"                           \n[235] \":\"                             \"13.5\"                         \n[237] \"24.600000000000001\"            \":\"                            \n[239] \"27.899999999999999\"            \":\"                            \n[241] \"20.399999999999999\"            \"14.1\"                         \n[243] \"14.4\"                          \"13.4\"                         \n[245] \":\"                             \":\"                            \n[247] \":\"                             \":\"                            \n[249] \"9.1999999999999993\"            \":\"                            \n[251] \"11.699999999999999\"            \"16.100000000000001\"           \n[253] \"16\"                            \":\"                            \n[255] \"41.399999999999999\"            \":\"                            \n[257] \":\"                             \":\"                            \n[259] \":\"                             \":\"                            \n[261] \"32.799999999999997\"            \":\"                            \n[263] \":\"                             \":\"                            \n[265] \":\"                             \":\"                            \n[267] \":\"                             \":\"                            \n[269] \":\"                             NA                             \n[271] NA                              \"not available\"                \n[273] NA                              \"Perinatal mortality rate\"     \n[275] \"Rate\"                          NA                             \n[277] \"1960\"                          \":\"                            \n[279] \":\"                             \":\"                            \n[281] \":\"                             \":\"                            \n[283] \"32.100000000000001\"            \"23\"                           \n[285] \"20.5\"                          \"26.199999999999999\"           \n[287] \"35.799999999999997\"            \"34.899999999999999\"           \n[289] \"22.5\"                          \"37.700000000000003\"           \n[291] \"26.399999999999999\"            \"42.799999999999997\"           \n[293] \":\"                             \"31.399999999999999\"           \n[295] \"33.100000000000001\"            \"41.899999999999999\"           \n[297] \":\"                             \":\"                            \n[299] \"16.100000000000001\"            \"32.200000000000003\"           \n[301] \"35\"                            \":\"                            \n[303] \"26.600000000000001\"            \"34.899999999999999\"           \n[305] \":\"                             \"41.100000000000001\"           \n[307] \":\"                             \"29.600000000000001\"           \n[309] \"21\"                            \"27.5\"                         \n[311] \"25.399999999999999\"            \":\"                            \n[313] \":\"                             \":\"                            \n[315] \":\"                             \"19.699999999999999\"           \n[317] \":\"                             \"23.699999999999999\"           \n[319] \"25.600000000000001\"            \"33.5\"                         \n[321] \":\"                             \"28.199999999999999\"           \n[323] \":\"                             \":\"                            \n[325] \":\"                             \":\"                            \n[327] \":\"                             \"25.800000000000001\"           \n[329] \":\"                             \":\"                            \n[331] \":\"                             \":\"                            \n[333] \":\"                             \":\"                            \n[335] \":\"                             \":\"                            \n[337] NA                              NA                             \n[339] \"not available\"                \n\n\nAfter exploring the data, you may realise that the name of the data is always in the second column. Our data starts where find we the ‚ÄúInfant mortality rate‚Äù. The first row of the table where we find ‚ÄúGEO/TIME‚Äù in the first column\n\ndata_start_index &lt;- pull(infant_mortality_df, 2) %&gt;%\n  purrr::detect_index(~ . == \"Infant mortality rate\" & !is.na(.))\n\ndata_start_index\n\n[1] 73\n\n\n\nstart_index &lt;- pull(infant_mortality_df, 1) %&gt;% \n  {. == \"GEO/TIME\" & !is.na(.)} %&gt;% \n  which() %&gt;% \n  detect(~ . &gt; data_start_index)\n\nAnd ends at the next empty cell (not ‚Äú:‚Äù)\n\nend_index &lt;- pull(infant_mortality_df, 2) %&gt;% \n  is.na() %&gt;%\n  which() %&gt;% \n  detect(~ . &gt; start_index)\n\n\ninfant_mortality_df %&gt;% \n  slice(start_index:end_index)\n\n# A tibble: 61 √ó 62\n   Infant mortality rate‚Ä¶¬π ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9  ...10\n   &lt;chr&gt;                   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 GEO/TIME                1960  1961  1962  1963  1964  1965  1966  1967  1968 \n 2 European Union - 27 co‚Ä¶ :     38.2‚Ä¶ 36.3‚Ä¶ 34.2‚Ä¶ 31.8‚Ä¶ 30    29.1‚Ä¶ 28.6‚Ä¶ 29.1‚Ä¶\n 3 European Union - 28 co‚Ä¶ :     36.2‚Ä¶ 34.6‚Ä¶ 32.7‚Ä¶ 30.5  28.6‚Ä¶ 28    27.5  27.8‚Ä¶\n 4 European Union - 27 co‚Ä¶ :     36    34.3‚Ä¶ 32.5  30.3‚Ä¶ 28.5  27.8‚Ä¶ 27.3‚Ä¶ 27.8‚Ä¶\n 5 Euro area - 19 countri‚Ä¶ :     34.7‚Ä¶ 33.2‚Ä¶ 31.8‚Ä¶ 29.6‚Ä¶ 28.3‚Ä¶ 27.6‚Ä¶ 26.1‚Ä¶ 25.6‚Ä¶\n 6 Euro area - 18 countri‚Ä¶ :     34.7‚Ä¶ 33.2‚Ä¶ 31.8‚Ä¶ 29.6‚Ä¶ 28.3‚Ä¶ 27.6‚Ä¶ 26.1‚Ä¶ 25.6‚Ä¶\n 7 Belgium                 31.3‚Ä¶ 28.1‚Ä¶ 27.5  27.1‚Ä¶ 25.3‚Ä¶ 23.6‚Ä¶ 24.6‚Ä¶ 22.8‚Ä¶ 21.6‚Ä¶\n 8 Bulgaria                45.1‚Ä¶ 37.7‚Ä¶ 37.2‚Ä¶ 35.7‚Ä¶ 32.8‚Ä¶ 30.8‚Ä¶ 32.2‚Ä¶ 33.1‚Ä¶ 28.3‚Ä¶\n 9 Czechia                 20    19.3‚Ä¶ 21.1‚Ä¶ 19.6‚Ä¶ 19.1‚Ä¶ 23.6‚Ä¶ 21.8‚Ä¶ 21.5  21.6‚Ä¶\n10 Denmark                 21.5  21.8‚Ä¶ 20.1‚Ä¶ 19.1‚Ä¶ 18.6‚Ä¶ 18.6‚Ä¶ 16.8‚Ä¶ 15.8‚Ä¶ 16.3‚Ä¶\n# ‚Ñπ 51 more rows\n# ‚Ñπ abbreviated name: ¬π‚Äã`Infant mortality rates [demo_minfind]`\n# ‚Ñπ 52 more variables: ...11 &lt;chr&gt;, ...12 &lt;chr&gt;, ...13 &lt;chr&gt;, ...14 &lt;chr&gt;,\n#   ...15 &lt;chr&gt;, ...16 &lt;chr&gt;, ...17 &lt;chr&gt;, ...18 &lt;chr&gt;, ...19 &lt;chr&gt;,\n#   ...20 &lt;chr&gt;, ...21 &lt;chr&gt;, ...22 &lt;chr&gt;, ...23 &lt;chr&gt;, ...24 &lt;chr&gt;,\n#   ...25 &lt;chr&gt;, ...26 &lt;chr&gt;, ...27 &lt;chr&gt;, ...28 &lt;chr&gt;, ...29 &lt;chr&gt;,\n#   ...30 &lt;chr&gt;, ...31 &lt;chr&gt;, ...32 &lt;chr&gt;, ...33 &lt;chr&gt;, ...34 &lt;chr&gt;, ‚Ä¶\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf the colnames are not tidy or they are threated as observation, then use the janitor package.\n\n\n\ninfant_mortality_df %&gt;% \n  slice(start_index:end_index) %&gt;% \n  janitor::row_to_names(1)\n\n# A tibble: 60 √ó 62\n   `GEO/TIME`     `1960` `1961` `1962` `1963` `1964` `1965` `1966` `1967` `1968`\n   &lt;chr&gt;          &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; \n 1 European Unio‚Ä¶ :      38.20‚Ä¶ 36.39‚Ä¶ 34.29‚Ä¶ 31.89‚Ä¶ 30     29.19‚Ä¶ 28.69‚Ä¶ 29.19‚Ä¶\n 2 European Unio‚Ä¶ :      36.20‚Ä¶ 34.60‚Ä¶ 32.70‚Ä¶ 30.5   28.69‚Ä¶ 28     27.5   27.89‚Ä¶\n 3 European Unio‚Ä¶ :      36     34.39‚Ä¶ 32.5   30.30‚Ä¶ 28.5   27.80‚Ä¶ 27.39‚Ä¶ 27.80‚Ä¶\n 4 Euro area - 1‚Ä¶ :      34.79‚Ä¶ 33.29‚Ä¶ 31.80‚Ä¶ 29.60‚Ä¶ 28.30‚Ä¶ 27.60‚Ä¶ 26.19‚Ä¶ 25.69‚Ä¶\n 5 Euro area - 1‚Ä¶ :      34.79‚Ä¶ 33.20‚Ä¶ 31.80‚Ä¶ 29.60‚Ä¶ 28.30‚Ä¶ 27.60‚Ä¶ 26.19‚Ä¶ 25.69‚Ä¶\n 6 Belgium        31.39‚Ä¶ 28.10‚Ä¶ 27.5   27.19‚Ä¶ 25.30‚Ä¶ 23.69‚Ä¶ 24.69‚Ä¶ 22.89‚Ä¶ 21.69‚Ä¶\n 7 Bulgaria       45.10‚Ä¶ 37.79‚Ä¶ 37.29‚Ä¶ 35.70‚Ä¶ 32.89‚Ä¶ 30.80‚Ä¶ 32.20‚Ä¶ 33.10‚Ä¶ 28.30‚Ä¶\n 8 Czechia        20     19.30‚Ä¶ 21.10‚Ä¶ 19.69‚Ä¶ 19.10‚Ä¶ 23.69‚Ä¶ 21.89‚Ä¶ 21.5   21.60‚Ä¶\n 9 Denmark        21.5   21.80‚Ä¶ 20.10‚Ä¶ 19.10‚Ä¶ 18.69‚Ä¶ 18.69‚Ä¶ 16.89‚Ä¶ 15.80‚Ä¶ 16.39‚Ä¶\n10 Germany (unti‚Ä¶ 33.79‚Ä¶ 31.69‚Ä¶ 29.30‚Ä¶ 27     25.30‚Ä¶ 23.89‚Ä¶ 23.60‚Ä¶ 22.89‚Ä¶ 22.80‚Ä¶\n# ‚Ñπ 50 more rows\n# ‚Ñπ 52 more variables: `1969` &lt;chr&gt;, `1970` &lt;chr&gt;, `1971` &lt;chr&gt;, `1972` &lt;chr&gt;,\n#   `1973` &lt;chr&gt;, `1974` &lt;chr&gt;, `1975` &lt;chr&gt;, `1976` &lt;chr&gt;, `1977` &lt;chr&gt;,\n#   `1978` &lt;chr&gt;, `1979` &lt;chr&gt;, `1980` &lt;chr&gt;, `1981` &lt;chr&gt;, `1982` &lt;chr&gt;,\n#   `1983` &lt;chr&gt;, `1984` &lt;chr&gt;, `1985` &lt;chr&gt;, `1986` &lt;chr&gt;, `1987` &lt;chr&gt;,\n#   `1988` &lt;chr&gt;, `1989` &lt;chr&gt;, `1990` &lt;chr&gt;, `1991` &lt;chr&gt;, `1992` &lt;chr&gt;,\n#   `1993` &lt;chr&gt;, `1994` &lt;chr&gt;, `1995` &lt;chr&gt;, `1996` &lt;chr&gt;, `1997` &lt;chr&gt;, ‚Ä¶\n\n\nNext issue: All columns are in character format\n\ninfant_mortality_df %&gt;% \n  slice(start_index:end_index) %&gt;% \n  janitor::row_to_names(1) %&gt;% \n* mutate_at(- 1, as.numeric) # all except the 1st col\n\n\n\n# A tibble: 60 √ó 62\n   `GEO/TIME`     `1960` `1961` `1962` `1963` `1964` `1965` `1966` `1967` `1968`\n   &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 European Unio‚Ä¶   NA     38.2   36.4   34.3   31.9   30     29.2   28.7   29.2\n 2 European Unio‚Ä¶   NA     36.2   34.6   32.7   30.5   28.7   28     27.5   27.9\n 3 European Unio‚Ä¶   NA     36     34.4   32.5   30.3   28.5   27.8   27.4   27.8\n 4 Euro area - 1‚Ä¶   NA     34.8   33.3   31.8   29.6   28.3   27.6   26.2   25.7\n 5 Euro area - 1‚Ä¶   NA     34.8   33.2   31.8   29.6   28.3   27.6   26.2   25.7\n 6 Belgium          31.4   28.1   27.5   27.2   25.3   23.7   24.7   22.9   21.7\n 7 Bulgaria         45.1   37.8   37.3   35.7   32.9   30.8   32.2   33.1   28.3\n 8 Czechia          20     19.3   21.1   19.7   19.1   23.7   21.9   21.5   21.6\n 9 Denmark          21.5   21.8   20.1   19.1   18.7   18.7   16.9   15.8   16.4\n10 Germany (unti‚Ä¶   33.8   31.7   29.3   27     25.3   23.9   23.6   22.9   22.8\n# ‚Ñπ 50 more rows\n# ‚Ñπ 52 more variables: `1969` &lt;dbl&gt;, `1970` &lt;dbl&gt;, `1971` &lt;dbl&gt;, `1972` &lt;dbl&gt;,\n#   `1973` &lt;dbl&gt;, `1974` &lt;dbl&gt;, `1975` &lt;dbl&gt;, `1976` &lt;dbl&gt;, `1977` &lt;dbl&gt;,\n#   `1978` &lt;dbl&gt;, `1979` &lt;dbl&gt;, `1980` &lt;dbl&gt;, `1981` &lt;dbl&gt;, `1982` &lt;dbl&gt;,\n#   `1983` &lt;dbl&gt;, `1984` &lt;dbl&gt;, `1985` &lt;dbl&gt;, `1986` &lt;dbl&gt;, `1987` &lt;dbl&gt;,\n#   `1988` &lt;dbl&gt;, `1989` &lt;dbl&gt;, `1990` &lt;dbl&gt;, `1991` &lt;dbl&gt;, `1992` &lt;dbl&gt;,\n#   `1993` &lt;dbl&gt;, `1994` &lt;dbl&gt;, `1995` &lt;dbl&gt;, `1996` &lt;dbl&gt;, `1997` &lt;dbl&gt;, ‚Ä¶\n\n\nAnd now lets transform it into long format.\n\ninfant_mortality_df %&gt;% \n  slice(start_index:end_index) %&gt;% \n  janitor::row_to_names(1) %&gt;% \n  mutate_at(- 1, as.numeric) %&gt;% \n* pivot_longer(- 1, names_to = \"TIME\", \n                 values_to = \"infant_mortality\")\n\n\n\n# A tibble: 3,660 √ó 3\n   `GEO/TIME`                                TIME  infant_mortality\n   &lt;chr&gt;                                     &lt;chr&gt;            &lt;dbl&gt;\n 1 European Union - 27 countries (from 2020) 1960              NA  \n 2 European Union - 27 countries (from 2020) 1961              38.2\n 3 European Union - 27 countries (from 2020) 1962              36.4\n 4 European Union - 27 countries (from 2020) 1963              34.3\n 5 European Union - 27 countries (from 2020) 1964              31.9\n 6 European Union - 27 countries (from 2020) 1965              30  \n 7 European Union - 27 countries (from 2020) 1966              29.2\n 8 European Union - 27 countries (from 2020) 1967              28.7\n 9 European Union - 27 countries (from 2020) 1968              29.2\n10 European Union - 27 countries (from 2020) 1969              28  \n# ‚Ñπ 3,650 more rows\n\n\nLets filter out aggregates and irrelevant.\n\ninfant_mortality_df %&gt;% \n  slice(start_index:end_index) %&gt;% \n  janitor::row_to_names(1) %&gt;% \n  mutate_at(- 1, as.numeric) %&gt;% \n  pivot_longer(- 1, names_to = \"TIME\", \n               values_to = \"infant_mortality\") %&gt;% \n  rename(geo = `GEO/TIME`) %&gt;% \n  mutate(\n    geo = countrycode(geo, \"country.name\", \"iso3c\"),\n  ) %&gt;% \n  filter(!is.na(geo))\n\nThis code filters out the rows where geo is in:\nEnough?\ncount() Quickly count the unique values of one or more variables\n\ninfant_mortality_df %&gt;% \n  slice(start_index:end_index) %&gt;% \n  janitor::row_to_names(1) %&gt;% \n  mutate_at(- 1, as.numeric) %&gt;% \n  pivot_longer(- 1, names_to = \"TIME\", \n               values_to = \"infant_mortality\") %&gt;% \n  rename(geo = `GEO/TIME`) %&gt;% \n  mutate(\n    geo = countrycode(geo, \"country.name\", \"iso3c\"),\n  ) %&gt;% \n  filter(!is.na(geo)) %&gt;% \n  count(geo, sort = TRUE)\n\n\n\n# A tibble: 47 √ó 2\n   geo       n\n   &lt;chr&gt; &lt;int&gt;\n 1 DEU     122\n 2 FRA     122\n 3 ALB      61\n 4 AND      61\n 5 ARM      61\n 6 AUT      61\n 7 AZE      61\n 8 BEL      61\n 9 BGR      61\n10 BIH      61\n# ‚Ñπ 37 more rows\n\n\nRemove the irrelevant duplication & convert TIME to numeric\n\ninfant_mortality_df %&gt;% \n  slice(start_index:end_index) %&gt;% \n  janitor::row_to_names(1) %&gt;% \n  mutate_at(- 1, as.numeric) %&gt;% \n  pivot_longer(- 1, names_to = \"TIME\", \n               values_to = \"infant_mortality\") %&gt;% \n  rename(geo = `GEO/TIME`) %&gt;% \n  filter(\n    geo != \"France (metropolitan)\" &\n      geo != \"Germany (until 1990 former territory of the FRG)\"\n  ) %&gt;% \nmutate(\n  geo = countrycode(geo, \"country.name\", \"iso3c\"),\n  TIME = as.numeric(TIME)\n) %&gt;% \n  filter(!is.na(geo)) \n\n\n\n# A tibble: 2,867 √ó 3\n   geo    TIME infant_mortality\n   &lt;chr&gt; &lt;dbl&gt;            &lt;dbl&gt;\n 1 BEL    1960             31.4\n 2 BEL    1961             28.1\n 3 BEL    1962             27.5\n 4 BEL    1963             27.2\n 5 BEL    1964             25.3\n 6 BEL    1965             23.7\n 7 BEL    1966             24.7\n 8 BEL    1967             22.9\n 9 BEL    1968             21.7\n10 BEL    1969             21.2\n# ‚Ñπ 2,857 more rows\n\n\nAssign it with the same name (re-write the table)\n\ninfant_mortality_df &lt;- infant_mortality_df %&gt;% \n  slice(start_index:end_index) %&gt;% \n  janitor::row_to_names(1) %&gt;% \n  mutate_at(- 1, as.numeric) %&gt;% \n  pivot_longer(- 1, names_to = \"TIME\", \n               values_to = \"infant_mortality\") %&gt;% \n  rename(geo = `GEO/TIME`) %&gt;% \n  filter(\n    geo != \"France (metropolitan)\" &\n      geo != \"Germany (until 1990 former territory of the FRG)\"\n  ) %&gt;% \nmutate(\n  geo = countrycode(geo, \"country.name\", \"iso3c\"),\n  TIME = as.numeric(TIME)\n) %&gt;% \n  filter(!is.na(geo)) \n\nLet‚Äôs transform fertility_df to a similar format\n\nfertility_df &lt;- read_csv(\"https://stats.oecd.org/sdmx-json/data/DP_LIVE/.FERTILITY.../OECD?contentType=csv&detail=code&separator=comma&csv-lang=en\")\n\n\nfertility_df &lt;- fertility_df %&gt;% \n  select(\n    geo = LOCATION, TIME, fertility = Value\n  )\n\nNow we need to link the two tables.\n\ndf &lt;- full_join(x = fertility_df, y = infant_mortality_df)\n\ndf\n\n# A tibble: 4,205 √ó 4\n   geo    TIME fertility infant_mortality\n   &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt;            &lt;dbl&gt;\n 1 AUS    1960      3.45               NA\n 2 AUS    1961      3.55               NA\n 3 AUS    1962      3.43               NA\n 4 AUS    1963      3.34               NA\n 5 AUS    1964      3.15               NA\n 6 AUS    1965      2.97               NA\n 7 AUS    1966      2.89               NA\n 8 AUS    1967      2.85               NA\n 9 AUS    1968      2.89               NA\n10 AUS    1969      2.89               NA\n# ‚Ñπ 4,195 more rows\n\n\nAnd now‚Ä¶ Let‚Äôs see what we have here. Last time we saw that we can generate summary statistics with skimr function.\n\nskimr::skim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n4205\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\ngeo\n0\n1\n2\n4\n0\n68\n0\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nTIME\n0\n1.00\n1990.42\n17.85\n1960.00\n1975.00\n1990.00\n2006.00\n2022.00\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\nfertility\n911\n0.78\n2.34\n1.20\n0.81\n1.59\n1.94\n2.59\n7.67\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\n\n\ninfant_mortality\n1890\n0.55\n14.33\n15.03\n0.00\n4.90\n9.70\n17.90\n137.60\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\nWe can also generate some pots and statistics easily with the powerful radiant package.\n\nradiant::radiant() # start a shiny app on your machine\n\nBut life is never so easy. Let‚Äôs suppose we want to visualise only a few countries to tell a story.\n\nbeveridge_plot &lt;- function(data, x, y, group, time, n_label = 5) {\n  data |&gt; \n    arrange({{ time }}) |&gt; \n    ggplot() +\n    aes({{ x }}, {{ y }}, color = {{ group }}) +\n    geom_path() +\n    geom_point(size = 2, shape = 16) +\n    ggrepel::geom_text_repel(data = ~ group_by(., {{ group }}) |&gt; \n                               slice(unique(floor(seq(from = 1, to = n(), \n                                                      length.out = n_label)))),\n                             aes(label = {{ time }}),\n                             show.legend = FALSE\n                               )\n}\n\n\ndf |&gt; \n  filter(geo %in% c(\"SVK\", \"HUN\", \"AUT\")) |&gt; \n  beveridge_plot(fertility, infant_mortality, geo, TIME)"
  },
  {
    "objectID": "content/05-content.html",
    "href": "content/05-content.html",
    "title": "Regex",
    "section": "",
    "text": "Motivation\n\n‚ÄúStrings are not glamorous, high-profile components of R, but they do play a big role in many data cleaning and preparation tasks. The stringr package provides a cohesive set of functions designed to make working with strings as easy as possible.‚Äù\n\nSource: Package description\n\n\n\n\n\n\nCHEATSHEETS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAll functions within stringr are prefixed with str_ and require a vector of strings as the primary argument. This design choice facilitates the effortless identification of the desired string manipulation function (just type ‚Äústr_‚Äù and use the TAB to browse).\n\n\n\ncourses &lt;- c(\"Big data\", \"Behavioral economics\", \"Dynamic macroeconomics 2\", \"Communication\", \"Economic instituions\")\n\ncourses\n\n[1] \"Big data\"                 \"Behavioral economics\"    \n[3] \"Dynamic macroeconomics 2\" \"Communication\"           \n[5] \"Economic instituions\"    \n\n\nBasics\nstringr is also part of the tidyverse, so you do not have to load it individually.\n\nlibrary(tidyverse)\n\nCombine strings:\n\nstr_c(courses, \" 2\")\n\n[1] \"Big data 2\"                 \"Behavioral economics 2\"    \n[3] \"Dynamic macroeconomics 2 2\" \"Communication 2\"           \n[5] \"Economic instituions 2\"    \n\n\n\nWhich subject is about economics?\n\nstr_detect(courses, \"economics\")\n\n[1] FALSE  TRUE  TRUE FALSE FALSE\n\n\nOf course, these functions can also be used in the structure seen earlier (in a tidy format).\n\ntibble(courses)\n\n# A tibble: 5 √ó 1\n  courses                 \n  &lt;chr&gt;                   \n1 Big data                \n2 Behavioral economics    \n3 Dynamic macroeconomics 2\n4 Communication           \n5 Economic instituions    \n\n\n\ntibble(courses) %&gt;% \n  mutate(\n    about_economics = str_detect(courses, \"economic\")\n  )\n\n# A tibble: 5 √ó 2\n  courses                  about_economics\n  &lt;chr&gt;                    &lt;lgl&gt;          \n1 Big data                 FALSE          \n2 Behavioral economics     TRUE           \n3 Dynamic macroeconomics 2 TRUE           \n4 Communication            FALSE          \n5 Economic instituions     FALSE          \n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you look carefully at the outcome, you can see that these functions are cAsE sENsItIVE (the FALSE value in the last row).\n\n\nSolution 1. - convert everything to lower case\n\ntibble(courses) %&gt;% \n  mutate(\n    courses = str_to_lower(courses),\n    about_economics = str_detect(courses, \"economic\")\n  )\n\n# A tibble: 5 √ó 2\n  courses                  about_economics\n  &lt;chr&gt;                    &lt;lgl&gt;          \n1 big data                 FALSE          \n2 behavioral economics     TRUE           \n3 dynamic macroeconomics 2 TRUE           \n4 communication            FALSE          \n5 economic instituions     TRUE           \n\n\nSolution 2. - detect with lower and upper case\n\ntibble(courses) %&gt;% \n  mutate(\n    about_economics = str_detect(courses, \"economic|Economic\")\n  )\n\n# A tibble: 5 √ó 2\n  courses                  about_economics\n  &lt;chr&gt;                    &lt;lgl&gt;          \n1 Big data                 FALSE          \n2 Behavioral economics     TRUE           \n3 Dynamic macroeconomics 2 TRUE           \n4 Communication            FALSE          \n5 Economic instituions     TRUE           \n\n\n\nSolution 3. - detect with lower and upper first letter\n\ntibble(courses) %&gt;% \n  mutate(\n    about_economics = str_detect(courses, \"[eE]conomic\")\n  )\n\n# A tibble: 5 √ó 2\n  courses                  about_economics\n  &lt;chr&gt;                    &lt;lgl&gt;          \n1 Big data                 FALSE          \n2 Behavioral economics     TRUE           \n3 Dynamic macroeconomics 2 TRUE           \n4 Communication            FALSE          \n5 Economic instituions     TRUE           \n\n\nRegex\n\nMost string functions work with regular expressions, a concise language for describing patterns of text.\n\n[eE]conomic was an example to regular expressions (regex): ‚Äúe‚Äù or ‚ÄúE‚Äù\nRegex has a great number of special characters that we can use to describe the patterns we are looking for\n\nFor example: \\\\d is for any numeric character\n\n\ntibble(courses) %&gt;% \n  mutate(\n    about_economics = str_detect(courses, \"economic\"),\n    not_single_course = str_detect(courses, \"\\\\d\")\n  )\n\n# A tibble: 5 √ó 3\n  courses                  about_economics not_single_course\n  &lt;chr&gt;                    &lt;lgl&gt;           &lt;lgl&gt;            \n1 Big data                 FALSE           FALSE            \n2 Behavioral economics     TRUE            FALSE            \n3 Dynamic macroeconomics 2 TRUE            TRUE             \n4 Communication            FALSE           FALSE            \n5 Economic instituions     FALSE           FALSE            \n\n\n\\\\s is for whitespaces (space/new line/tabulator)\n\ntibble(courses) %&gt;% \n  mutate(\n    contain_spaces = str_detect(courses, \"\\\\s\")\n  )\n\n# A tibble: 5 √ó 2\n  courses                  contain_spaces\n  &lt;chr&gt;                    &lt;lgl&gt;         \n1 Big data                 TRUE          \n2 Behavioral economics     TRUE          \n3 Dynamic macroeconomics 2 TRUE          \n4 Communication            FALSE         \n5 Economic instituions     TRUE          \n\n\n\\\\w is for letters (but all of them contain letters)\n\ntibble(courses) %&gt;% \n  mutate(\n    contain_letter = str_detect(courses, \"\\\\w\")\n  )\n\n# A tibble: 5 √ó 2\n  courses                  contain_letter\n  &lt;chr&gt;                    &lt;lgl&gt;         \n1 Big data                 TRUE          \n2 Behavioral economics     TRUE          \n3 Dynamic macroeconomics 2 TRUE          \n4 Communication            TRUE          \n5 Economic instituions     TRUE          \n\n\n\n\n\n\n\n\nTip\n\n\n\nEach of the regex expressions presented previously has an opposite. The same code in upper case. For instance, \\\\W is for non-letter characters (numbers or white-spaces)\n\ntibble(courses) %&gt;% \n  mutate(\n    contain_non_letter = str_detect(courses, \"\\\\W\")\n  )\n\n# A tibble: 5 √ó 2\n  courses                  contain_non_letter\n  &lt;chr&gt;                    &lt;lgl&gt;             \n1 Big data                 TRUE              \n2 Behavioral economics     TRUE              \n3 Dynamic macroeconomics 2 TRUE              \n4 Communication            FALSE             \n5 Economic instituions     TRUE              \n\n\n\n\nThere are several other functions in the {stringr} package. We will cover a few in the following examples.\n\ntibble(courses) %&gt;% \n  mutate(\n    n_non_letter = str_count(courses, \"\\\\W\"),\n    n_character = str_length(courses)\n  )\n\n# A tibble: 5 √ó 3\n  courses                  n_non_letter n_character\n  &lt;chr&gt;                           &lt;int&gt;       &lt;int&gt;\n1 Big data                            1           8\n2 Behavioral economics                1          20\n3 Dynamic macroeconomics 2            2          24\n4 Communication                       0          13\n5 Economic instituions                1          20\n\n\nExtract date from url\nhttps://economaniablog.hu/2022/09/14/how-to-forecast-the-business-cycle-sentiment-speaks/\n\nx &lt;- \"https://economaniablog.hu/2022/09/14/how-to-forecast-the-business-cycle-sentiment-speaks/\"\n\n\nstr_extract(x, \"20\\\\d\\\\d/\\\\d\\\\d/\\\\d\\\\d\")\n\n[1] \"2022/09/14\"\n\n\nAn alternative solution:\n\nstr_extract(x, \"[\\\\d/-]{3,}\") %&gt;% # digit, / or - and more than 3\n  str_remove(\"[/-]$\") %&gt;% # if it is at the end\n  str_remove(\"^[/-]\") # if it is at the beginning\n\n[1] \"2022/09/14\"\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThose who want to work with webscraping and/or text analysis tools will really need to learn how to use the {stringr} functions!\n\n\nIs it a website?\n\nstr_starts(x, \"https://\")\n\n[1] TRUE\n\n\nRemove the base url, assuming that its length is always the same\n\nstr_sub(x, end = 26)\n\n[1] \"https://economaniablog.hu/\"\n\n\n\nhttps://economaniablog.hu/2022/09/14/how-to-forecast-the-business-cycle-sentiment-speaks/\nRemove the base url, assuming it lasts until the date\n\nstr_replace(x, \".*20\\\\d\\\\d/\\\\d\\\\d/\\\\d\\\\d/\", \"\")\n\n[1] \"how-to-forecast-the-business-cycle-sentiment-speaks/\"\n\n\nHere the . refers to anything, and * denotes any repetition. Thus .* before the pattern means anything before the pattern, and .* after the pattern means anything after the pattern."
  },
  {
    "objectID": "content/06-content.html",
    "href": "content/06-content.html",
    "title": "Functional programming",
    "section": "",
    "text": "‚Äú{purrr} enhances R‚Äôs functional programming (FP) toolkit by providing a complete and consistent set of tools for working with functions and vectors. If you‚Äôve never heard of FP before, the best place to start is the family of map() functions which allow you to replace many for loops with code that is both more succinct and easier to read. The best place to learn about the map() functions is the iteration chapter in R for data science.‚Äù\nSource: Package description\nThe purpose of functional programming, as it is written in description of the package, is to implement iterations (recursions) in a readable manner in our code. It is going to be just as a huge advantage of R programming as the dplyr package for tabular data.\nThe approach is very similar to what we have seen with the apply family, where there is an input object and we apply the specified function to each of its elements. This was the lapply function we encountered earlier, as we discussed previously."
  },
  {
    "objectID": "content/06-content.html#nested-tibbles",
    "href": "content/06-content.html#nested-tibbles",
    "title": "Functional programming",
    "section": "Nested tibbles",
    "text": "Nested tibbles\nThe above seen functionality, that we can store a list as a column of a tibble is great, but what if we need the whole tables as one df. Well, we can simple unnest the columns.\n\ntibble(file_names) %&gt;% \n  mutate(\n    data = map(file_names, read_csv),\n    file_names = str_remove(file_names, \".*/\"), # remove the path\n    file_names = str_remove(file_names, \".csv\"),\n    avg_cite = map_dbl(data, ~ mean(.$Cites, na.rm = TRUE)) #&lt;\n  ) %&gt;% \n  unnest(data) \n\n# A tibble: 1,978 √ó 28\n   file_names     Cites Authors Title  Year Source Publisher ArticleURL CitesURL\n   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;   \n 1 daily-inflati‚Ä¶  3295 GP Com‚Ä¶ The ‚Ä¶  2011 Quart‚Ä¶ Wiley On‚Ä¶ https://r‚Ä¶ https:/‚Ä¶\n 2 daily-inflati‚Ä¶   885 E Naka‚Ä¶ High‚Ä¶  2018 The Q‚Ä¶ academic‚Ä¶ https://a‚Ä¶ https:/‚Ä¶\n 3 daily-inflati‚Ä¶  2708 E Cast‚Ä¶ Synt‚Ä¶  2008 Synth‚Ä¶ degruyte‚Ä¶ https://w‚Ä¶ https:/‚Ä¶\n 4 daily-inflati‚Ä¶   660 RC Cor‚Ä¶ An e‚Ä¶  2018 Journ‚Ä¶ Wiley On‚Ä¶ https://a‚Ä¶ https:/‚Ä¶\n 5 daily-inflati‚Ä¶   164 MC Med‚Ä¶ Fore‚Ä¶  2021 Journ‚Ä¶ Taylor &‚Ä¶ https://w‚Ä¶ https:/‚Ä¶\n 6 daily-inflati‚Ä¶  5219 GW Sch‚Ä¶ Why ‚Ä¶  1989 The j‚Ä¶ Wiley On‚Ä¶ https://o‚Ä¶ https:/‚Ä¶\n 7 daily-inflati‚Ä¶  1701 PR Han‚Ä¶ The ‚Ä¶  2011 Econo‚Ä¶ Wiley On‚Ä¶ https://o‚Ä¶ https:/‚Ä¶\n 8 daily-inflati‚Ä¶  2573 LJ Chr‚Ä¶ The ‚Ä¶  2003 inter‚Ä¶ Wiley On‚Ä¶ https://o‚Ä¶ https:/‚Ä¶\n 9 daily-inflati‚Ä¶  4813 F Black Noise  1986 The j‚Ä¶ Wiley On‚Ä¶ https://o‚Ä¶ https:/‚Ä¶\n10 daily-inflati‚Ä¶   220 C Bind‚Ä¶ Coro‚Ä¶  2020 Revie‚Ä¶ direct.m‚Ä¶ https://d‚Ä¶ https:/‚Ä¶\n# ‚Ñπ 1,968 more rows\n# ‚Ñπ 19 more variables: GSRank &lt;dbl&gt;, QueryDate &lt;dttm&gt;, Type &lt;chr&gt;, DOI &lt;chr&gt;,\n#   ISSN &lt;lgl&gt;, CitationURL &lt;lgl&gt;, Volume &lt;lgl&gt;, Issue &lt;lgl&gt;, StartPage &lt;lgl&gt;,\n#   EndPage &lt;lgl&gt;, ECC &lt;dbl&gt;, CitesPerYear &lt;dbl&gt;, CitesPerAuthor &lt;dbl&gt;,\n#   AuthorCount &lt;dbl&gt;, Age &lt;dbl&gt;, Abstract &lt;chr&gt;, FullTextURL &lt;chr&gt;,\n#   RelatedURL &lt;chr&gt;, avg_cite &lt;dbl&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou may realize that the original unnested columns are copied to each corresponding observation.\n\n\nWe can simply use the nest function if we want to achieve the opposite.\n\nmap_dfr(file_names, read_csv, .id = \"keyword\") %&gt;% \n  mutate(\n    keyword = file_names[as.numeric(keyword)],\n    keyword = str_remove(keyword, \".*/\"), # remove the path\n    keyword = str_remove(keyword, \".csv\") # remove extension\n  ) %&gt;%\n  nest(\n    data = - keyword, # everything except \"keyword\" to the \"data\" column\n    .by = keyword\n    )\n\n# A tibble: 2 √ó 2\n  keyword                         data               \n  &lt;chr&gt;                           &lt;list&gt;             \n1 daily-inflation-online          &lt;tibble [998 √ó 26]&gt;\n2 inflation-expectations-forecast &lt;tibble [980 √ó 26]&gt;\n\n\n\nmap_dfr(file_names, read_csv, .id = \"keyword\") %&gt;% \n  mutate(\n    keyword = file_names[as.numeric(keyword)],\n    keyword = str_remove(keyword, \".*/\"), # remove the path\n    keyword = str_remove(keyword, \".csv\") # remove extension\n  ) %&gt;%\n  group_by(keyword) %&gt;%\n  nest()\n\n# A tibble: 2 √ó 2\n# Groups:   keyword [2]\n  keyword                         data               \n  &lt;chr&gt;                           &lt;list&gt;             \n1 daily-inflation-online          &lt;tibble [998 √ó 26]&gt;\n2 inflation-expectations-forecast &lt;tibble [980 √ó 26]&gt;"
  },
  {
    "objectID": "content/06-content.html#exercise",
    "href": "content/06-content.html#exercise",
    "title": "Functional programming",
    "section": "Exercise",
    "text": "Exercise\nLets open the url of the 5 most cited articles by the 2 topics, which is newer than 10 years, and the abstarct is about the US.\n\n\n\n\n\n\nTip\n\n\n\nThe walk function works similarly like map, but it does not return any value, it is useful if you want to generate side-effects (like opening something in your browser, with the browseURL).\n\n\n\nmap_dfr(file_names, read_csv, .id = \"keyword\") %&gt;% \n  mutate(\n    keyword = file_names[as.numeric(keyword)],\n    keyword = str_remove(keyword, \".*/\"), # remove the path\n    keyword = str_remove(keyword, \".csv\") # remove extension\n  ) %&gt;%\n  filter(Year &gt;= lubridate::year(Sys.Date()) - 10) %&gt;% \n  filter(str_detect(Abstract, \" US | USA\")) %&gt;% \n  group_by() %&gt;% \n  slice_max(Cites, n = 10, by = keyword) %&gt;% \n  arrange(Cites) %&gt;% \n  pull(ArticleURL) %&gt;% \n  walk(browseURL)\n\n\nlibrary(tidyverse)\n\nstockret &lt;- function(n = 1000, p = .55) {\n  ifelse(runif(n - 1) &gt; (1 - p), 2, 0)\n}\n\nkelly_ret &lt;- function(p = .5, f = .5, n = 200, m = 10000) {\n  accumulate(\n    .x = stockret(n, p),\n    .f = ~ {\n      keep = floor(.x * (1 - f))\n      play = .x - keep\n      earned = floor(keep + play * .y)\n      # message(\"K:\", round(keep), \"p\", round(play), \"x\", round(.y, 4), \"e\", round(earned))\n      earned\n    },\n    .init = m\n  )\n}\n\n\nparams_df &lt;- crossing(\n  p = seq(from = .5, to = .6, length.out = 3),\n  f = seq(from = 0, to = .7, length.out = 10)\n)\n\nparams_df\n\n# A tibble: 30 √ó 2\n       p      f\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1   0.5 0     \n 2   0.5 0.0778\n 3   0.5 0.156 \n 4   0.5 0.233 \n 5   0.5 0.311 \n 6   0.5 0.389 \n 7   0.5 0.467 \n 8   0.5 0.544 \n 9   0.5 0.622 \n10   0.5 0.7   \n# ‚Ñπ 20 more rows\n\n\n\nresults_df &lt;- params_df %&gt;% \n  mutate(\n    r = map2(p, f, ~ {\n      replicate(10000, last(kelly_ret(p = .x, f = .y)), simplify = TRUE)\n    }, .progress = TRUE), # .progress: show progress line\n    avg_r = map_dbl(r, mean)\n  )\n\n ‚ñ†‚ñ†                                 3% |  ETA:  2m\n\n\n ‚ñ†‚ñ†‚ñ†                                7% |  ETA:  2m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†                              10% |  ETA:  2m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                             13% |  ETA:  2m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                            17% |  ETA:  2m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                           20% |  ETA:  2m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                          23% |  ETA:  2m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                         27% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                        30% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                       33% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                      37% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                     40% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                    43% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                   47% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                  50% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                 53% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                57% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†               60% |  ETA: 49s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†              63% |  ETA: 45s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†             67% |  ETA: 41s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†            70% |  ETA: 37s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†           73% |  ETA: 33s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†          77% |  ETA: 29s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†         80% |  ETA: 25s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†        83% |  ETA: 20s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†       87% |  ETA: 16s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†      90% |  ETA: 12s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†     93% |  ETA:  8s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†    97% |  ETA:  4s\n\nresults_df\n\n# A tibble: 30 √ó 4\n       p      f r                    avg_r\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;list&gt;               &lt;dbl&gt;\n 1   0.5 0      &lt;dbl [10,000]&gt; 10000      \n 2   0.5 0.0778 &lt;dbl [10,000]&gt; 10067.     \n 3   0.5 0.156  &lt;dbl [10,000]&gt; 10287.     \n 4   0.5 0.233  &lt;dbl [10,000]&gt;  6046.     \n 5   0.5 0.311  &lt;dbl [10,000]&gt;  2259.     \n 6   0.5 0.389  &lt;dbl [10,000]&gt;   610.     \n 7   0.5 0.467  &lt;dbl [10,000]&gt;    50.8    \n 8   0.5 0.544  &lt;dbl [10,000]&gt;     0.00860\n 9   0.5 0.622  &lt;dbl [10,000]&gt;     0      \n10   0.5 0.7    &lt;dbl [10,000]&gt;     0      \n# ‚Ñπ 20 more rows\n\n\n\nlibrary(gt)\n\nresults_df %&gt;%\n  select(-r) %&gt;%\n  mutate(p = format(p, digits = 2)) %&gt;%\n  pivot_wider(names_from = p, values_from = avg_r) %&gt;%\n  gt() %&gt;%\n  fmt_number(-f, decimals = 0) %&gt;%\n  fmt_number(f, decimals = 2) %&gt;%\n  tab_spanner(\"p\", -1) %&gt;% \n  data_color(2) %&gt;%\n  data_color(3) %&gt;%\n  data_color(4)\n\n\n\n\n\n\n\nf\n      \n        p\n      \n    \n\n0.50\n      0.55\n      0.60\n    \n\n\n\n0.00\n10,000\n10,000\n10,000\n\n\n0.08\n10,067\n46,030\n212,650\n\n\n0.16\n10,287\n210,835\n4,184,787\n\n\n0.23\n6,046\n919,002\n87,429,495\n\n\n0.31\n2,259\n1,936,808\n1,425,716,604\n\n\n0.39\n610\n1,594,350\n5,149,530,456\n\n\n0.47\n51\n354,241\n19,601,120,279\n\n\n0.54\n0\n55,296\n34,492,225,580\n\n\n0.62\n0\n370\n803,305,667\n\n\n0.70\n0\n0\n309,720"
  },
  {
    "objectID": "content/07-content.html",
    "href": "content/07-content.html",
    "title": "A whole game",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lubridate)\n\n\n\n\n\n\ndate_to_colname &lt;- function(.data) {\n  # * wide panel format &gt; header are the dates started at 2nd col from 2017\n  dates &lt;- seq.Date(\n    from = as.Date(\"2007-01-01\"),\n    by = \"days\", \n    to = Sys.Date()\n  ) |&gt; \n    keep(~ lubridate::wday(., week_start = 1) %in% 1:5) |&gt; \n    head(ncol(.data) - 1) |&gt; \n    as.character()\n  \n  .data |&gt; \n    set_names(c(\"ticker\", dates))\n}\n\n\nbloomberg_raw &lt;- list.files(\"../data/bloomberg\", full.names = TRUE) |&gt; \n  keep(str_detect, \"/bloomberg_scores\\\\d{1,2}.xlsx\") |&gt; \n  map(.progress = \"reading raw data\", \\(x) {\n    list(\n      news_heat = readxl::read_xlsx(x, sheet = 1, progress = FALSE) |&gt; \n        date_to_colname(),\n      sentiment_avg = readxl::read_xlsx(x, sheet = 2, progress = FALSE) |&gt; \n        date_to_colname()\n    )\n  })\n\nreading raw data ‚ñ†‚ñ†‚ñ†‚ñ†                              10% |  ETA:  1m\n\n\nNew names:\nNew names:\nreading raw data ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 20% | ETA: 1m\nreading raw data ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 30% | ETA: 1m\nreading raw data ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 40% | ETA: 1m\nreading raw data ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 50% | ETA: 46s\nreading raw data ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 60% | ETA: 37s\nreading raw data ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 70% | ETA: 27s\nreading raw data ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 80% | ETA: 18s\nreading raw data ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 90% | ETA: 9s\n‚Ä¢ `` -&gt; `...4274`\n‚Ä¢ `` -&gt; `...4275`\n‚Ä¢ `` -&gt; `...4276`\n‚Ä¢ `` -&gt; `...4277`\n‚Ä¢ `` -&gt; `...4278`\n‚Ä¢ `` -&gt; `...4279`\n‚Ä¢ `` -&gt; `...4280`\n‚Ä¢ `` -&gt; `...4281`\n‚Ä¢ `` -&gt; `...4282`\n‚Ä¢ `` -&gt; `...4283`\n‚Ä¢ `` -&gt; `...4284`\n‚Ä¢ `` -&gt; `...4285`\n‚Ä¢ `` -&gt; `...4286`\n‚Ä¢ `` -&gt; `...4287`\n‚Ä¢ `` -&gt; `...4288`\n‚Ä¢ `` -&gt; `...4289`\n‚Ä¢ `` -&gt; `...4290`\n‚Ä¢ `` -&gt; `...4291`\n‚Ä¢ `` -&gt; `...4292`\n\n\n\nnews_heat_df &lt;- bloomberg_raw |&gt; \n  map_dfr(\\(x) {\n    x$news_heat |&gt; \n      pivot_longer(-1, \n                   names_to = \"time\",\n                   names_transform = ymd, \n                   values_to = \"news_heat\") |&gt; \n      mutate(\n        ticker = str_remove(ticker, \" .*\"),\n        news_heat = factor(news_heat, levels = 0:4, ordered = TRUE)\n      )\n  }) |&gt; \n  drop_na()\n\nnews_heat_df\n\n# A tibble: 22,129,018 √ó 3\n   ticker time       news_heat\n   &lt;chr&gt;  &lt;date&gt;     &lt;ord&gt;    \n 1 AAPL   2010-02-16 0        \n 2 AAPL   2010-02-17 2        \n 3 AAPL   2010-02-18 1        \n 4 AAPL   2010-02-19 0        \n 5 AAPL   2010-02-22 2        \n 6 AAPL   2010-02-23 3        \n 7 AAPL   2010-02-24 2        \n 8 AAPL   2010-02-25 4        \n 9 AAPL   2010-02-26 3        \n10 AAPL   2010-03-01 4        \n# ‚Ñπ 22,129,008 more rows\n\n\n\nsentiment_avg_df &lt;- bloomberg_raw |&gt; \n  map_dfr(\\(x) {\n    x$sentiment_avg |&gt; \n      pivot_longer(-1, \n                   names_to = \"time\",\n                   names_transform = ymd, \n                   values_to = \"sentiment_avg\") |&gt; \n      mutate(\n        ticker = str_remove(ticker, \" .*\"),\n        sentiment_avg = as.numeric(sentiment_avg)\n      )\n  }) |&gt; \n  drop_na()\n\nsentiment_avg_df\n\n# A tibble: 31,270,528 √ó 3\n   ticker time       sentiment_avg\n   &lt;chr&gt;  &lt;date&gt;             &lt;dbl&gt;\n 1 AMZN   2007-01-04       -0.500 \n 2 AMZN   2007-01-05       -0.500 \n 3 AMZN   2007-01-08       -0.500 \n 4 AMZN   2007-01-09       -0.500 \n 5 AMZN   2007-01-10        0.0555\n 6 AMZN   2007-01-11        0.0555\n 7 AMZN   2007-01-12        0.0555\n 8 AMZN   2007-01-15        0.0555\n 9 AMZN   2007-01-16        0.0555\n10 AMZN   2007-01-17        0.0555\n# ‚Ñπ 31,270,518 more rows\n\n\n\nbloomberg_df &lt;- list(\n  news_heat_df, \n  sentiment_avg_df\n) |&gt; \n  reduce(full_join, by = join_by(ticker, time)) |&gt; \n  arrange(ticker, time)\n\nbloomberg_df\n\n# A tibble: 34,499,579 √ó 4\n   ticker time       news_heat sentiment_avg\n   &lt;chr&gt;  &lt;date&gt;     &lt;ord&gt;             &lt;dbl&gt;\n 1 A      2007-01-05 &lt;NA&gt;             0.0506\n 2 A      2007-01-08 &lt;NA&gt;             0.0506\n 3 A      2007-01-09 &lt;NA&gt;             0.0506\n 4 A      2007-01-10 &lt;NA&gt;             0.0506\n 5 A      2007-01-11 &lt;NA&gt;             0.0506\n 6 A      2007-01-12 &lt;NA&gt;             0     \n 7 A      2007-01-15 &lt;NA&gt;             0     \n 8 A      2007-01-16 &lt;NA&gt;             0     \n 9 A      2007-01-17 &lt;NA&gt;             0.558 \n10 A      2007-01-18 &lt;NA&gt;             0.319 \n# ‚Ñπ 34,499,569 more rows"
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Readings, lectures, and videos",
    "section": "",
    "text": "This website contains readings related to the lecture. I don‚Äôt expect you to read the chapters before the class, as we‚Äôll cover everything during the lessons. If you have any questions or find any mistakes, please don‚Äôt hesitate to reach out to me."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            Big Data & Data Visualisation\n        ",
    "section": "",
    "text": "Big Data & Data Visualisation\n        \n        \n            Learn how to analyse, understand, and communicate about your data!\n        \n        \n            Fall 2023 MNB Insitute John von Neumann University\n        \n    \n    \n        \n            \n            \n            \n        \n    \n\n\n\n\n\nInstructor\n\n ¬† Marcell Gran√°t\n ¬† Infopark I Building\n ¬† granat.marcell@nje.hu\n ¬† Schedule an appointment\n\n\n\nCourse details\n\n ¬† on Tuesdays\n ¬† Sept 11‚ÄìDecember 02, 2023\n ¬† 15:00-18.30\n ¬† Neumann Room (computer lab)\n\n\n\nContacting me\nE-mails or Teams are the best ways to get in contact with me. I will try to respond to all course-related e-mails and messages within 24 hours. Feel free to schedule an appointment for an online meeting (especially if I am your supervisor)."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Here‚Äôs your roadmap for the semester!\n\nContent (): This page contains the readings, slides, and recorded lectures for the topic. Read and watch these first.\nAssignment (): This page contains the instructions for each assignment. Assignments are due by 11:59 PM on the day they‚Äôre listed.\n\n\n\n\n\n\n\nSubscribe!\n\n\n\nYou can subscribe to this calendar URL in Outlook, Google Calendar, or Apple Calendar:\n\n\n\n ‚ÄÉDownload"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Marcell Gran√°t\n\n\n ¬† Infopark I Building\n\n ¬† granat.marcell@nje.hu\n\n\n ¬† Schedule an appointment\n\n\n\n\n ¬† on Tuesdays\n\n ¬† Sept 11‚ÄìDecember 02, 2023\n\n ¬† 15:00-18.30\n\n ¬† Neumann Room (computer lab)"
  },
  {
    "objectID": "syllabus.html#course-objectives",
    "href": "syllabus.html#course-objectives",
    "title": "Syllabus",
    "section": "Course objectives",
    "text": "Course objectives\n\nIn the digital era, an abundant amount of data is created every day, which contains valuable information about the economy, but their proper handling is not trivial. Data analysis and visualization have now become one of the most important skills in business, but in the world of research, it is clearly the most important.\n\nThis course introduces several statistical and visualization methods that helps to work with data, such as advanced inferential statistics and dimension reduction techniques. In addition, we will put a lot of effort into helping you to use  programming language (properly) to be able to apply these tools in practice."
  },
  {
    "objectID": "syllabus.html#topics",
    "href": "syllabus.html#topics",
    "title": "Syllabus",
    "section": "Topics",
    "text": "Topics\n\n\n\n\n\n\nCaution\n\n\n\nThis is just a plan! We are still in the early stages of this program. We don‚Äôt have much experience with how much preliminary knowledge you have, so the final agenda may change a bit as we progress.\n\n\n\n\n\n\n\n\n\nWeek\n      Topic\n    \n\n\n1\nIntroduction to R, Descriptive Statistics\n\n\n2\nData Manipulation I\n\n\n3\nData Manipulation II, Functional Programming\n\n\n4\nRegression, Classification\n\n\n5\nData Visualization I\n\n\n6\nData Visualization II, Tools for Publishing\n\n\n7\nCasuality\n\n\n8\nTidy Modelling\n\n\n9\nClustering\n\n\n10\nWeb scraping, Dimension Reduction\n\n\n11\nText Mining"
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Syllabus",
    "section": "Course materials",
    "text": "Course materials\nReadings that I highly recommend for this course are free, as this course focuses primarily on the technical aspects of data analysis, which are rapidly evolving. The majority of pertinent sources are available online.\nBooks\nHighly recommended (and also free)\n\nWickham, H., √áetinkaya-Rundel, M., & Grolemund, G. (2023).¬†R for data science. ‚Äù O‚ÄôReilly Media, Inc.‚Äù\nWitten, D., & James, G. (2013).¬†An introduction to statistical learning with applications in R. springer publication.\nKuhn, M., & Silge, J. (2022).¬†Tidy modeling with R. ‚Äù O‚ÄôReilly Media, Inc.‚Äù\nVillanueva, R. A. M., & Chen, Z. J. (2019). ggplot2: elegant graphics for data analysis.\nRecommended\n\nG√°bor B√©k√©s, G√°bor K√©zdi. 2021. Data Analysis for Business, Economics, and Policy. Cambridge University Press\nCole Nussbaumer Knaflic. 2015. Storytelling with Data: A Data Visualization Guide for Business Professionals. 1st Edition. Wiley\nSchedule an appointment\nSeriously. Feel free to schedule an appointment for an online meeting (especially if I am your supervisor). I strive to keep this calendar up-to-date, so that the vacant slots are accessible to you. Oh, and it would be greatly appreciated if you could provide a brief overview of the topic we will be discussing, so I can do any necessary preparation beforehand."
  },
  {
    "objectID": "syllabus.html#assignments",
    "href": "syllabus.html#assignments",
    "title": "Syllabus",
    "section": "Assignments",
    "text": "Assignments\nYou can find the assignments on the assignments page.\nYou have the opportunity to fill out the task multiple times, but for personal reasons, I ask you not to try it for too long (I pay for the server usage fee, not the university), or run it manually. Simply copy and paste the following code into your R console, and you will be all set (You must install both of those packages first):\n\n# install.packages(\"git2r\")\n# install.packages(\"rmarkdown\")\n# install.packages(\"learnr\")\n# install.packages(\"devtools\")\n# install.packages(\"httr\")\n# devtools::install_github(\"rundel/learnrhash\")\n# install.packages(\"tidyverse\")\n# devtools::install_github(\"rstudio/gradethis\")\n# install.packages(\"shinyalert\")\n\nunlink(\"repo\", recursive = TRUE)\ngit2r::clone(url = \"https://github.com/MarcellGranat/bigdata2023_learnr.git\", local_path = \"repo\")\nrmarkdown::run(\"repo/learnr.Rmd\")\n\nOnce you have completed all the tasks, simply click on the generate button to obtain a hash code. If you got a message that your results are saved, then its saved, and you do not have any additional task. But if you do not get it, then send this hash code to me via email (the server is on local computers and anything may happen ü§∑‚Äç‚ôÇÔ∏è).\nThis hash code contains your answers as well as other relevant information about your submission. You must complete each problem set by Monday, 23:59 of the following week.\nLate work\nYou will lose 0.5 point per day for each day a problem set is late (yes, even if it‚Äôs only 5 minutes after the deadline). The table with current points are automatically generated from the server."
  },
  {
    "objectID": "syllabus.html#grades",
    "href": "syllabus.html#grades",
    "title": "Syllabus",
    "section": "Grades",
    "text": "Grades\nYou will earn a significant portion of the points with the end-of-semester written exam, but passing the course also requires completing (minimum 24 points from the 48) the ongoing problem set assignments throughout the year.\n\n\n\n\n\n\n\nAssignment\n      Points\n      Percent\n    \n\n\nProblem sets\n50\n20%\n\n\nExam\n200\n80%\n\n\nTotal\n250\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\n      Range\n    \n\n\n5\n90‚Äì100%\n\n\n4\n80‚Äì89%\n\n\n3\n66‚Äì79%\n\n\n2\n50‚Äì65%\n\n\n1\n0‚Äì49%"
  },
  {
    "objectID": "content/06-content.html#exercise-1",
    "href": "content/06-content.html#exercise-1",
    "title": "Functional programming",
    "section": "Exercise 1",
    "text": "Exercise 1\nLets open the url of the 5 most cited articles by the 2 topics, which is newer than 10 years, and the abstarct is about the US.\n\n\n\n\n\n\nTip\n\n\n\nThe walk function works similarly like map, but it does not return any value, it is useful if you want to generate side-effects (like opening something in your browser, with the browseURL).\n\n\nSolution:\n\nmap_dfr(file_names, read_csv, .id = \"keyword\") %&gt;% \n  mutate(\n    keyword = file_names[as.numeric(keyword)],\n    keyword = str_remove(keyword, \".*/\"), # remove the path\n    keyword = str_remove(keyword, \".csv\") # remove extension\n  ) %&gt;%\n  filter(Year &gt;= lubridate::year(Sys.Date()) - 10) %&gt;% \n  filter(str_detect(Abstract, \" US | USA\")) %&gt;% \n  group_by() %&gt;% \n  slice_max(Cites, n = 10, by = keyword) %&gt;% \n  arrange(Cites) %&gt;% \n  pull(ArticleURL) %&gt;% \n  walk(browseURL)"
  },
  {
    "objectID": "content/06-content.html#exercise-2",
    "href": "content/06-content.html#exercise-2",
    "title": "Functional programming",
    "section": "Exercise 2",
    "text": "Exercise 2\nLet us create a simulation to determine the optimal investment ratio (\\(f\\)) given a probability, (\\(p\\)), of doubling our invested money and a probability of \\(1-p\\) of losing it. We will play this game for a total of 200 rounds. What should be the value of \\(f\\), given a specific value of \\(p\\), in order to achieve maximum return?\n\nlibrary(tidyverse)\n\ncoins &lt;- function(n = 1000, p = .55) {\n  ifelse(runif(n - 1) &gt; (1 - p), 2, 0)\n}\n\nret &lt;- function(p = .5, f = .5, n = 200, m = 10000) {\n  accumulate(\n    .x = coins(n, p),\n    .f = ~ {\n      keep = floor(.x * (1 - f))\n      play = .x - keep\n      earned = floor(keep + play * .y)\n      # message(\"K:\", round(keep), \"p\", round(play), \"x\", round(.y, 4), \"e\", round(earned))\n      earned\n    },\n    .init = m\n  )\n}\n\n\nparams_df &lt;- crossing(\n  p = seq(from = .5, to = .6, length.out = 3),\n  f = seq(from = 0, to = .7, length.out = 10)\n)\n\nparams_df\n\n# A tibble: 30 √ó 2\n       p      f\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1   0.5 0     \n 2   0.5 0.0778\n 3   0.5 0.156 \n 4   0.5 0.233 \n 5   0.5 0.311 \n 6   0.5 0.389 \n 7   0.5 0.467 \n 8   0.5 0.544 \n 9   0.5 0.622 \n10   0.5 0.7   \n# ‚Ñπ 20 more rows\n\n\n\nresults_df &lt;- params_df %&gt;% \n  mutate(\n    r = map2(p, f, ~ {\n      replicate(10000, last(ret(p = .x, f = .y)), simplify = TRUE)\n    }, .progress = TRUE), # .progress: show progress line\n    avg_r = map_dbl(r, mean)\n  )\n\n ‚ñ†‚ñ†                                 3% |  ETA:  2m\n\n\n ‚ñ†‚ñ†‚ñ†                                7% |  ETA:  2m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†                              10% |  ETA:  2m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                             13% |  ETA:  2m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                            17% |  ETA:  2m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                           20% |  ETA:  2m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                          23% |  ETA:  2m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                         27% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                        30% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                       33% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                      37% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                     40% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                    43% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                   47% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                  50% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                 53% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                57% |  ETA:  1m\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†               60% |  ETA: 49s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†              63% |  ETA: 44s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†             67% |  ETA: 40s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†            70% |  ETA: 37s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†           73% |  ETA: 33s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†          77% |  ETA: 29s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†         80% |  ETA: 25s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†        83% |  ETA: 20s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†       87% |  ETA: 16s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†      90% |  ETA: 12s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†     93% |  ETA:  8s\n\n\n ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†    97% |  ETA:  4s\n\nresults_df\n\n# A tibble: 30 √ó 4\n       p      f r                  avg_r\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;list&gt;             &lt;dbl&gt;\n 1   0.5 0      &lt;dbl [10,000]&gt; 10000    \n 2   0.5 0.0778 &lt;dbl [10,000]&gt; 10483.   \n 3   0.5 0.156  &lt;dbl [10,000]&gt;  9619.   \n 4   0.5 0.233  &lt;dbl [10,000]&gt; 15987.   \n 5   0.5 0.311  &lt;dbl [10,000]&gt;  2677.   \n 6   0.5 0.389  &lt;dbl [10,000]&gt;   517.   \n 7   0.5 0.467  &lt;dbl [10,000]&gt;  1707.   \n 8   0.5 0.544  &lt;dbl [10,000]&gt;     0.319\n 9   0.5 0.622  &lt;dbl [10,000]&gt;     0    \n10   0.5 0.7    &lt;dbl [10,000]&gt;     0    \n# ‚Ñπ 20 more rows\n\n\n\nlibrary(gt)\n\nresults_df %&gt;%\n  select(-r) %&gt;%\n  mutate(p = format(p, digits = 2)) %&gt;%\n  pivot_wider(names_from = p, values_from = avg_r) %&gt;%\n  gt() %&gt;%\n  fmt_number(-f, decimals = 0) %&gt;%\n  fmt_number(f, decimals = 2) %&gt;%\n  tab_spanner(\"p\", -1) %&gt;% \n  data_color(2) %&gt;%\n  data_color(3) %&gt;%\n  data_color(4)\n\n\n\n\n\n\n\nf\n      \n        p\n      \n    \n\n0.50\n      0.55\n      0.60\n    \n\n\n\n0.00\n10,000\n10,000\n10,000\n\n\n0.08\n10,067\n46,030\n212,650\n\n\n0.16\n10,287\n210,835\n4,184,787\n\n\n0.23\n6,046\n919,002\n87,429,495\n\n\n0.31\n2,259\n1,936,808\n1,425,716,604\n\n\n0.39\n610\n1,594,350\n5,149,530,456\n\n\n0.47\n51\n354,241\n19,601,120,279\n\n\n0.54\n0\n55,296\n34,492,225,580\n\n\n0.62\n0\n370\n803,305,667\n\n\n0.70\n0\n0\n309,720"
  }
]